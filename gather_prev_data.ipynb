{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haznitrama/scale-resources/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_from_disk\n",
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbn_wiki_id = load_from_disk(\"dataset/paralel_id_cbn_16k\")\n",
    "cbn_wiki_jv = load_from_disk(\"dataset/paralel_jv_cbn_3k\")\n",
    "cbn_wiki_su = load_from_disk(\"dataset/paralel_su_cbn_3k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_wiki_id = load_from_disk(\"synthetic/id_titles/id_wiki-id/translated\")\n",
    "id_wiki_jv = load_from_disk(\"synthetic/id_titles/id_wiki-jv/translated\")\n",
    "id_wiki_su = load_from_disk(\"synthetic/id_titles/id_wiki-su/translated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "id_cbn = concatenate_datasets([cbn_wiki_id, id_wiki_id])\n",
    "jv_cbn = concatenate_datasets([cbn_wiki_jv, id_wiki_jv])\n",
    "su_cbn = concatenate_datasets([cbn_wiki_su, id_wiki_su])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28551"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(su_cbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 127648/127648 [00:01<00:00, 78528.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 23849/23849 [00:00<00:00, 121734.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 28551/28551 [00:00<00:00, 198725.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "id_cbn.save_to_disk(\"dataset/paralel_id_cbn_127k\")\n",
    "jv_cbn.save_to_disk(\"dataset/paralel_jv_cbn_24k\")\n",
    "su_cbn.save_to_disk(\"dataset/paralel_su_cbn_28k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_bali_dict = load_from_disk(\"dataset/paralel_dataset_from_bali_dict\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_bali_dict = paralel_bali_dict.remove_columns([\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_60k = load_from_disk(\"dataset/paralel_dataset_60k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset(data):\n",
    "    data[\"id\"] = data[\"custom_id\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 59257/59257 [00:08<00:00, 7163.49 examples/s] \n"
     ]
    }
   ],
   "source": [
    "paralel_60k = paralel_60k.map(convert_dataset, remove_columns=[\"custom_id\", \"text\", \"prompt_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_300k = load_from_disk(\"dataset/paralel_dataset_300k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'cirebonese', 'indonesian', 'balinese'],\n",
       "    num_rows: 296203\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paralel_300k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'balinese', 'indonesian', 'cirebonese'],\n",
       "    num_rows: 22248\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paralel_bali_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['indonesian', 'cirebonese', 'balinese', 'id'],\n",
       "    num_rows: 59257\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paralel_60k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392513"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "348695 + 43818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_377k = load_from_disk(\"dataset/paralel_3_lang/paralel_dataset_377k_dedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_44k = load_from_disk(\"dataset/paralel_3_lang/paralel_dataset_44k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'cirebonese', 'indonesian', 'balinese'],\n",
       "    num_rows: 348695\n",
       "})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paralel_377k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'indonesian', 'cirebonese', 'balinese'],\n",
       "    num_rows: 43818\n",
       "})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paralel_44k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "paralel_400k = concatenate_datasets([paralel_377k, paralel_44k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (4/4 shards): 100%|██████████| 392513/392513 [00:27<00:00, 14353.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "paralel_400k.save_to_disk(\"dataset/paralel_3_lang/paralel_dataset_400k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '826b756d-1984-4a9b-9e44-603ef375821c',\n",
       " 'cirebonese': 'Benteng Tiworo punika salah satunggal badan peninggalan sajarah saking Karaton Tiworo. Badan lan abha saking kubu punika taksih bisa tag saksikan hinga babak yasa. Benteng Tiworo kapanggihan ing Kelurahan Waumere, anatap Tiworo Kepulauan, kabupaten Muna Barat, Sulawesi Tenggara.\\n\\nBenteng Tiworo kadamel saking watu-watu alit lan ageng ingkang asusun rapi, lan ngadeg kuwat ing pusér Kota anatap Tiworo Kepulauan. Dhuwuré bervariasi, antawis telu ngantos papat meter. Kubu punika akělit amba kirang langkung loro hektar. Lokasi punika pinakangga saking imamat Karaton Tiworo. Hinga sapunika lokasi Benteng Tiworopun taksih disaklarkan, sanadyan badan aslinipun sampun ngalami owah-owahan.\\n\\nSajarah\\nMiturut sajarah, Benteng Tiworo dibangun ing abad XVI dening Raja Muna ingkang La Ode Asmana. Pembuatan kubu punika saking bahan watu ingkang dikangkat masyarakat kalawan cara serendeng sepanjang 150 kilometer. Watu-watu ingkang dipun-ginakaken kanggé damel Benteng Tiworo dipun-datangkan saking Lokawoghe ingkang kapanggihan ing babag Desa Tongkuno lawas. Konon jare benteng punika damel namung ing wanci satunggal dalu, dening para kuli ingkang dereng angènal rasa abot lan ringan. Kanggé ngangkat watu ingkang badhé dipun-ginakaken mbangun benteng, para kuli nglapisi tanganipun ngginakaken kain sutra. Ing babak yasa Benteng Tiworo, imamat sampun lumampah. Nanging dereng kabentuk kino nanging kestabilan, anyar sawisé punika wonten penunjukan pimpinan.\\n\\nSaliyan Benteng Tiworo, ing Tiworo ugi wonten kubu sanes ingkang letakipun taksih wonten ing kompleks Benteng Tiworo inggih punika Benteng Waobu. Saliyan punika, ing tengah-tengah Benteng Tiworo, ngadeg satunggal mesjid ingkang asma mesjid Sangia Bharakati. Mesjid punika dibangun tebih sadèrèngipun Indonesia merdeka, pas ing taun 1469. Mesjid punika dibangun dening Raja Tiworo ingkang asma Sangia Bharakati utawi La Ode Asmana.\\n\\nIng jaman rumiyin, Benteng Tiworo dipun-ginakaken minangka panggonan imamat lan pertahanan. Pusér pertahan benteng punika kapanggihan ing babag wétan. Ing babak kadadosan ibur ing wanua Tiworo, ingkang nglakoni ibur punika inggih punika sesama tiang Muna lan sanes tiang Muna kaliyan tiang Belanda. Karaton saking Benteng Tiworo piyambak mboten nate kapagut dening tentara sekutu.\\n\\nSaliyan minangka kubu pertahanan, Benteng Tiworo punika ugi nyekapi minangka panggonan pelantikan Raja. Raja ingkang terakhir dilantik ing benteng punika inggih punika La Ode Sampaga.\\n\\nIng babak punika, benteng punika migunani minangka sistem sajarah sekalian minangka panggonan wisata \"Benteng Tiworo\" kanthi asma Tiworo. Tiworo punika simbol ing imamat ingkang gadhah makna kanggé ngingetaken masyarakatipun supados tetep ana ing satunggal arah.\\n\\nReferensi\\n\\nT',\n",
       " 'indonesian': 'Benteng Tiworo adalah salah satu bentuk peninggalan bersejarah dari Kerajaan Tiworo. Bentuk dan keindahan dari benteng ini masih dapat kita saksikan hingga saat ini. Benteng Tiworo terletak di Kelurahan Waumere, Kecamatan Tiworo Kepulauan, Kabupaten Muna Barat, Sulawesi Tenggara.\\n\\nBenteng Tiworo terbuat dari batu-batu kecil dan besar yang tersusun rapi, dan berdiri kokoh di pusat Kota Kecamatan Tiworo Kepulauan. Tingginya bervariasi, antara tiga sampai empat meter. Benteng ini memiliki luas kurang lebih dua hektar. Lokasi ini menjadi pusat dari pemerintahan Kerajaan Tiworo. Hingga kini lokasi Benteng Tiworopun masih disaklarkan, meski bentuk aslinya telah mengalami perubahan.\\n\\nSejarah \\nMenurut sejarah, Benteng Tiworo dibangun pada abad XVI oleh Raja Muna yaitu La Ode Asmana. Pembuatan Benteng ini dari bahan batu yang diangkat masyarakat dengan cara berjejer sepanjang 150 kilometer. Batu-batu yang digunakan untuk membuat Benteng Tiworo didatangkan dari Lokawoghe yang terletak di bagian Desa Tongkuno lama. Konon katanya benteng ini dibuat hanya dalam waktu satu malam, oleh para pekerja yang belum mengenal rasa berat dan ringan. Untuk mengangkat batu yang akan digunakan membnangun benteng, para pekerja melapisi tangan mereka menggunakan kain sutera.Pada saat pembangunan Benteng Tiworo, pemerintahan telah berjalan. Namun belum terbentuk kino melainkan kestabilan, baru setelah itu ada penunjukan pimpinan.\\n\\nSelain Benteng Tiworo, di Tiworo juga terdapat benteng lain yang letaknya masih berada dalam kompleks Benteng Tiworo yaitu Benteng Waobu. Selain itu, di tengah-tengah Benteng Tiworo, berdiri sebuah mesjid bernama mesjid Sangia Bharakati. Mesjid ini dibangun jauh sebelum Indonesia merdeka, tepatnya pada tahun 1469. Mesjid ini dibangun oleh Raja Tiworo yang bernama Sangia Bharakati atau La Ode Asmana.\\n\\nPada zaman dahulu, Benteng Tiworo digunakan sebagai tempat pemerintahan dan pertahanan. Pusat pertahan benteng ini terletak di bagian timur. Pada saat terjadi peperangan di daerah Tiworo, yang melakukan peperangan tersebut adalah sesama orang Muna dan bukan orang Muna dengan orang Belanda. Kerajaan dari Benteng Tiworo sendiri tidak pernah diserang oleh tentara sekutu.\\n\\nSelain sebagai kubu pertahanan, Benteng Tiworo ini juga berperan sebagai tempat pelantikan Raja. Raja yang terakhir dilantik di benteng ini adalah La ode Sampaga.\\n\\nPada saat ini, benteng ini berfungsi sebagai sistem sejarah sekaligus sebagai tempat wisata \"Benteng Tiworo\" dengan nama Tiworo. Tiworo adalah simbol dalam pemerintahan yang memiliki makna untuk menghimbau masyarakatnya agar tetap ada pada satu arah.\\n\\nReferensi \\n\\nT',\n",
       " 'balinese': 'Benteng Tiworo punika silih tunggil adeg peninggalan bersejarah saking Karaton Tiworo. Adegan lan kaindahan saking benteng puniki kantun bisain iragane pabalih hingga daweg niki. Benteng Tiworo kapangit ring Kelurahan Waumere, Kecamatan Tiworo Kepulauan, Kabupaten Muna Barat, Sulawesi Tenggara.\\n\\nBenteng Tiworo kadamel saking batu-batu alit lan ageng sane mapilpil rapi, lan madeg kokoh ring pungsed Kota Kecamatan Tiworo Kepulauan. Tingginya bervariasi, embang tiga ngantos papat meter. Benteng puniki madue linggah kirang langkung dua hektar. Lokasi puniki dados pungsed saking pemerintahan Karaton Tiworo. Hingga niki lokasi Benteng Tiworopun kantun disaklarkan, yadiapin bentuk aslinya sampun ngalami perubahan.\\n\\nBabad\\nManut babad, Benteng Tiworo kagawé ring satawarsa XVI antuk Datu Muna inggih punika La Ode Asmana. Pembuatan Benteng puniki saking bakal batu sane kapenekang parajana antuk cara berjejer salantang 150 kilometer. Batu-batu sane kaanggena ring ngadegang Benteng Tiworo kadatangan saking Lokawoghe sane kapangit ring baga Desa Tongkuno lawas. Kocap, benteng puniki kagae asiki ring waktu a, antuk para kuli sane dereng ngidang rasa abot lan ringan. Kangge ngentas batu sane dados kaanggen ngadegang benteng, para kuli mabed tangan iraga nganggé kain sutera. Tatkala pangruwak Benteng Tiworo, pemerintahan sampun nglaksanayang. Nanging dereng mareka kino malihne kestabilan, anyar disubane punika wenten penunjukan pimpinan.\\n\\nSaklian Benteng Tiworo, ring Tiworo malih wenten benteng sane letakne kantun ring kompleks Benteng Tiworo inggih punika Benteng Waobu. Saklian punika, ring tengah-tengah Benteng Tiworo, madeg sebuah mesjid mawasta mesjid Sangia Bharakati. Mesjid puniki kagawé adoh sadurung Indonesia merdeka, tepatne ring tahun 1469. Mesjid puniki kagawé antuk Datu Tiworo sane mawasta Sangia Bharakati utawi La Ode Asmana.\\n\\nRing jaman kapungkur, Benteng Tiworo kaanggen dados batur pemerintahan lan pertahanan. Pungsed pertahanan benteng puniki kapangit ring baga timur. Tatkala paijeng samara ring wawidangan Tiworo, sane maganin samara punika inggih punika sesama anak Muna lan nenten anak Muna sareng anak Belanda. Karaton saking Benteng Tiworo punika nenten kantun marejek antuk bala sekutu.\\n\\nSaklian dados kubu pertahanan, Benteng Tiworo puniki malih dados batur pelantikan Datu. Datu sane pamutus dilantik ring benteng puniki inggih punika La Ode Sampaga.\\n\\nTatkala niki, benteng puniki makarya dados sistem sejarah sakaligus dados penglipuran \"Benteng Tiworo\" mawi nama Tiworo. Tiworo punika simbol ring pemerintahan sane madue piteges kangge ngimbau parajana sane ajeg wenten ring satu arah.\\n\\nPanyangkaan'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paralel_400k[456]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cbn = load_from_disk(\"dataset/paralel_2_lang/paralel_id_cbn_127k_dedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_id_text = set(paralel_377k[\"indonesian\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_id_cbn(data):\n",
    "    return data[\"text\"] not in existing_id_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 92504/92504 [00:13<00:00, 6913.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_id_cbn = id_cbn.filter(filter_id_cbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 44743/44743 [00:00<00:00, 53675.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_id_cbn.save_to_disk(\"dataset/paralel_2_lang/paralel_id_cbn_127k_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "id_mmlu = load_dataset(\"indolem/IndoMMLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "bali_mmlu = id_mmlu[\"test\"].filter(lambda x: x[\"subject\"] == \"Balinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'Balinese',\n",
       " 'group': 'Local languages and cultures',\n",
       " 'level': 'SD',\n",
       " 'class': '2',\n",
       " 'question': 'Dugas Galungane titiang mabakti nganggon udeng baru. Udeng basa Bali alusne...',\n",
       " 'options': \"['A. destar', 'B. kampuh', 'C. wastra']\",\n",
       " 'answer': 'A',\n",
       " 'is_for_fewshot': '0'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bali_mmlu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import uuid\n",
    "\n",
    "def transform_data(item):\n",
    "    question_id = str(uuid.uuid4())\n",
    "\n",
    "    # Parse the options string into a list\n",
    "    options_str = item['options']\n",
    "    options_list = ast.literal_eval(options_str)\n",
    "    \n",
    "    # Extract labels and texts from options\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for opt in options_list:\n",
    "        # Split by dot and strip whitespace\n",
    "        parts = opt.split('.', 1)\n",
    "        labels.append(parts[0].strip().lower())  # Get 'A', 'B', 'C' and convert to lowercase\n",
    "        texts.append(parts[1].strip())  # Get the actual option text\n",
    "    \n",
    "    # Create the transformed dictionary\n",
    "    transformed = {\n",
    "        \"context\": \"\",\n",
    "        \"question\": item['question'],\n",
    "        \"choices\": {\n",
    "            \"label\": labels,\n",
    "            \"text\": texts\n",
    "        },\n",
    "        \"answer\": item['answer'].lower(),  # Convert answer to lowercase to match labels\n",
    "        \"category\": [],\n",
    "        \"grade\": int(item['class']),\n",
    "        \"question_id\": question_id\n",
    "    }\n",
    "    \n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 471/471 [00:00<00:00, 7187.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "bali_mmlu = bali_mmlu.map(transform_data, remove_columns=['options', 'class', 'subject', 'group', 'level', 'class', 'is_for_fewshot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Dugas Galungane titiang mabakti nganggon udeng baru. Udeng basa Bali alusne...',\n",
       " 'answer': 'a',\n",
       " 'context': '',\n",
       " 'choices': {'label': ['a', 'b', 'c'], 'text': ['destar', 'kampuh', 'wastra']},\n",
       " 'category': [],\n",
       " 'grade': 2,\n",
       " 'question_id': 'e22caaf5-7596-4d76-a04e-61637f60a8c1'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bali_mmlu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 471/471 [00:00<00:00, 60177.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "bali_mmlu.save_to_disk(\"dataset/bali_mmlu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_377k = load_from_disk(\"dataset/paralel_3_lang/paralel_dataset_377k_dedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'cirebonese', 'indonesian', 'balinese'],\n",
       "    num_rows: 348695\n",
       "})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paralel_377k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "def num_tokens_from_string(string: str, encoder) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    num_tokens = len(encoder.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def count_tokens_in_dataset(dataset, field_name, num_tokens_from_string, encoder):\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for item in dataset:\n",
    "        text = item[field_name]\n",
    "        tokens = num_tokens_from_string(text, encoder)\n",
    "        total_tokens += tokens\n",
    "    \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_400k = load_from_disk(\"dataset/paralel_3_lang/paralel_dataset_400k_dedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168216484\n"
     ]
    }
   ],
   "source": [
    "print(count_tokens_in_dataset(paralel_400k, \"indonesian\", num_tokens_from_string, encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182488277\n"
     ]
    }
   ],
   "source": [
    "print(count_tokens_in_dataset(paralel_400k, \"cirebonese\", num_tokens_from_string, encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184671868\n"
     ]
    }
   ],
   "source": [
    "print(count_tokens_in_dataset(paralel_400k, \"balinese\", num_tokens_from_string, encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_400k = load_from_disk(\"dataset/id_hq/id_hq_data_prompt_300k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Joel Robles Blázquez (lahir 17 Juni 1990), sing dikenal kanthi jeneng Joel, ya iku pemain bal-balan profésional Spanyol sing main dadi kiper kanggo klub Liga Utama Inggris Everton F.C. Robles miwiti karir profésional ing Atlético Madrid, wiwitané ing tim C ing taun 2008, sadurungé maju ing tim B ing taun sabanjuré, banjur diunggahaké dadi pemain senior. Dhèwèké dipindhah menyang Rayo Vallecano ing Januari 2012, lan setaun sangarepé nggabung karo klub Liga Utama Wigan Athletic, ing endi dhèwèké dadi bagéyan saka pamenang FA Cup Final 2013 sadurungé gabung karo Everton.\\n\\nRobles main ing pertandhingan internasional kanggo Spanyol ing tingkat U-16, U-17, U-21 lan U-23, lan dadi bagéan saka skuad sing menang ing Kejuaraan Eropah U-21 2013.\\n\\nKarir klub\\n\\nAtlético Madrid\\nLair ing Getafe, Madrid, Robles kalebu asil produk pelatihan remaja ing Kutha Getafe. Dhèwèké gabung karo Atlético Madrid ing taun 2005. Dhèwèké ngrampungaké mangsa profésional kapisané ing mangsa (2009-10), dadi pemain cadhangan ing Segunda División B. Nalika tanggal 27 Dhésèmber 2009, dhèwèké napakasmanikontrak profésional tetandhingan nganti wulan Juni 2014.\\n\\nRujukan',\n",
       " 'id': 'cd696568-03ff-4d75-9e83-26f2c9faf7d4',\n",
       " 'prompt_text': 'Translate the given Indonesian text in the <id_text> tag below into Cirebonese & Balinese with the help of some word-to-word translation provided below. For one word, there can be multiple translations, and you need to choose the right one based on the context. Not all word need to be translated such as named entities, therefore you need to properly choose which word need to be translated and which one is the right translation based on context. The translations are as follows:\\n\\n<cbn_translation>\\n- sing: iki\\n- dikenal: teter, tineter\\n- ya: inggih, iya, muhun, enggih\\n- sing: iki\\n- main: dolan, soan\\n- utama: utami, utama\\n- maju: maju, majeng\\n- utama: utami, utama\\n- main: dolan, soan\\n- tingkat: tingkat, darajat, tatar\\n- sing: iki\\n- menang: abibawa, wijaya\\n- tanggal: purucut, mérucut\\n\\n</cbn_translation>\\n\\n<bali_translation>\\n- ya: aa, inggih, nah, nggih, to ba\\n- pemain: pamain\\n- main: palali\\n- klub: kelup\\n- utama: dasar, luih, luwih, pokok, singgih, utama\\n- maju: ngarepang\\n- pemain: pamain\\n- klub: kelup\\n- utama: dasar, luih, luwih, pokok, singgih, utama\\n- gabung: angkep, gapuk, gapul\\n- main: palali\\n- tingkat: apap, kaping, masepat, pangked, panta, sepat\\n- menang: cok, duga, gim, menang, wijaya\\n- klub: kelup\\n- remaja: bajang, bantiran, batun salak, daha, nedeng, yuana\\n- gabung: angkep, gapuk, gapul\\n- mangsa: caplok, tadaha, tetadahan\\n- mangsa: caplok, tadaha, tetadahan\\n- pemain: pamain\\n- tanggal: ala ayu\\n\\n</bali_translation>\\n\\n<id_text>\\nJoel Robles Blázquez (lahir 17 Juni 1990), sing dikenal kanthi jeneng Joel, ya iku pemain bal-balan profésional Spanyol sing main dadi kiper kanggo klub Liga Utama Inggris Everton F.C. Robles miwiti karir profésional ing Atlético Madrid, wiwitané ing tim C ing taun 2008, sadurungé maju ing tim B ing taun sabanjuré, banjur diunggahaké dadi pemain senior. Dhèwèké dipindhah menyang Rayo Vallecano ing Januari 2012, lan setaun sangarepé nggabung karo klub Liga Utama Wigan Athletic, ing endi dhèwèké dadi bagéyan saka pamenang FA Cup Final 2013 sadurungé gabung karo Everton.\\n\\nRobles main ing pertandhingan internasional kanggo Spanyol ing tingkat U-16, U-17, U-21 lan U-23, lan dadi bagéan saka skuad sing menang ing Kejuaraan Eropah U-21 2013.\\n\\nKarir klub\\n\\nAtlético Madrid\\nLair ing Getafe, Madrid, Robles kalebu asil produk pelatihan remaja ing Kutha Getafe. Dhèwèké gabung karo Atlético Madrid ing taun 2005. Dhèwèké ngrampungaké mangsa profésional kapisané ing mangsa (2009-10), dadi pemain cadhangan ing Segunda División B. Nalika tanggal 27 Dhésèmber 2009, dhèwèké napakasmanikontrak profésional tetandhingan nganti wulan Juni 2014.\\n\\nRujukan\\n</id_text>\\n\\nReturn only the translated text in JSON format with key \"cirebonese_text\" for the Cirebonese translation and \"balinese_text\" for the Balinese translation.'}"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(paralel_400k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'a1c3096e-943c-4787-9870-18017e6a701b',\n",
       " 'cirebonese': 'Ari 2002 XG4 mangrupa hiji astéroid. Ieu asteroid téh bagéan tina astéroid Amor, anu nganjrek deukeut jeung marcapada. Ékséntrisitas orbit ieu astéroid kacatet gedéna 0.480, sedengkeun magnitudo mutlakna 18.2. Ari nu cios référénsina mah nyaéta MPO 237836.\\n\\nBebentukan\\nKawas sakumna astéroid, ieu astéroid kabentuk tina nébula panonpoé primordial minangka beubeulahan planétisimal, objék di nébula marcapada ngora nu teu cukup badag pikeun robah jadi planét.\\n\\nRujukan\\n\\nTutumbu kaluar\\nDaptar astéroid Amor - The sejagat Astronomical Union Minor Planet Center.\\n\\n338347\\n338347',\n",
       " 'indonesian': 'Ari 2002 XG4 mangrupa hiji astéroid. Ieu asteroid téh bagéan tina astéroid Amor, anu nganjrek deukeut jeung marcapada. Ékséntrisitas orbit ieu astéroid kacatet gedéna 0.480, sedengkeun magnitudo mutlakna 18.2. Ari nu jadi référénsina mah nyaéta MPO\\xa0237836.\\n\\nBebentukan\\nKawas sakumna astéroid, ieu astéroid kabentuk tina nébula panonpoé primordial minangka beubeulahan planétisimal, objék di nébula marcapada ngora nu teu cukup badag pikeun robah jadi planét.\\n\\nRujukan\\n\\nTutumbu kaluar\\nDaptar astéroid Amor - The International Astronomical Union Minor Planet Center.\\n\\n338347\\n338347',\n",
       " 'balinese': 'Ari 2002 XG4 punika dados hiji astéroid. Punika astéroid punika bagéan saking astéroid Amor, sane nganjrek deukeut ring marcapada. Ékséntrisitas orbit punika astéroid kacatet ageng 0.480, sedeng magnitudo mutlakne 18.2. Ari sane dados référénsina punika ring MPO 237836.\\n\\nBebentukan\\nKawis sakumna astéroid, punika astéroid kabentuk saking nébula panonpoé primordial dados beubeulahan planétisimal, objék ring nébula marcapada ngora sane taler dados cukup badag ring robah dados planét.\\n\\nRujukan\\n\\nTutumbu kaluar\\nDaptar astéroid Amor - The International Astronomical Union Minor Planet Center.\\n\\n338347\\n338347'}"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(paralel_300k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# load the model\n",
    "model = fasttext.load_model(\"./models/glotlid/model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "class CustomLID:\n",
    "    def __init__(self, model_path, languages = -1, mode='before'):\n",
    "        self.model = fasttext.load_model(model_path)\n",
    "        self.output_matrix = self.model.get_output_matrix()\n",
    "        self.labels = self.model.get_labels()\n",
    "        \n",
    "        # compute language_indices\n",
    "        if languages !=-1 and isinstance(languages, list):\n",
    "            self.language_indices = [self.labels.index(l) for l in list(set(languages)) if l in self.labels]\n",
    "\n",
    "        else:\n",
    "            self.language_indices = list(range(len(self.labels)))\n",
    "\n",
    "        # limit labels to language_indices\n",
    "        self.labels = list(np.array(self.labels)[self.language_indices])\n",
    "        \n",
    "        # predict\n",
    "        self.predict = self.predict_limit_after_softmax if mode=='after' else self.predict_limit_before_softmax\n",
    "\n",
    "    \n",
    "    def predict_limit_before_softmax(self, text, k=1):\n",
    "        \n",
    "        # sentence vector\n",
    "        sentence_vector = self.model.get_sentence_vector(text)\n",
    "        \n",
    "        # dot\n",
    "        result_vector = np.dot(self.output_matrix[self.language_indices, :], sentence_vector)\n",
    "\n",
    "        # softmax\n",
    "        softmax_result = np.exp(result_vector - np.max(result_vector)) / np.sum(np.exp(result_vector - np.max(result_vector)))\n",
    "\n",
    "        # top k predictions\n",
    "        top_k_indices = np.argsort(softmax_result)[-k:][::-1]\n",
    "        top_k_labels = [self.labels[i] for i in top_k_indices]\n",
    "        top_k_probs = softmax_result[top_k_indices]\n",
    "\n",
    "        return tuple(top_k_labels), top_k_probs\n",
    "\n",
    "\n",
    "    def predict_limit_after_softmax(self, text, k=1):\n",
    "        \n",
    "        # sentence vector\n",
    "        sentence_vector = self.model.get_sentence_vector(text)\n",
    "        \n",
    "        # dot\n",
    "        result_vector = np.dot(self.output_matrix, sentence_vector)\n",
    "\n",
    "        # softmax\n",
    "        softmax_result = np.exp(result_vector - np.max(result_vector)) / np.sum(np.exp(result_vector - np.max(result_vector)))\n",
    "\n",
    "        # limit softmax to language_indices\n",
    "        softmax_result = softmax_result[self.language_indices]\n",
    "\n",
    "        \n",
    "        # top k predictions\n",
    "        top_k_indices = np.argsort(softmax_result)[-k:][::-1]\n",
    "        top_k_labels = [self.labels[i] for i in top_k_indices]\n",
    "        top_k_probs = softmax_result[top_k_indices]\n",
    "\n",
    "        return tuple(top_k_labels), top_k_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make sure these languages are available in GlotLID check the list of supported labels in model.labels\n",
    "limited_languages = ['__label__ind_Latn', '__label__sun_Latn', '__label__jav_Latn', '__label__ban_Latn']\n",
    "\n",
    "model = CustomLID(\"./models/glotlid/model.bin\", languages = limited_languages , mode='before')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(\"halo halo bandung ibu kota priangan\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__ind_Latn',), array([0.99998975], dtype=float32))"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "model.predict(random.choice(paralel_705k)[\"balinese\"].split(\"\\n\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# 1. Load the dataset\n",
    "paralel_705k = datasets.load_from_disk(\"dataset/paralel_3_lang/combined_paralel_dataset_705k_dedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_non_id_text(data):\n",
    "    sentences = data[\"indonesian\"].split(\"\\n\")\n",
    "    first_line = sentences[0]\n",
    "    first_pred = model.predict(first_line)[0][0]\n",
    "    if len(sentences) == 1:\n",
    "      if first_pred == \"__label__ind_Latn\":\n",
    "          return None\n",
    "      else:\n",
    "          return data\n",
    "    else:\n",
    "      second_line = sentences[1]\n",
    "      second_pred = model.predict(second_line)[0][0]\n",
    "      if first_pred == \"__label__ind_Latn\" and second_pred == \"__label__ind_Latn\":\n",
    "          return None\n",
    "      else:\n",
    "          return data\n",
    "\n",
    "def filter_id_text(data):\n",
    "    sentences = data[\"indonesian\"].split(\"\\n\")\n",
    "    first_line = sentences[0]\n",
    "    first_pred = model.predict(first_line)[0][0]\n",
    "    if len(sentences) == 1:\n",
    "      if first_pred == \"__label__ind_Latn\":\n",
    "          return data\n",
    "      else:\n",
    "          return None\n",
    "    else:\n",
    "      second_line = sentences[1]\n",
    "      second_pred = model.predict(second_line)[0][0]\n",
    "      if first_pred == \"__label__ind_Latn\" and second_pred == \"__label__ind_Latn\":\n",
    "          return data\n",
    "      else:\n",
    "          return None\n",
    "\n",
    "def filter_id_text_2(data):\n",
    "    sentences = data[\"text\"].split(\"\\n\")\n",
    "    first_line = sentences[0]\n",
    "    first_pred = model.predict(first_line)[0][0]\n",
    "    if len(sentences) == 1:\n",
    "      if first_pred == \"__label__ind_Latn\":\n",
    "          return data\n",
    "      else:\n",
    "          return None\n",
    "    else:\n",
    "      second_line = sentences[1]\n",
    "      second_pred = model.predict(second_line)[0][0]\n",
    "      if first_pred == \"__label__ind_Latn\" and second_pred == \"__label__ind_Latn\":\n",
    "          return data\n",
    "      else:\n",
    "          return None\n",
    "\n",
    "def filter_all_text(data):\n",
    "    cbn_sentences = data[\"cirebonese\"].split(\"\\n\")\n",
    "    ban_sentences = data[\"balinese\"].split(\"\\n\")\n",
    "    \n",
    "    cbn_first_line = cbn_sentences[0]\n",
    "    cbn_first_pred = model.predict(cbn_first_line)[0][0]\n",
    "    cbn_verdict = False\n",
    "    if len(cbn_sentences) == 1:\n",
    "      if cbn_first_pred == \"__label__jav_Latn\":\n",
    "          cbn_verdict = True\n",
    "    else:\n",
    "      cbn_second_line = cbn_sentences[1]\n",
    "      cbn_second_pred = model.predict(cbn_second_line)[0][0]\n",
    "      if cbn_first_pred == \"__label__jav_Latn\" or cbn_second_pred == \"__label__jav_Latn\":\n",
    "          cbn_verdict = True\n",
    "\n",
    "    ban_first_line = ban_sentences[0]\n",
    "    ban_first_pred = model.predict(ban_first_line)[0][0]\n",
    "    ban_verdict = False\n",
    "    if len(ban_sentences) == 1:\n",
    "      if ban_first_pred == \"__label__ban_Latn\":\n",
    "          ban_verdict = True\n",
    "    else:\n",
    "      ban_second_line = ban_sentences[1]\n",
    "      ban_second_pred = model.predict(ban_second_line)[0][0]\n",
    "      if ban_first_pred == \"__label__ban_Latn\" or ban_second_pred == \"__label__ban_Latn\":\n",
    "          ban_verdict = True\n",
    "\n",
    "    if cbn_verdict and ban_verdict:\n",
    "        return data\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=8): 100%|██████████| 557859/557859 [01:21<00:00, 6808.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "clean_paralel_705k = paralel_705k.filter(filter_all_text, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'cirebonese', 'indonesian', 'balinese'],\n",
       "    num_rows: 491113\n",
       "})"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_paralel_705k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_paralel_705k = load_from_disk(\"dataset/paralel_3_lang/combined_paralel_dataset_705k_dedup_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_word_length(example):\n",
    "    word_count_cbn = len(example['cirebonese'].split())\n",
    "    word_count_bali = len(example['balinese'].split())\n",
    "    return 20 <= word_count_cbn <= 50 and 20 <= word_count_bali <= 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 491113/491113 [01:05<00:00, 7551.89 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset\n",
    "filtered_dataset = clean_paralel_705k.filter(filter_by_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '60304c80-13e3-4a79-88bf-722ba2650aaf',\n",
       " 'cirebonese': 'HAT-P-1b punika sěkěrět planet jaba surya ingkang terletak sukat 520,92 taun dilah saking Bumi. Planet niki akělit pada taun 2006 kalawan ngginakaken metode transit. HAT-P-1b akělit massa amun 0,525 massa Jupiter.\\n\\nReferensi \\n\\nPlanet jaba surya',\n",
       " 'indonesian': 'HAT-P-1b adalah sebuah planet luar surya yang terletak sekitar 520,92 tahun cahaya dari Bumi. Planet ini ditemukan pada tahun 2006 dengan menggunakan metode  transit. HAT-P-1b memiliki massa sebesar 0,525 massa Jupiter.\\n\\nReferensi \\n \\n\\nPlanet luar surya',\n",
       " 'balinese': 'HAT-P-1b punika abulih planet jaba surya sane terletak sawatara 520,92 tahun dipta saking Bumi. Planet niki kangen ri tahun 2006 ajak makebah metode transit. HAT-P-1b madue massa amun 0,525 massa Jupiter.\\n\\nReferensi \\n\\nPlanet jaba surya'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(filtered_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Annotation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "bali_paralel = load_from_disk(\"dataset/paralel_3_lang/paralel_dataset_22k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 22248/22248 [00:01<00:00, 17114.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "bali_med = bali_paralel.filter(filter_by_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '816a70ef-a61a-4b83-ad48-ee4f50b77804',\n",
       " 'balinese': 'Ketut Santos Fernandez uli cerik jenek di Lampung. Uling cerik biasa tuturina ane serem-serem teken reramane. Gumi Bali ento tenget, kerana sabilang wai krama Baline jemet mabanten.',\n",
       " 'indonesian': 'Ketut Santos Fernandez sejak kanak-kanak tinggal di Lampung. Semenjak usia dini selalu diceritakan hal yang mistis oleh orangtuanya. Pulau Bali disebut angker sebab setiap hari warga Bali rajin sembahyang dan menghaturkan banten.',\n",
       " 'prompt': 'Translate the given Indonesian text in the <id_text> tag below into Cirebonese with the help of some word-to-word translation provided below. For one word, there can be multiple translations, and you need to choose the right one based on the context. The translations are as follows:\\n- setiap hari: sabên, saban dina, amban dina, unggal dina, saban\\n- sejak: atêwêk\\n- tinggal: tilar, tinggal\\n- di: teng, ning, dipun, di, ada\\n- semenjak: jég, sajég\\n- usia: umur, ayusa, ayusya\\n- selalu: kaduk, tanakadhat, pijêr, anggong, tamolah, juga, manggeh, nalar, teher, tamoli\\n- hal: bab, pérkara, pratingkah, babag, ihwal, babagan, tingkah, de, soal, pêri, purih, pri, arta, tabeat\\n- yang: ingkang, kang, sing\\n- oleh: polah, pulah, jiyadan, dening, jiyat, ulih\\n- pulau: nusa, pulo\\n- bali: ari-ari\\n- disebut: kaharan\\n- angker: singid, ingid, sungil\\n- sebab: sebab, amargi\\n- setiap: saban\\n- hari: dinten, dina\\n- warga: wargi, warga\\n- bali: ari-ari\\n- rajin: abangêt, brégas, bangêt, amijah, yatna, séput, abantér, bèrégas, ambélédig, sapit, sêbêt, senél, uson, anindita, aglis, gêtol, usen, têkun, bantér, sru, kêbat, wijah, sigép, dhêngkut, aig, upaper, gélis, bélédig\\n- dan: lan, dan\\n\\n<id_text>\\nKetut Santos Fernandez sejak kanak-kanak tinggal di Lampung. Semenjak usia dini selalu diceritakan hal yang mistis oleh orangtuanya. Pulau Bali disebut angker sebab setiap hari warga Bali rajin sembahyang dan menghaturkan banten.\\n</id_text>\\n\\nReturn only the translated text in JSON format with key \"translated_text\".',\n",
       " 'cirebonese': 'Ketut Santos Fernandez atêwêk cilik tilar ning Lampung. Jég ayusa dini kaduk diceritakan babag sing mistis dening wong tuwané. Pulo Bali kaharan singid amargi saban dinten wargi Bali rajin sembahyang lan menghaturkan banten.'}"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(bali_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_short = load_from_disk(\"dataset/paralel_3_lang/filtered_paralel_dataset_90k_short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "banned_words = [\"referensi\", \"pranala luar\", \"lihat juga\", \"lihat pula\", \"rujukan\", \"catatan kaki\", \"pranala\", \"kota\", \"kecamatan\", \"kelurahan\", \"kabupaten\", \"negara\", \"provinsi\", \"desa\", \"adalah\", \"merujuk\"]\n",
    "\n",
    "def filter_id_text(data):\n",
    "    for word in banned_words:\n",
    "        if word in data[\"indonesian\"].lower():\n",
    "            return None\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 93748/93748 [00:04<00:00, 20814.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "clean_short = filter_short.filter(filter_id_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'cirebonese', 'indonesian', 'balinese'],\n",
       "    num_rows: 1714\n",
       "})"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'e4e2ec19-1ae4-4eae-bd49-490dd01368fc',\n",
       " 'cirebonese': 'Jujuluk Natakusuma, sěkěrět jujuluk kepangeranan ingkang gage atêwêk adêg Amangkurat II ning Kesultanan Mataram, Jawa. Natakusuma, nama asli Paku Alam I.',\n",
       " 'indonesian': 'Gelar Natakusuma, sebuah gelar kepangeranan yang dimulai sejak pemerintahan Amangkurat II di Kesultanan Mataram, Jawa.\\n Natakusuma, nama asli Paku Alam I.',\n",
       " 'balinese': 'Desak Natakusuma, abulih desak kepangeranan sane ngametuang sasukate pemerintahan Amangkurat II ring Kesultanan Mataram, Jawa. Natakusuma, aran asli Paku Alam I.'}"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(clean_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1714/1714 [00:00<00:00, 108252.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "clean_short.save_to_disk(\"dataset/paralel_3_lang/filtered_paralel_dataset_1k_short_nowiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_dictionary(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "bali_indo_dict = load_dictionary(\"dict/bali_idn.json\")\n",
    "indo_bali_dict = load_dictionary(\"dict/idn_bali.json\")\n",
    "cbn_indo_dict = load_dictionary(\"dict/cbn_idn.json\")\n",
    "indo_cbn_dict = load_dictionary(\"dict/idn_cbn.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def extract_symbols(word):\n",
    "    # Extract prefix and suffix symbols\n",
    "    prefix = re.match(r'^[^\\w\\'`êÊ]*', word).group(0)\n",
    "    suffix = re.search(r'[^\\w\\'`êÊ]*$', word).group(0)\n",
    "    return prefix, suffix\n",
    "\n",
    "def clean_word(word):\n",
    "    # Keep alphanumeric, apostrophes, backticks and ê\n",
    "    cleaned = re.sub(r'[^\\w\\'`êÊ]', '', word)\n",
    "    return cleaned\n",
    "\n",
    "def correct_and_analyze_text(text, to_idn_dict, from_idn_dict):\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    valid_count = 0\n",
    "    corrected_count = 0\n",
    "    correction_dict = {}\n",
    "\n",
    "    for word in words:\n",
    "        original_word = word\n",
    "        prefix, suffix = extract_symbols(word)\n",
    "        word = clean_word(word)\n",
    "        \n",
    "        if word.islower():\n",
    "            if word.lower() in to_idn_dict:\n",
    "                corrected_words.append(original_word)\n",
    "                valid_count += 1\n",
    "            else:\n",
    "                translations = from_idn_dict.get(word.lower(), None)\n",
    "                if translations:\n",
    "                    replacement = translations[0] if translations else random.choice(translations)\n",
    "                    # Add back the symbols to the translation\n",
    "                    replacement = prefix + replacement + suffix\n",
    "                    corrected_words.append(replacement)\n",
    "                    corrected_count += 1\n",
    "                    correction_dict[original_word] = replacement\n",
    "                else:\n",
    "                    corrected_words.append(original_word)\n",
    "        else:\n",
    "            corrected_words.append(original_word)\n",
    "            valid_count += 1\n",
    "    \n",
    "    total_words = len(words)\n",
    "    invalid_count = total_words - valid_count - corrected_count\n",
    "    \n",
    "    return {\n",
    "        'corrected_text': ' '.join(corrected_words),\n",
    "        'valid_percentage': (valid_count / total_words) * 100,\n",
    "        'invalid_percentage': (invalid_count / total_words) * 100,\n",
    "        'corrected_percentage': (corrected_count / total_words) * 100,\n",
    "        'corrections': correction_dict\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_and_analyze_text_ban_dataset(data):\n",
    "    result = correct_and_analyze_text(\n",
    "        data['balinese'],\n",
    "        bali_indo_dict,\n",
    "        indo_bali_dict\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'balinese': data['balinese'].strip(),\n",
    "        'indonesian': data['indonesian'].strip(),\n",
    "        'balinese_corrected': result['corrected_text'],\n",
    "        'valid_percentage': result['valid_percentage'],\n",
    "        'invalid_percentage': result['invalid_percentage'],\n",
    "        'corrected_percentage': result['corrected_percentage'],\n",
    "        'corrections': list(result['corrections'].items()),\n",
    "    }\n",
    "\n",
    "def correct_and_analyze_text_cbn_dataset(data):\n",
    "    result = correct_and_analyze_text(\n",
    "        data['cirebonese'],\n",
    "        cbn_indo_dict,\n",
    "        indo_cbn_dict\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'cirebonese': data['cirebonese'].strip(),\n",
    "        'indonesian': data['indonesian'].strip(),\n",
    "        'cirebonese_corrected': result['corrected_text'],\n",
    "        'valid_percentage': result['valid_percentage'],\n",
    "        'invalid_percentage': result['invalid_percentage'],\n",
    "        'corrected_percentage': result['corrected_percentage'],\n",
    "        'corrections': list(result['corrections'].items()),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = clean_short.shuffle(seed=42).select(range(600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = sample.select(range(300))\n",
    "sample_2 = sample.select(range(300, 600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 7103.81 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 6936.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ban_sample = sample_2.map(correct_and_analyze_text_ban_dataset, remove_columns=['cirebonese'])\n",
    "cbn_sample = sample_1.map(correct_and_analyze_text_cbn_dataset, remove_columns=['balinese'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'f90d59f1-cfe1-4297-a65f-0c6f2c2d7256',\n",
       " 'indonesian': 'Pintu air (pelayaran), perangkat untuk mengatur kedalaman pada alur pelayaran.\\n Pintu air (floodgate), perangkat untuk mengontrol aliran air di sungai, kanal, atau waduk.\\n Sluis, perangkat untuk mengontrol tinggi aliran air di sungai atau kanal.',\n",
       " 'balinese': 'Apes yeh (pelayaran), prajuru buat ngatur kedalaman ri celocoh pelayaran. Apes yeh (gembok banjir), prajuru buat ngontrol kecoran yeh ring tukad, kanal, utawi waduk. Sluis, prajuru buat ngontrol ganggas kecoran yeh ring tukad utawi kanal.',\n",
       " 'balinese_corrected': 'Apes yeh (pelayaran), prajuru buat ngatur kedalaman ri celocoh pelayaran. Apes yeh (gembok blabar), prajuru buat ngontrol kecoran yeh ring tukad, kanal, utawi waduk. Sluis, prajuru buat ngontrol ganggas kecoran yeh ring tukad utawi kanal.',\n",
       " 'valid_percentage': 77.14285714285715,\n",
       " 'invalid_percentage': 20.0,\n",
       " 'corrected_percentage': 2.857142857142857,\n",
       " 'corrections': [['banjir),', 'blabar),']]}"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(ban_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '6dcb330f-fde0-4513-a748-6956c63757e9',\n",
       " 'cirebonese': 'Orde Anyar bisa ngacu ing pirang-pirang bab ing ngandhap: \\n\\n Orde Anyar, têngêr dum mangsa imamat Presiden Nusantara ke-2 Soeharto. \\n Orde Anyar, order politik ning Jerman Nazi.',\n",
       " 'indonesian': 'Orde Baru dapat mengacu pada beberapa hal berikut:\\n\\n Orde Baru, sebutan bagi masa pemerintahan Presiden Indonesia ke-2 Soeharto.\\n Orde Baru, tatanan politik di Jerman Nazi.',\n",
       " 'cirebonese_corrected': 'Orde Anyar bisa ngacu ing pirang-pirang bab ing ngandhap: Orde Anyar, têngêr dum mangsa imamat Presiden Nusantara ke-2 Soeharto. Orde Anyar, order politik ning Jerman Nazi.',\n",
       " 'valid_percentage': 88.46153846153845,\n",
       " 'invalid_percentage': 11.538461538461538,\n",
       " 'corrected_percentage': 0.0,\n",
       " 'corrections': []}"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(cbn_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_csv(dataset, output_file):\n",
    "    # Convert corrections list of tuples to dictionary string format\n",
    "    def format_corrections(corrections):\n",
    "        return '\\n'.join(f\"{orig} -> {corr}\" for orig, corr in corrections)\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "    # Format the corrections column\n",
    "    if 'corrections' in df.columns:\n",
    "        df['corrections'] = df['corrections'].apply(format_corrections)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Dataset saved to {output_file}\")\n",
    "    print(f\"Total rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to dataset/annotation/ban_sample_300/ban.csv\n",
      "Total rows: 300\n",
      "Dataset saved to dataset/annotation/cbn_sample_300/cbn.csv\n",
      "Total rows: 300\n"
     ]
    }
   ],
   "source": [
    "dataset_to_csv(ban_sample, \"dataset/annotation/ban_sample_300/ban.csv\")\n",
    "dataset_to_csv(cbn_sample, \"dataset/annotation/cbn_sample_300/cbn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 300/300 [00:00<00:00, 66572.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 300/300 [00:00<00:00, 74441.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ban_sample.save_to_disk(\"dataset/annotation/ban_sample_300\")\n",
    "cbn_sample.save_to_disk(\"dataset/annotation/cbn_sample_300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Previous High-Quality Balinese Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee440bc7c6047edac3972334de00f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8869337dfd429483c668d8bcb2e317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a5a50ec1b14bd2af84359a5171267b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "udhr-lid.csv:   0%|          | 0.00/7.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bb4f0ae22f41f8872a5d52bb3c9bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/27757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "dset = datasets.load_dataset(\"cis-lmu/udhr-lid\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f150f5a557194d948e15004eb8522dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/27757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_udhr = dset['test'].filter(lambda x: x['iso639-3'] == 'ban')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b0094d070a47f4b0277404de7ff2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_udhr.save_to_disk(\"dataset/bali_hq/bali_udhr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9dd5998689d4a44b6a9256e1e090949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/47.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96abc4acd0d8469f9587b03fc0c1265b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.tsv:   0%|          | 0.00/114k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cb743dc6f54dfb8ee7129fdc149d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dev.tsv:   0%|          | 0.00/15.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c89c28faf4476b9d091f1a142031bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.tsv:   0%|          | 0.00/33.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ca6786062846b58ef130392156c7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e339156cf72491ba87ea9f20491c1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2705634e9ac4c13808433aa96c42efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_sib200 = load_dataset(\"Davlan/sib200\", \"ban_Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index_id', 'category', 'text'],\n",
       "    num_rows: 701\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bali_sib200[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579dc1538ba3482da51f52fca9f8199d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/701 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_sib200[\"train\"].save_to_disk(\"dataset/bali_hq/bali_sib200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1778f5b039a4e7ba2b4bddf0d6df968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/48.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029814398714449c8d552428f81c7d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00000-of-00001.arrow:   0%|          | 0.00/8.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5026356f31642eb88e02894398c8723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_glot500 = dataset = load_dataset('cis-lmu/Glot500', 'ban_Latn', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b90bf4778c4a819327e2ea05593279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/48958 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_glot500.save_to_disk(\"dataset/bali_hq/bali_glot500\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e52b9566a44018889df4bb2f08bbbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ban_clean_0000.jsonl.gz:   0%|          | 0.00/1.19M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8a763825f54fdabce984b5b15f1a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ban_noisy_0000.jsonl.gz:   0%|          | 0.00/12.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f444e539f448038fc4e5f3bbe3e0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating clean split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce58c2903f8454e9668918efd6f37d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating noisy split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_madlad = load_dataset(\"allenai/madlad-400\", \"ban\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8962147d9c4fc6a3566d4a5f697532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/637 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_madlad[\"clean\"].save_to_disk(\"dataset/bali_hq/bali_madlad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba2728d38a04c49a94ee470b70c37d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ban_Latn.json.zst:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4f807ac5ff498ea9b60897947c72d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_nllb = load_dataset(\"acul3/KoPI-NLLB\", \"ban_Latn-neardup\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b268850c854edca4bf2e435b730b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/244545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_nllb[\"train\"].save_to_disk(\"dataset/bali_hq/bali_nllb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8929b86d8207475797e02994247eaa6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"dataset/bali_hq/id-ban.tsv\", index_col=0, sep=\"\\t\")\n",
    "\n",
    "df = df.dropna()\n",
    "df['Balinese'] = df['Balinese'].astype(str)\n",
    "# Extract only the balinese column and convert to dict format\n",
    "balinese_data = {\n",
    "    'text': df['Balinese'].tolist()\n",
    "}\n",
    "\n",
    "# Convert to HuggingFace dataset\n",
    "balinese_dataset = Dataset.from_dict(balinese_data)\n",
    "\n",
    "# Save to disk\n",
    "balinese_dataset.save_to_disk(\"dataset/bali_hq/bali_indonmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 20611\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balinese_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "bali_indonmt = load_from_disk(\"dataset/bali_hq/bali_indonmt\")\n",
    "bali_nusax = load_from_disk(\"dataset/bali_hq/bali_nusax\")\n",
    "bali_wiki = load_from_disk(\"dataset/bali_hq/bali_wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset(data):\n",
    "    data[\"text\"] = data[\"sentence\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bfdf829eb046ed8359adb2627468a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_udhr = bali_udhr.map(convert_dataset, remove_columns=[\"sentence\", \"id\", \"iso639-3\", \"iso15924\", \"language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 60\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bali_udhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 20611\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bali_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "all_bali_hq = concatenate_datasets([bali_udhr, bali_sib200, bali_glot500, bali_madlad, bali_nllb, bali_indonmt, bali_nusax, bali_wiki])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bali_hq = all_bali_hq.remove_columns(['index_id', 'category', 'dataset', 'script', 'lang_script', 'url', 'score', 'source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf9094ce89a44bc8b6cc505068ba761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/280246 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_bali_hq.save_to_disk(\"dataset/bali_hq/all_bali_hq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_bali_text(data):\n",
    "    ban_sentences = data[\"text\"].split(\"\\n\")\n",
    "    \n",
    "    ban_first_line = ban_sentences[0]\n",
    "    ban_first_pred = model.predict(ban_first_line)[0][0]\n",
    "    ban_verdict = False\n",
    "    if len(ban_sentences) == 1:\n",
    "      if ban_first_pred == \"__label__ban_Latn\":\n",
    "          ban_verdict = True\n",
    "    else:\n",
    "      ban_second_line = ban_sentences[1]\n",
    "      ban_second_pred = model.predict(ban_second_line)[0][0]\n",
    "      if ban_first_pred == \"__label__ban_Latn\" or ban_second_pred == \"__label__ban_Latn\":\n",
    "          ban_verdict = True\n",
    "\n",
    "    if ban_verdict:\n",
    "        return data\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bali_hq_dedup = load_from_disk(\"dataset/bali_hq/all_bali_hq_dedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function filter_bali_text at 0x7f632236fb50> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Filter (num_proc=8): 100%|██████████| 262601/262601 [00:11<00:00, 22536.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "all_bali_hq_clean = all_bali_hq_dedup.filter(filter_bali_text, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11075340"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens_in_dataset(all_bali_hq_clean, \"text\", num_tokens_from_string, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 201404/201404 [00:00<00:00, 207678.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "all_bali_hq_clean.save_to_disk(\"dataset/bali_hq/all_bali_hq_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 6026/6026 [00:00<00:00, 26282.14 examples/s]\n",
      "Generating validation split: 100%|██████████| 335/335 [00:00<00:00, 78690.18 examples/s]\n",
      "Generating test split: 100%|██████████| 335/335 [00:00<00:00, 77974.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# or for a specific language\n",
    "bali_ift = load_dataset(\"akoksal/muri-it-language-split\", \"ban\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 6026/6026 [00:00<00:00, 603449.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "bali_ift =  bali_ift[\"train\"]\n",
    "bali_ift.save_to_disk(\"dataset/bali_ift/bali_muri\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English High-Quality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_wiki = load_from_disk(\"dataset/en_hq/en_wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5615855"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens_in_dataset(en_wiki, \"text\", num_tokens_from_string, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
