{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haznitrama/scale-resources/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_from_disk\n",
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "def num_tokens_from_string(string: str, encoder) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    num_tokens = len(encoder.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def count_tokens_in_dataset(dataset, field_name, num_tokens_from_string, encoder):\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for item in dataset:\n",
    "        text = item[field_name]\n",
    "        tokens = num_tokens_from_string(text, encoder)\n",
    "        total_tokens += tokens\n",
    "    \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbn_wiki_id = load_from_disk(\"dataset/paralel_id_cbn_16k\")\n",
    "cbn_wiki_jv = load_from_disk(\"dataset/paralel_jv_cbn_3k\")\n",
    "cbn_wiki_su = load_from_disk(\"dataset/paralel_su_cbn_3k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_wiki_id = load_from_disk(\"synthetic/id_titles/id_wiki-id/translated\")\n",
    "id_wiki_jv = load_from_disk(\"synthetic/id_titles/id_wiki-jv/translated\")\n",
    "id_wiki_su = load_from_disk(\"synthetic/id_titles/id_wiki-su/translated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "id_cbn = concatenate_datasets([cbn_wiki_id, id_wiki_id])\n",
    "jv_cbn = concatenate_datasets([cbn_wiki_jv, id_wiki_jv])\n",
    "su_cbn = concatenate_datasets([cbn_wiki_su, id_wiki_su])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28551"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(su_cbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 127648/127648 [00:01<00:00, 78528.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 23849/23849 [00:00<00:00, 121734.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 28551/28551 [00:00<00:00, 198725.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "id_cbn.save_to_disk(\"dataset/paralel_id_cbn_127k\")\n",
    "jv_cbn.save_to_disk(\"dataset/paralel_jv_cbn_24k\")\n",
    "su_cbn.save_to_disk(\"dataset/paralel_su_cbn_28k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_bali_dict = load_from_disk(\"dataset/paralel_dataset_from_bali_dict\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_bali_dict = paralel_bali_dict.remove_columns([\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_60k = load_from_disk(\"dataset/paralel_dataset_60k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset(data):\n",
    "    data[\"id\"] = data[\"custom_id\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 59257/59257 [00:08<00:00, 7163.49 examples/s] \n"
     ]
    }
   ],
   "source": [
    "paralel_60k = paralel_60k.map(convert_dataset, remove_columns=[\"custom_id\", \"text\", \"prompt_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_300k = load_from_disk(\"dataset/paralel_dataset_300k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'cirebonese', 'indonesian', 'balinese'],\n",
       "    num_rows: 296203\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paralel_300k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'balinese', 'indonesian', 'cirebonese'],\n",
       "    num_rows: 22248\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paralel_bali_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['indonesian', 'cirebonese', 'balinese', 'id'],\n",
       "    num_rows: 59257\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paralel_60k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392513"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "348695 + 43818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_377k = load_from_disk(\"dataset/paralel_3_lang/paralel_dataset_377k_dedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_44k = load_from_disk(\"dataset/paralel_3_lang/paralel_dataset_44k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'cirebonese', 'indonesian', 'balinese'],\n",
       "    num_rows: 348695\n",
       "})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paralel_377k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'indonesian', 'cirebonese', 'balinese'],\n",
       "    num_rows: 43818\n",
       "})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paralel_44k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "paralel_400k = concatenate_datasets([paralel_377k, paralel_44k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (4/4 shards): 100%|██████████| 392513/392513 [00:27<00:00, 14353.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "paralel_400k.save_to_disk(\"dataset/paralel_3_lang/paralel_dataset_400k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '826b756d-1984-4a9b-9e44-603ef375821c',\n",
       " 'cirebonese': 'Benteng Tiworo punika salah satunggal badan peninggalan sajarah saking Karaton Tiworo. Badan lan abha saking kubu punika taksih bisa tag saksikan hinga babak yasa. Benteng Tiworo kapanggihan ing Kelurahan Waumere, anatap Tiworo Kepulauan, kabupaten Muna Barat, Sulawesi Tenggara.\\n\\nBenteng Tiworo kadamel saking watu-watu alit lan ageng ingkang asusun rapi, lan ngadeg kuwat ing pusér Kota anatap Tiworo Kepulauan. Dhuwuré bervariasi, antawis telu ngantos papat meter. Kubu punika akělit amba kirang langkung loro hektar. Lokasi punika pinakangga saking imamat Karaton Tiworo. Hinga sapunika lokasi Benteng Tiworopun taksih disaklarkan, sanadyan badan aslinipun sampun ngalami owah-owahan.\\n\\nSajarah\\nMiturut sajarah, Benteng Tiworo dibangun ing abad XVI dening Raja Muna ingkang La Ode Asmana. Pembuatan kubu punika saking bahan watu ingkang dikangkat masyarakat kalawan cara serendeng sepanjang 150 kilometer. Watu-watu ingkang dipun-ginakaken kanggé damel Benteng Tiworo dipun-datangkan saking Lokawoghe ingkang kapanggihan ing babag Desa Tongkuno lawas. Konon jare benteng punika damel namung ing wanci satunggal dalu, dening para kuli ingkang dereng angènal rasa abot lan ringan. Kanggé ngangkat watu ingkang badhé dipun-ginakaken mbangun benteng, para kuli nglapisi tanganipun ngginakaken kain sutra. Ing babak yasa Benteng Tiworo, imamat sampun lumampah. Nanging dereng kabentuk kino nanging kestabilan, anyar sawisé punika wonten penunjukan pimpinan.\\n\\nSaliyan Benteng Tiworo, ing Tiworo ugi wonten kubu sanes ingkang letakipun taksih wonten ing kompleks Benteng Tiworo inggih punika Benteng Waobu. Saliyan punika, ing tengah-tengah Benteng Tiworo, ngadeg satunggal mesjid ingkang asma mesjid Sangia Bharakati. Mesjid punika dibangun tebih sadèrèngipun Indonesia merdeka, pas ing taun 1469. Mesjid punika dibangun dening Raja Tiworo ingkang asma Sangia Bharakati utawi La Ode Asmana.\\n\\nIng jaman rumiyin, Benteng Tiworo dipun-ginakaken minangka panggonan imamat lan pertahanan. Pusér pertahan benteng punika kapanggihan ing babag wétan. Ing babak kadadosan ibur ing wanua Tiworo, ingkang nglakoni ibur punika inggih punika sesama tiang Muna lan sanes tiang Muna kaliyan tiang Belanda. Karaton saking Benteng Tiworo piyambak mboten nate kapagut dening tentara sekutu.\\n\\nSaliyan minangka kubu pertahanan, Benteng Tiworo punika ugi nyekapi minangka panggonan pelantikan Raja. Raja ingkang terakhir dilantik ing benteng punika inggih punika La Ode Sampaga.\\n\\nIng babak punika, benteng punika migunani minangka sistem sajarah sekalian minangka panggonan wisata \"Benteng Tiworo\" kanthi asma Tiworo. Tiworo punika simbol ing imamat ingkang gadhah makna kanggé ngingetaken masyarakatipun supados tetep ana ing satunggal arah.\\n\\nReferensi\\n\\nT',\n",
       " 'indonesian': 'Benteng Tiworo adalah salah satu bentuk peninggalan bersejarah dari Kerajaan Tiworo. Bentuk dan keindahan dari benteng ini masih dapat kita saksikan hingga saat ini. Benteng Tiworo terletak di Kelurahan Waumere, Kecamatan Tiworo Kepulauan, Kabupaten Muna Barat, Sulawesi Tenggara.\\n\\nBenteng Tiworo terbuat dari batu-batu kecil dan besar yang tersusun rapi, dan berdiri kokoh di pusat Kota Kecamatan Tiworo Kepulauan. Tingginya bervariasi, antara tiga sampai empat meter. Benteng ini memiliki luas kurang lebih dua hektar. Lokasi ini menjadi pusat dari pemerintahan Kerajaan Tiworo. Hingga kini lokasi Benteng Tiworopun masih disaklarkan, meski bentuk aslinya telah mengalami perubahan.\\n\\nSejarah \\nMenurut sejarah, Benteng Tiworo dibangun pada abad XVI oleh Raja Muna yaitu La Ode Asmana. Pembuatan Benteng ini dari bahan batu yang diangkat masyarakat dengan cara berjejer sepanjang 150 kilometer. Batu-batu yang digunakan untuk membuat Benteng Tiworo didatangkan dari Lokawoghe yang terletak di bagian Desa Tongkuno lama. Konon katanya benteng ini dibuat hanya dalam waktu satu malam, oleh para pekerja yang belum mengenal rasa berat dan ringan. Untuk mengangkat batu yang akan digunakan membnangun benteng, para pekerja melapisi tangan mereka menggunakan kain sutera.Pada saat pembangunan Benteng Tiworo, pemerintahan telah berjalan. Namun belum terbentuk kino melainkan kestabilan, baru setelah itu ada penunjukan pimpinan.\\n\\nSelain Benteng Tiworo, di Tiworo juga terdapat benteng lain yang letaknya masih berada dalam kompleks Benteng Tiworo yaitu Benteng Waobu. Selain itu, di tengah-tengah Benteng Tiworo, berdiri sebuah mesjid bernama mesjid Sangia Bharakati. Mesjid ini dibangun jauh sebelum Indonesia merdeka, tepatnya pada tahun 1469. Mesjid ini dibangun oleh Raja Tiworo yang bernama Sangia Bharakati atau La Ode Asmana.\\n\\nPada zaman dahulu, Benteng Tiworo digunakan sebagai tempat pemerintahan dan pertahanan. Pusat pertahan benteng ini terletak di bagian timur. Pada saat terjadi peperangan di daerah Tiworo, yang melakukan peperangan tersebut adalah sesama orang Muna dan bukan orang Muna dengan orang Belanda. Kerajaan dari Benteng Tiworo sendiri tidak pernah diserang oleh tentara sekutu.\\n\\nSelain sebagai kubu pertahanan, Benteng Tiworo ini juga berperan sebagai tempat pelantikan Raja. Raja yang terakhir dilantik di benteng ini adalah La ode Sampaga.\\n\\nPada saat ini, benteng ini berfungsi sebagai sistem sejarah sekaligus sebagai tempat wisata \"Benteng Tiworo\" dengan nama Tiworo. Tiworo adalah simbol dalam pemerintahan yang memiliki makna untuk menghimbau masyarakatnya agar tetap ada pada satu arah.\\n\\nReferensi \\n\\nT',\n",
       " 'balinese': 'Benteng Tiworo punika silih tunggil adeg peninggalan bersejarah saking Karaton Tiworo. Adegan lan kaindahan saking benteng puniki kantun bisain iragane pabalih hingga daweg niki. Benteng Tiworo kapangit ring Kelurahan Waumere, Kecamatan Tiworo Kepulauan, Kabupaten Muna Barat, Sulawesi Tenggara.\\n\\nBenteng Tiworo kadamel saking batu-batu alit lan ageng sane mapilpil rapi, lan madeg kokoh ring pungsed Kota Kecamatan Tiworo Kepulauan. Tingginya bervariasi, embang tiga ngantos papat meter. Benteng puniki madue linggah kirang langkung dua hektar. Lokasi puniki dados pungsed saking pemerintahan Karaton Tiworo. Hingga niki lokasi Benteng Tiworopun kantun disaklarkan, yadiapin bentuk aslinya sampun ngalami perubahan.\\n\\nBabad\\nManut babad, Benteng Tiworo kagawé ring satawarsa XVI antuk Datu Muna inggih punika La Ode Asmana. Pembuatan Benteng puniki saking bakal batu sane kapenekang parajana antuk cara berjejer salantang 150 kilometer. Batu-batu sane kaanggena ring ngadegang Benteng Tiworo kadatangan saking Lokawoghe sane kapangit ring baga Desa Tongkuno lawas. Kocap, benteng puniki kagae asiki ring waktu a, antuk para kuli sane dereng ngidang rasa abot lan ringan. Kangge ngentas batu sane dados kaanggen ngadegang benteng, para kuli mabed tangan iraga nganggé kain sutera. Tatkala pangruwak Benteng Tiworo, pemerintahan sampun nglaksanayang. Nanging dereng mareka kino malihne kestabilan, anyar disubane punika wenten penunjukan pimpinan.\\n\\nSaklian Benteng Tiworo, ring Tiworo malih wenten benteng sane letakne kantun ring kompleks Benteng Tiworo inggih punika Benteng Waobu. Saklian punika, ring tengah-tengah Benteng Tiworo, madeg sebuah mesjid mawasta mesjid Sangia Bharakati. Mesjid puniki kagawé adoh sadurung Indonesia merdeka, tepatne ring tahun 1469. Mesjid puniki kagawé antuk Datu Tiworo sane mawasta Sangia Bharakati utawi La Ode Asmana.\\n\\nRing jaman kapungkur, Benteng Tiworo kaanggen dados batur pemerintahan lan pertahanan. Pungsed pertahanan benteng puniki kapangit ring baga timur. Tatkala paijeng samara ring wawidangan Tiworo, sane maganin samara punika inggih punika sesama anak Muna lan nenten anak Muna sareng anak Belanda. Karaton saking Benteng Tiworo punika nenten kantun marejek antuk bala sekutu.\\n\\nSaklian dados kubu pertahanan, Benteng Tiworo puniki malih dados batur pelantikan Datu. Datu sane pamutus dilantik ring benteng puniki inggih punika La Ode Sampaga.\\n\\nTatkala niki, benteng puniki makarya dados sistem sejarah sakaligus dados penglipuran \"Benteng Tiworo\" mawi nama Tiworo. Tiworo punika simbol ring pemerintahan sane madue piteges kangge ngimbau parajana sane ajeg wenten ring satu arah.\\n\\nPanyangkaan'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paralel_400k[456]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cbn = load_from_disk(\"dataset/paralel_2_lang/paralel_id_cbn_127k_dedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_id_text = set(paralel_377k[\"indonesian\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_id_cbn(data):\n",
    "    return data[\"text\"] not in existing_id_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 92504/92504 [00:13<00:00, 6913.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_id_cbn = id_cbn.filter(filter_id_cbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 44743/44743 [00:00<00:00, 53675.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_id_cbn.save_to_disk(\"dataset/paralel_2_lang/paralel_id_cbn_127k_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "id_mmlu = load_dataset(\"indolem/IndoMMLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "bali_mmlu = id_mmlu[\"test\"].filter(lambda x: x[\"subject\"] == \"Balinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'Balinese',\n",
       " 'group': 'Local languages and cultures',\n",
       " 'level': 'SD',\n",
       " 'class': '2',\n",
       " 'question': 'Dugas Galungane titiang mabakti nganggon udeng baru. Udeng basa Bali alusne...',\n",
       " 'options': \"['A. destar', 'B. kampuh', 'C. wastra']\",\n",
       " 'answer': 'A',\n",
       " 'is_for_fewshot': '0'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bali_mmlu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import uuid\n",
    "\n",
    "def transform_data(item):\n",
    "    question_id = str(uuid.uuid4())\n",
    "\n",
    "    # Parse the options string into a list\n",
    "    options_str = item['options']\n",
    "    options_list = ast.literal_eval(options_str)\n",
    "    \n",
    "    # Extract labels and texts from options\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for opt in options_list:\n",
    "        # Split by dot and strip whitespace\n",
    "        parts = opt.split('.', 1)\n",
    "        labels.append(parts[0].strip().lower())  # Get 'A', 'B', 'C' and convert to lowercase\n",
    "        texts.append(parts[1].strip())  # Get the actual option text\n",
    "    \n",
    "    # Create the transformed dictionary\n",
    "    transformed = {\n",
    "        \"context\": \"\",\n",
    "        \"question\": item['question'],\n",
    "        \"choices\": {\n",
    "            \"label\": labels,\n",
    "            \"text\": texts\n",
    "        },\n",
    "        \"answer\": item['answer'].lower(),  # Convert answer to lowercase to match labels\n",
    "        \"category\": [],\n",
    "        \"grade\": int(item['class']),\n",
    "        \"question_id\": question_id\n",
    "    }\n",
    "    \n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 471/471 [00:00<00:00, 7187.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "bali_mmlu = bali_mmlu.map(transform_data, remove_columns=['options', 'class', 'subject', 'group', 'level', 'class', 'is_for_fewshot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Dugas Galungane titiang mabakti nganggon udeng baru. Udeng basa Bali alusne...',\n",
       " 'answer': 'a',\n",
       " 'context': '',\n",
       " 'choices': {'label': ['a', 'b', 'c'], 'text': ['destar', 'kampuh', 'wastra']},\n",
       " 'category': [],\n",
       " 'grade': 2,\n",
       " 'question_id': 'e22caaf5-7596-4d76-a04e-61637f60a8c1'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bali_mmlu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 471/471 [00:00<00:00, 60177.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "bali_mmlu.save_to_disk(\"dataset/bali_mmlu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_377k = load_from_disk(\"dataset/paralel_3_lang/paralel_dataset_377k_dedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'cirebonese', 'indonesian', 'balinese'],\n",
       "    num_rows: 348695\n",
       "})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paralel_377k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_400k = load_from_disk(\"dataset/paralel_3_lang/combined_paralel_dataset_705k_dedup_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217599839\n"
     ]
    }
   ],
   "source": [
    "print(count_tokens_in_dataset(paralel_400k, \"indonesian\", num_tokens_from_string, encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238299598\n"
     ]
    }
   ],
   "source": [
    "print(count_tokens_in_dataset(paralel_400k, \"cirebonese\", num_tokens_from_string, encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239976914\n"
     ]
    }
   ],
   "source": [
    "print(count_tokens_in_dataset(paralel_400k, \"balinese\", num_tokens_from_string, encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Joel Robles Blázquez (lahir 17 Juni 1990), sing dikenal kanthi jeneng Joel, ya iku pemain bal-balan profésional Spanyol sing main dadi kiper kanggo klub Liga Utama Inggris Everton F.C. Robles miwiti karir profésional ing Atlético Madrid, wiwitané ing tim C ing taun 2008, sadurungé maju ing tim B ing taun sabanjuré, banjur diunggahaké dadi pemain senior. Dhèwèké dipindhah menyang Rayo Vallecano ing Januari 2012, lan setaun sangarepé nggabung karo klub Liga Utama Wigan Athletic, ing endi dhèwèké dadi bagéyan saka pamenang FA Cup Final 2013 sadurungé gabung karo Everton.\\n\\nRobles main ing pertandhingan internasional kanggo Spanyol ing tingkat U-16, U-17, U-21 lan U-23, lan dadi bagéan saka skuad sing menang ing Kejuaraan Eropah U-21 2013.\\n\\nKarir klub\\n\\nAtlético Madrid\\nLair ing Getafe, Madrid, Robles kalebu asil produk pelatihan remaja ing Kutha Getafe. Dhèwèké gabung karo Atlético Madrid ing taun 2005. Dhèwèké ngrampungaké mangsa profésional kapisané ing mangsa (2009-10), dadi pemain cadhangan ing Segunda División B. Nalika tanggal 27 Dhésèmber 2009, dhèwèké napakasmanikontrak profésional tetandhingan nganti wulan Juni 2014.\\n\\nRujukan',\n",
       " 'id': 'cd696568-03ff-4d75-9e83-26f2c9faf7d4',\n",
       " 'prompt_text': 'Translate the given Indonesian text in the <id_text> tag below into Cirebonese & Balinese with the help of some word-to-word translation provided below. For one word, there can be multiple translations, and you need to choose the right one based on the context. Not all word need to be translated such as named entities, therefore you need to properly choose which word need to be translated and which one is the right translation based on context. The translations are as follows:\\n\\n<cbn_translation>\\n- sing: iki\\n- dikenal: teter, tineter\\n- ya: inggih, iya, muhun, enggih\\n- sing: iki\\n- main: dolan, soan\\n- utama: utami, utama\\n- maju: maju, majeng\\n- utama: utami, utama\\n- main: dolan, soan\\n- tingkat: tingkat, darajat, tatar\\n- sing: iki\\n- menang: abibawa, wijaya\\n- tanggal: purucut, mérucut\\n\\n</cbn_translation>\\n\\n<bali_translation>\\n- ya: aa, inggih, nah, nggih, to ba\\n- pemain: pamain\\n- main: palali\\n- klub: kelup\\n- utama: dasar, luih, luwih, pokok, singgih, utama\\n- maju: ngarepang\\n- pemain: pamain\\n- klub: kelup\\n- utama: dasar, luih, luwih, pokok, singgih, utama\\n- gabung: angkep, gapuk, gapul\\n- main: palali\\n- tingkat: apap, kaping, masepat, pangked, panta, sepat\\n- menang: cok, duga, gim, menang, wijaya\\n- klub: kelup\\n- remaja: bajang, bantiran, batun salak, daha, nedeng, yuana\\n- gabung: angkep, gapuk, gapul\\n- mangsa: caplok, tadaha, tetadahan\\n- mangsa: caplok, tadaha, tetadahan\\n- pemain: pamain\\n- tanggal: ala ayu\\n\\n</bali_translation>\\n\\n<id_text>\\nJoel Robles Blázquez (lahir 17 Juni 1990), sing dikenal kanthi jeneng Joel, ya iku pemain bal-balan profésional Spanyol sing main dadi kiper kanggo klub Liga Utama Inggris Everton F.C. Robles miwiti karir profésional ing Atlético Madrid, wiwitané ing tim C ing taun 2008, sadurungé maju ing tim B ing taun sabanjuré, banjur diunggahaké dadi pemain senior. Dhèwèké dipindhah menyang Rayo Vallecano ing Januari 2012, lan setaun sangarepé nggabung karo klub Liga Utama Wigan Athletic, ing endi dhèwèké dadi bagéyan saka pamenang FA Cup Final 2013 sadurungé gabung karo Everton.\\n\\nRobles main ing pertandhingan internasional kanggo Spanyol ing tingkat U-16, U-17, U-21 lan U-23, lan dadi bagéan saka skuad sing menang ing Kejuaraan Eropah U-21 2013.\\n\\nKarir klub\\n\\nAtlético Madrid\\nLair ing Getafe, Madrid, Robles kalebu asil produk pelatihan remaja ing Kutha Getafe. Dhèwèké gabung karo Atlético Madrid ing taun 2005. Dhèwèké ngrampungaké mangsa profésional kapisané ing mangsa (2009-10), dadi pemain cadhangan ing Segunda División B. Nalika tanggal 27 Dhésèmber 2009, dhèwèké napakasmanikontrak profésional tetandhingan nganti wulan Juni 2014.\\n\\nRujukan\\n</id_text>\\n\\nReturn only the translated text in JSON format with key \"cirebonese_text\" for the Cirebonese translation and \"balinese_text\" for the Balinese translation.'}"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(paralel_400k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'a1c3096e-943c-4787-9870-18017e6a701b',\n",
       " 'cirebonese': 'Ari 2002 XG4 mangrupa hiji astéroid. Ieu asteroid téh bagéan tina astéroid Amor, anu nganjrek deukeut jeung marcapada. Ékséntrisitas orbit ieu astéroid kacatet gedéna 0.480, sedengkeun magnitudo mutlakna 18.2. Ari nu cios référénsina mah nyaéta MPO 237836.\\n\\nBebentukan\\nKawas sakumna astéroid, ieu astéroid kabentuk tina nébula panonpoé primordial minangka beubeulahan planétisimal, objék di nébula marcapada ngora nu teu cukup badag pikeun robah jadi planét.\\n\\nRujukan\\n\\nTutumbu kaluar\\nDaptar astéroid Amor - The sejagat Astronomical Union Minor Planet Center.\\n\\n338347\\n338347',\n",
       " 'indonesian': 'Ari 2002 XG4 mangrupa hiji astéroid. Ieu asteroid téh bagéan tina astéroid Amor, anu nganjrek deukeut jeung marcapada. Ékséntrisitas orbit ieu astéroid kacatet gedéna 0.480, sedengkeun magnitudo mutlakna 18.2. Ari nu jadi référénsina mah nyaéta MPO\\xa0237836.\\n\\nBebentukan\\nKawas sakumna astéroid, ieu astéroid kabentuk tina nébula panonpoé primordial minangka beubeulahan planétisimal, objék di nébula marcapada ngora nu teu cukup badag pikeun robah jadi planét.\\n\\nRujukan\\n\\nTutumbu kaluar\\nDaptar astéroid Amor - The International Astronomical Union Minor Planet Center.\\n\\n338347\\n338347',\n",
       " 'balinese': 'Ari 2002 XG4 punika dados hiji astéroid. Punika astéroid punika bagéan saking astéroid Amor, sane nganjrek deukeut ring marcapada. Ékséntrisitas orbit punika astéroid kacatet ageng 0.480, sedeng magnitudo mutlakne 18.2. Ari sane dados référénsina punika ring MPO 237836.\\n\\nBebentukan\\nKawis sakumna astéroid, punika astéroid kabentuk saking nébula panonpoé primordial dados beubeulahan planétisimal, objék ring nébula marcapada ngora sane taler dados cukup badag ring robah dados planét.\\n\\nRujukan\\n\\nTutumbu kaluar\\nDaptar astéroid Amor - The International Astronomical Union Minor Planet Center.\\n\\n338347\\n338347'}"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(paralel_300k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(\"halo halo bandung ibu kota priangan\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__ind_Latn',), array([0.99998975], dtype=float32))"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "model.predict(random.choice(paralel_705k)[\"balinese\"].split(\"\\n\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# 1. Load the dataset\n",
    "paralel_705k = datasets.load_from_disk(\"dataset/paralel_3_lang/combined_paralel_dataset_705k_dedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=8): 100%|██████████| 557859/557859 [01:21<00:00, 6808.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "clean_paralel_705k = paralel_705k.filter(filter_all_text, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'cirebonese', 'indonesian', 'balinese'],\n",
       "    num_rows: 491113\n",
       "})"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_paralel_705k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_paralel_705k = load_from_disk(\"dataset/paralel_3_lang/combined_paralel_dataset_705k_dedup_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_word_length(example):\n",
    "    word_count_cbn = len(example['cirebonese'].split())\n",
    "    word_count_bali = len(example['balinese'].split())\n",
    "    return 20 <= word_count_cbn <= 50 and 20 <= word_count_bali <= 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 491113/491113 [01:05<00:00, 7551.89 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset\n",
    "filtered_dataset = clean_paralel_705k.filter(filter_by_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '60304c80-13e3-4a79-88bf-722ba2650aaf',\n",
       " 'cirebonese': 'HAT-P-1b punika sěkěrět planet jaba surya ingkang terletak sukat 520,92 taun dilah saking Bumi. Planet niki akělit pada taun 2006 kalawan ngginakaken metode transit. HAT-P-1b akělit massa amun 0,525 massa Jupiter.\\n\\nReferensi \\n\\nPlanet jaba surya',\n",
       " 'indonesian': 'HAT-P-1b adalah sebuah planet luar surya yang terletak sekitar 520,92 tahun cahaya dari Bumi. Planet ini ditemukan pada tahun 2006 dengan menggunakan metode  transit. HAT-P-1b memiliki massa sebesar 0,525 massa Jupiter.\\n\\nReferensi \\n \\n\\nPlanet luar surya',\n",
       " 'balinese': 'HAT-P-1b punika abulih planet jaba surya sane terletak sawatara 520,92 tahun dipta saking Bumi. Planet niki kangen ri tahun 2006 ajak makebah metode transit. HAT-P-1b madue massa amun 0,525 massa Jupiter.\\n\\nReferensi \\n\\nPlanet jaba surya'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(filtered_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Annotation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "bali_paralel = load_from_disk(\"dataset/paralel_3_lang/paralel_dataset_22k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 22248/22248 [00:01<00:00, 17114.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "bali_med = bali_paralel.filter(filter_by_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '816a70ef-a61a-4b83-ad48-ee4f50b77804',\n",
       " 'balinese': 'Ketut Santos Fernandez uli cerik jenek di Lampung. Uling cerik biasa tuturina ane serem-serem teken reramane. Gumi Bali ento tenget, kerana sabilang wai krama Baline jemet mabanten.',\n",
       " 'indonesian': 'Ketut Santos Fernandez sejak kanak-kanak tinggal di Lampung. Semenjak usia dini selalu diceritakan hal yang mistis oleh orangtuanya. Pulau Bali disebut angker sebab setiap hari warga Bali rajin sembahyang dan menghaturkan banten.',\n",
       " 'prompt': 'Translate the given Indonesian text in the <id_text> tag below into Cirebonese with the help of some word-to-word translation provided below. For one word, there can be multiple translations, and you need to choose the right one based on the context. The translations are as follows:\\n- setiap hari: sabên, saban dina, amban dina, unggal dina, saban\\n- sejak: atêwêk\\n- tinggal: tilar, tinggal\\n- di: teng, ning, dipun, di, ada\\n- semenjak: jég, sajég\\n- usia: umur, ayusa, ayusya\\n- selalu: kaduk, tanakadhat, pijêr, anggong, tamolah, juga, manggeh, nalar, teher, tamoli\\n- hal: bab, pérkara, pratingkah, babag, ihwal, babagan, tingkah, de, soal, pêri, purih, pri, arta, tabeat\\n- yang: ingkang, kang, sing\\n- oleh: polah, pulah, jiyadan, dening, jiyat, ulih\\n- pulau: nusa, pulo\\n- bali: ari-ari\\n- disebut: kaharan\\n- angker: singid, ingid, sungil\\n- sebab: sebab, amargi\\n- setiap: saban\\n- hari: dinten, dina\\n- warga: wargi, warga\\n- bali: ari-ari\\n- rajin: abangêt, brégas, bangêt, amijah, yatna, séput, abantér, bèrégas, ambélédig, sapit, sêbêt, senél, uson, anindita, aglis, gêtol, usen, têkun, bantér, sru, kêbat, wijah, sigép, dhêngkut, aig, upaper, gélis, bélédig\\n- dan: lan, dan\\n\\n<id_text>\\nKetut Santos Fernandez sejak kanak-kanak tinggal di Lampung. Semenjak usia dini selalu diceritakan hal yang mistis oleh orangtuanya. Pulau Bali disebut angker sebab setiap hari warga Bali rajin sembahyang dan menghaturkan banten.\\n</id_text>\\n\\nReturn only the translated text in JSON format with key \"translated_text\".',\n",
       " 'cirebonese': 'Ketut Santos Fernandez atêwêk cilik tilar ning Lampung. Jég ayusa dini kaduk diceritakan babag sing mistis dening wong tuwané. Pulo Bali kaharan singid amargi saban dinten wargi Bali rajin sembahyang lan menghaturkan banten.'}"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(bali_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_short = load_from_disk(\"dataset/paralel_3_lang/filtered_paralel_dataset_90k_short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "banned_words = [\"referensi\", \"pranala luar\", \"lihat juga\", \"lihat pula\", \"rujukan\", \"catatan kaki\", \"pranala\", \"kota\", \"kecamatan\", \"kelurahan\", \"kabupaten\", \"negara\", \"provinsi\", \"desa\", \"adalah\", \"merujuk\"]\n",
    "\n",
    "def filter_id_text(data):\n",
    "    for word in banned_words:\n",
    "        if word in data[\"indonesian\"].lower():\n",
    "            return None\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 93748/93748 [00:04<00:00, 20814.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "clean_short = filter_short.filter(filter_id_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'cirebonese', 'indonesian', 'balinese'],\n",
       "    num_rows: 1714\n",
       "})"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'e4e2ec19-1ae4-4eae-bd49-490dd01368fc',\n",
       " 'cirebonese': 'Jujuluk Natakusuma, sěkěrět jujuluk kepangeranan ingkang gage atêwêk adêg Amangkurat II ning Kesultanan Mataram, Jawa. Natakusuma, nama asli Paku Alam I.',\n",
       " 'indonesian': 'Gelar Natakusuma, sebuah gelar kepangeranan yang dimulai sejak pemerintahan Amangkurat II di Kesultanan Mataram, Jawa.\\n Natakusuma, nama asli Paku Alam I.',\n",
       " 'balinese': 'Desak Natakusuma, abulih desak kepangeranan sane ngametuang sasukate pemerintahan Amangkurat II ring Kesultanan Mataram, Jawa. Natakusuma, aran asli Paku Alam I.'}"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(clean_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1714/1714 [00:00<00:00, 108252.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "clean_short.save_to_disk(\"dataset/paralel_3_lang/filtered_paralel_dataset_1k_short_nowiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_dictionary(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "bali_indo_dict = load_dictionary(\"dict/bali_idn.json\")\n",
    "indo_bali_dict = load_dictionary(\"dict/idn_bali.json\")\n",
    "cbn_indo_dict = load_dictionary(\"dict/cbn_idn.json\")\n",
    "indo_cbn_dict = load_dictionary(\"dict/idn_cbn.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def extract_symbols(word):\n",
    "    # Extract prefix and suffix symbols\n",
    "    prefix = re.match(r'^[^\\w\\'`êÊ]*', word).group(0)\n",
    "    suffix = re.search(r'[^\\w\\'`êÊ]*$', word).group(0)\n",
    "    return prefix, suffix\n",
    "\n",
    "def clean_word(word):\n",
    "    # Keep alphanumeric, apostrophes, backticks and ê\n",
    "    cleaned = re.sub(r'[^\\w\\'`êÊ]', '', word)\n",
    "    return cleaned\n",
    "\n",
    "def correct_and_analyze_text(text, to_idn_dict, from_idn_dict):\n",
    "    # Split by whitespace but keep the separators\n",
    "    words_with_spaces = re.split(r'(\\s+)', text)\n",
    "    corrected_words = []\n",
    "    valid_count = 0\n",
    "    corrected_count = 0\n",
    "    correction_dict = {}\n",
    "\n",
    "    for word in words_with_spaces:\n",
    "        # If it's just whitespace, preserve it\n",
    "        if word.isspace():\n",
    "            corrected_words.append(word)\n",
    "            continue\n",
    "\n",
    "        original_word = word\n",
    "        prefix, suffix = extract_symbols(word)\n",
    "        cleaned_word = clean_word(word)\n",
    "        \n",
    "        # Check if all characters are lowercase\n",
    "        if cleaned_word and all(c.islower() for c in cleaned_word):\n",
    "            if cleaned_word in to_idn_dict:\n",
    "                corrected_words.append(original_word)\n",
    "                valid_count += 1\n",
    "            else:\n",
    "                translations = from_idn_dict.get(cleaned_word, None)\n",
    "                if translations:\n",
    "                    replacement = translations[0] if len(translations) == 1 else random.choice(translations)\n",
    "                    # Add back the symbols to the translation\n",
    "                    replacement = prefix + replacement + suffix\n",
    "                    corrected_words.append(replacement)\n",
    "                    corrected_count += 1\n",
    "                    correction_dict[original_word] = replacement\n",
    "                else:\n",
    "                    corrected_words.append(original_word)\n",
    "        else:\n",
    "            corrected_words.append(original_word)\n",
    "            valid_count += 1\n",
    "    \n",
    "    # Count only non-whitespace elements for statistics\n",
    "    total_words = sum(1 for w in words_with_spaces if not w.isspace())\n",
    "    invalid_count = total_words - valid_count - corrected_count\n",
    "    \n",
    "    return {\n",
    "        'corrected_text': ''.join(corrected_words),\n",
    "        'valid_percentage': (valid_count / total_words) * 100,\n",
    "        'invalid_percentage': (invalid_count / total_words) * 100,\n",
    "        'corrected_percentage': (corrected_count / total_words) * 100,\n",
    "        'corrections': correction_dict\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_and_analyze_text_ban_dataset(data):\n",
    "    result = correct_and_analyze_text(\n",
    "        data['balinese'],\n",
    "        bali_indo_dict,\n",
    "        indo_bali_dict\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'balinese': data['balinese'].strip(),\n",
    "        'indonesian': data['indonesian'].strip(),\n",
    "        'balinese_corrected': result['corrected_text'],\n",
    "        'valid_percentage': result['valid_percentage'],\n",
    "        'invalid_percentage': result['invalid_percentage'],\n",
    "        'corrected_percentage': result['corrected_percentage'],\n",
    "        'corrections': list(result['corrections'].items()),\n",
    "    }\n",
    "\n",
    "def correct_and_analyze_text_cbn_dataset(data):\n",
    "    result = correct_and_analyze_text(\n",
    "        data['cirebonese'],\n",
    "        cbn_indo_dict,\n",
    "        indo_cbn_dict\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'cirebonese': data['cirebonese'].strip(),\n",
    "        'indonesian': data['indonesian'].strip(),\n",
    "        'cirebonese_corrected': result['corrected_text'],\n",
    "        'valid_percentage': result['valid_percentage'],\n",
    "        'invalid_percentage': result['invalid_percentage'],\n",
    "        'corrected_percentage': result['corrected_percentage'],\n",
    "        'corrections': list(result['corrections'].items()),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = clean_short.shuffle(seed=42).select(range(600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = sample.select(range(300))\n",
    "sample_2 = sample.select(range(300, 600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 7103.81 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 6936.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ban_sample = sample_2.map(correct_and_analyze_text_ban_dataset, remove_columns=['cirebonese'])\n",
    "cbn_sample = sample_1.map(correct_and_analyze_text_cbn_dataset, remove_columns=['balinese'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'f90d59f1-cfe1-4297-a65f-0c6f2c2d7256',\n",
       " 'indonesian': 'Pintu air (pelayaran), perangkat untuk mengatur kedalaman pada alur pelayaran.\\n Pintu air (floodgate), perangkat untuk mengontrol aliran air di sungai, kanal, atau waduk.\\n Sluis, perangkat untuk mengontrol tinggi aliran air di sungai atau kanal.',\n",
       " 'balinese': 'Apes yeh (pelayaran), prajuru buat ngatur kedalaman ri celocoh pelayaran. Apes yeh (gembok banjir), prajuru buat ngontrol kecoran yeh ring tukad, kanal, utawi waduk. Sluis, prajuru buat ngontrol ganggas kecoran yeh ring tukad utawi kanal.',\n",
       " 'balinese_corrected': 'Apes yeh (pelayaran), prajuru buat ngatur kedalaman ri celocoh pelayaran. Apes yeh (gembok blabar), prajuru buat ngontrol kecoran yeh ring tukad, kanal, utawi waduk. Sluis, prajuru buat ngontrol ganggas kecoran yeh ring tukad utawi kanal.',\n",
       " 'valid_percentage': 77.14285714285715,\n",
       " 'invalid_percentage': 20.0,\n",
       " 'corrected_percentage': 2.857142857142857,\n",
       " 'corrections': [['banjir),', 'blabar),']]}"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(ban_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '6dcb330f-fde0-4513-a748-6956c63757e9',\n",
       " 'cirebonese': 'Orde Anyar bisa ngacu ing pirang-pirang bab ing ngandhap: \\n\\n Orde Anyar, têngêr dum mangsa imamat Presiden Nusantara ke-2 Soeharto. \\n Orde Anyar, order politik ning Jerman Nazi.',\n",
       " 'indonesian': 'Orde Baru dapat mengacu pada beberapa hal berikut:\\n\\n Orde Baru, sebutan bagi masa pemerintahan Presiden Indonesia ke-2 Soeharto.\\n Orde Baru, tatanan politik di Jerman Nazi.',\n",
       " 'cirebonese_corrected': 'Orde Anyar bisa ngacu ing pirang-pirang bab ing ngandhap: Orde Anyar, têngêr dum mangsa imamat Presiden Nusantara ke-2 Soeharto. Orde Anyar, order politik ning Jerman Nazi.',\n",
       " 'valid_percentage': 88.46153846153845,\n",
       " 'invalid_percentage': 11.538461538461538,\n",
       " 'corrected_percentage': 0.0,\n",
       " 'corrections': []}"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(cbn_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_csv(dataset, output_file):\n",
    "    # Convert corrections list of tuples to dictionary string format\n",
    "    def format_corrections(corrections):\n",
    "        return '\\n'.join(f\"{orig} -> {corr}\" for orig, corr in corrections)\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "    # Format the corrections column\n",
    "    if 'corrections' in df.columns:\n",
    "        df['corrections'] = df['corrections'].apply(format_corrections)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Dataset saved to {output_file}\")\n",
    "    print(f\"Total rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to dataset/annotation/ban_sample_300/ban.csv\n",
      "Total rows: 300\n",
      "Dataset saved to dataset/annotation/cbn_sample_300/cbn.csv\n",
      "Total rows: 300\n"
     ]
    }
   ],
   "source": [
    "dataset_to_csv(ban_sample, \"dataset/annotation/ban_sample_300/ban.csv\")\n",
    "dataset_to_csv(cbn_sample, \"dataset/annotation/cbn_sample_300/cbn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 300/300 [00:00<00:00, 66572.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 300/300 [00:00<00:00, 74441.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ban_sample.save_to_disk(\"dataset/annotation/ban_sample_300\")\n",
    "cbn_sample.save_to_disk(\"dataset/annotation/cbn_sample_300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Previous High-Quality Balinese Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee440bc7c6047edac3972334de00f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8869337dfd429483c668d8bcb2e317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a5a50ec1b14bd2af84359a5171267b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "udhr-lid.csv:   0%|          | 0.00/7.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bb4f0ae22f41f8872a5d52bb3c9bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/27757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "dset = datasets.load_dataset(\"cis-lmu/udhr-lid\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f150f5a557194d948e15004eb8522dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/27757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_udhr = dset['test'].filter(lambda x: x['iso639-3'] == 'ban')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b0094d070a47f4b0277404de7ff2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_udhr.save_to_disk(\"dataset/bali_hq/bali_udhr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9dd5998689d4a44b6a9256e1e090949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/47.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96abc4acd0d8469f9587b03fc0c1265b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.tsv:   0%|          | 0.00/114k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cb743dc6f54dfb8ee7129fdc149d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dev.tsv:   0%|          | 0.00/15.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c89c28faf4476b9d091f1a142031bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.tsv:   0%|          | 0.00/33.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ca6786062846b58ef130392156c7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e339156cf72491ba87ea9f20491c1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2705634e9ac4c13808433aa96c42efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_sib200 = load_dataset(\"Davlan/sib200\", \"ban_Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index_id', 'category', 'text'],\n",
       "    num_rows: 701\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bali_sib200[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579dc1538ba3482da51f52fca9f8199d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/701 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_sib200[\"train\"].save_to_disk(\"dataset/bali_hq/bali_sib200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1778f5b039a4e7ba2b4bddf0d6df968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/48.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029814398714449c8d552428f81c7d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00000-of-00001.arrow:   0%|          | 0.00/8.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5026356f31642eb88e02894398c8723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_glot500 = dataset = load_dataset('cis-lmu/Glot500', 'ban_Latn', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b90bf4778c4a819327e2ea05593279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/48958 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_glot500.save_to_disk(\"dataset/bali_hq/bali_glot500\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e52b9566a44018889df4bb2f08bbbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ban_clean_0000.jsonl.gz:   0%|          | 0.00/1.19M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8a763825f54fdabce984b5b15f1a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ban_noisy_0000.jsonl.gz:   0%|          | 0.00/12.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f444e539f448038fc4e5f3bbe3e0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating clean split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce58c2903f8454e9668918efd6f37d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating noisy split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_madlad = load_dataset(\"allenai/madlad-400\", \"ban\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8962147d9c4fc6a3566d4a5f697532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/637 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_madlad[\"clean\"].save_to_disk(\"dataset/bali_hq/bali_madlad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba2728d38a04c49a94ee470b70c37d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ban_Latn.json.zst:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4f807ac5ff498ea9b60897947c72d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_nllb = load_dataset(\"acul3/KoPI-NLLB\", \"ban_Latn-neardup\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b268850c854edca4bf2e435b730b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/244545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_nllb[\"train\"].save_to_disk(\"dataset/bali_hq/bali_nllb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8929b86d8207475797e02994247eaa6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"dataset/bali_hq/id-ban.tsv\", index_col=0, sep=\"\\t\")\n",
    "\n",
    "df = df.dropna()\n",
    "df['Balinese'] = df['Balinese'].astype(str)\n",
    "# Extract only the balinese column and convert to dict format\n",
    "balinese_data = {\n",
    "    'text': df['Balinese'].tolist()\n",
    "}\n",
    "\n",
    "# Convert to HuggingFace dataset\n",
    "balinese_dataset = Dataset.from_dict(balinese_data)\n",
    "\n",
    "# Save to disk\n",
    "balinese_dataset.save_to_disk(\"dataset/bali_hq/bali_indonmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 20611\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balinese_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "bali_indonmt = load_from_disk(\"dataset/bali_hq/bali_indonmt\")\n",
    "bali_nusax = load_from_disk(\"dataset/bali_hq/bali_nusax\")\n",
    "bali_wiki = load_from_disk(\"dataset/bali_hq/bali_wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset(data):\n",
    "    data[\"text\"] = data[\"sentence\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bfdf829eb046ed8359adb2627468a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bali_udhr = bali_udhr.map(convert_dataset, remove_columns=[\"sentence\", \"id\", \"iso639-3\", \"iso15924\", \"language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 60\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bali_udhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 20611\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bali_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "all_bali_hq = concatenate_datasets([bali_udhr, bali_sib200, bali_glot500, bali_madlad, bali_nllb, bali_indonmt, bali_nusax, bali_wiki])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bali_hq = all_bali_hq.remove_columns(['index_id', 'category', 'dataset', 'script', 'lang_script', 'url', 'score', 'source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf9094ce89a44bc8b6cc505068ba761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/280246 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_bali_hq.save_to_disk(\"dataset/bali_hq/all_bali_hq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_bali_text(data):\n",
    "    ban_sentences = data[\"text\"].split(\"\\n\")\n",
    "    \n",
    "    ban_first_line = ban_sentences[0]\n",
    "    ban_first_pred = model.predict(ban_first_line)[0][0]\n",
    "    ban_verdict = False\n",
    "    if len(ban_sentences) == 1:\n",
    "      if ban_first_pred == \"__label__ban_Latn\":\n",
    "          ban_verdict = True\n",
    "    else:\n",
    "      ban_second_line = ban_sentences[1]\n",
    "      ban_second_pred = model.predict(ban_second_line)[0][0]\n",
    "      if ban_first_pred == \"__label__ban_Latn\" or ban_second_pred == \"__label__ban_Latn\":\n",
    "          ban_verdict = True\n",
    "\n",
    "    if ban_verdict:\n",
    "        return data\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bali_hq_dedup = load_from_disk(\"dataset/bali_hq/all_bali_hq_dedup\")\n",
    "all_bali_hq_clean = all_bali_hq_dedup.filter(filter_bali_text, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11075340"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens_in_dataset(all_bali_hq_clean, \"text\", num_tokens_from_string, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 201404/201404 [00:00<00:00, 207678.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "all_bali_hq_clean.save_to_disk(\"dataset/bali_hq/all_bali_hq_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 6026/6026 [00:00<00:00, 26282.14 examples/s]\n",
      "Generating validation split: 100%|██████████| 335/335 [00:00<00:00, 78690.18 examples/s]\n",
      "Generating test split: 100%|██████████| 335/335 [00:00<00:00, 77974.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# or for a specific language\n",
    "bali_ift = load_dataset(\"akoksal/muri-it-language-split\", \"ban\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 6026/6026 [00:00<00:00, 603449.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "bali_ift =  bali_ift[\"train\"]\n",
    "bali_ift.save_to_disk(\"dataset/bali_ift/bali_muri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bali_clean = load_from_disk(\"dataset/bali_hq/all_bali_hq_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English High-Quality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_wiki = load_from_disk(\"dataset/en_hq/en_wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5615855"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens_in_dataset(en_wiki, \"text\", num_tokens_from_string, encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cirebonese High-Quality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haznitrama/.venv-cuda124/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import datasets\n",
    "import os\n",
    "\n",
    "def is_valid_content(text):\n",
    "    # Check if content is valid based on criteria\n",
    "    if text.strip().startswith(\"The image\"):\n",
    "        return False\n",
    "    tokens = text.split()\n",
    "    return len(tokens) > 10\n",
    "\n",
    "def gather_corpus():\n",
    "    # Get base path\n",
    "    base_path = os.path.expanduser(\"~/Cirebonese/corpus/\")\n",
    "    \n",
    "    texts = []\n",
    "    titles = []\n",
    "    \n",
    "    # Walk through the directory\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        # Skip the dict subdirectory\n",
    "        if \"dict\" in root:\n",
    "            continue\n",
    "            \n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = Path(root) / file\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read().strip()\n",
    "                        \n",
    "                    if is_valid_content(content):\n",
    "                        texts.append(content)\n",
    "                        titles.append(file.replace(\".txt\", \"\"))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file}: {str(e)}\")\n",
    "\n",
    "    # Create dataset dictionary\n",
    "    dataset_dict = {\n",
    "        \"text\": texts,\n",
    "        \"title\": titles\n",
    "    }\n",
    "    \n",
    "    # Convert to HuggingFace dataset\n",
    "    dataset = datasets.Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid documents: 2121\n",
      "\n",
      "First few examples:\n",
      "{'text': ['Tugas Kelompok Bahasa Cirebon\\n\\n          “Drama Bahasa Cirebon”\\n\\n\\n\\nNama Anggota :\\n \\uf076 Alifia Tasya Rizkiani (03)\\n \\uf076 Farhan Primanata (15)\\n \\uf076 Faydita Laila Qodrish (16)\\n \\uf076 Riyan Dwi Julianto (40)\\n\\n\\n\\n\\n                    Kelas : 9C\\n\\n\\n\\n               SMPN 2 CIREBON\\n\\x0c                   “Sederekan”\\nAdegan 1\\nSuasana rame sesampune ngedamel soal kang kari dinten niki.\\nUAS sampun pragat. Lare-lare seneng saged ngerjanang soale\\nangsal sae. Ta,mpi kecuali Rais sareng rencang-rencange.\\n\\x0cKabehane seneng sesampune lepas teng buku sing nyibukaken\\nkiyambeke akhir pekan seniki. Sesampune bel muni kiyambek\\nkesusu melajeng teng kelas.\\nSelma : Sae’e kepripun sesampune selese tugas-tugas akhir\\nsekolah niki?\\nNurul : Kula sareng rencang-rencang perean mawon nggih?\\nPripun sareng serencang sejene?\\nErina : Bade perean teng pundi, rul? Kula milet mawon.\\nNurul : Kepripun, sampeyan boten bosen teng griya mawon?\\nSelma : Kepripun upami rencang-rencang kesah teng dusun\\nmawon?\\nIfa : Nah! Niku ide sing sae! Kula setuju liburan teng dusun\\nIntan : Setuju!\\nUna : Sae pisan! Tapi sampeyan milet kan, is?\\nRais : Nggih kula milet. Sampeyan milet boten, gar?\\nEnggar      : Milet laah..\\nSesampune sepakat, rencang-rencang wangsul teng griyae\\nmasing-masing.\\n\\n\\n\\n\\nAdegan 2\\nDinten niki penerimaan rapot munggahan kelas. Sekabeh murid\\nnonggoni hasil rapot. Sesampune pragat lare-lare kumpul teng\\ngriyae Enggar.\\nIntan : Kok Rais boten dugi-dugi ya? Sampun lami nonggonane\\nIfa : Iya,.. teng pundi ya si Rais? Coba di telepon\\n\\x0cSelma : (nelpon Rais) Duuuh boten aktif\\nUna : Wonten napa sareng Rais ya?\\nNurul : Kepripun upami kula teng griyae mawon?\\nEnggar       : Eh, niki kula angsal sms saking Rais\\nIfa : Mriki kula kang bacaken (ngrebut hp Enggar)\\n“Maaf rencang-rencang kula boten saged dugi teng griyae\\nEnggar”\\nUna : Pripun sih Rais?\\nNurul : Mengkin wonten napa-napa sareng Rais? Kita dugi\\nmawon teng griyae mawon yu..\\nAdegan 3\\nLare-lare sampun dugi teng griyae Rais. Dereng sempet\\nmelajeng, Rais sampun kedah teng teras griyae sareng murung.\\nEnggar       : Lah, sampeyan punapa, is?\\nRais : Boten napa-napa (senyum terpaksa)\\nSelma : Rais, upami wonten masalah dongen sareng kita-kita,\\nwonten punapa?\\nRais : Kula boten lulus mata pelajaran Biology, kula mboten\\nsaged ngerjai soal-soale, sampeyan saget bantu mboten?\\n(nunjukaken kesedihan)\\nIfa : Kita-kita pasti bantu, is. Mengkin kita saged belajar\\nbareng-bareng\\nIntan : Nggih, is. Mengkin mawon nggih, boten saniki, saniki\\nsampun sonten\\nEnggar       : Sampeyan aja ngomong mboten saged, pasti\\nsaged, is! (sembari nepuk pundak rais)\\nUna : Sampun, aja sedih malih ya, is..\\nAdegan 4\\n\\x0cSekiki maninge, sampun mlebet dinten kesetunggal perean.\\nLare-lare bantu Rais belajar Biology teng griyae Rais.\\nRais : (Cengar-cengir)\\nSelma : Wonten napa, is? Cengar-cengir kiyambek mengkoten?\\n(sambil gemuyu)\\nRais : Boten napa-napa, kula seneng gadah rencang kaya kalian\\nNurul : Kula sampun nganggep Rais kaya sederek kula mawon,\\nis..\\nIfa : Kita sampun lami sederekan, jadi susah seneng bareng-\\nbareng\\nIntan : Nah.. leres tuh, fa..\\nSelma : Sederekan ge perlu pengorbanan\\nNurul : Boten cinta mawon sing butuh pengorbanan, nggih Sel\\n(sambil gemuyu)\\nUna : Hei... sampeyan-sampeyan siweg punapa? Sing mau\\nngobrol mawon. Deleng tuh si Enggar ngedamel soale\\nkiyambekan.\\nEnggar     : Kula mah tiyang paling rajin (senyum\\nngebanggaaken diri)\\nUna : huuuuu.. kula ge rajin\\nIntan : Sampun.. sampun.. ayo kita muali maning belajare!\\nMengkin kedaluan\\nIfa : Nggih, teras kula saged bantu napa, is?\\nRais : Kula gadah buku soal-soal Biology, kalian mengkin bantu\\nkula ngedamel soal-soale\\nIfa : Iyaa.. pasti, Is\\nEnggar     : Ya sampun, ayo kita damel bareng-bareng\\n\\x0cSuasanae hening ning tempat kiyambeke, kabehane siweg\\nserius ngedamel soal-soal biology, jam sampun nunjuken pukul\\n13.00 WIB, sampun cekap nganggo mereka belajar bareng.\\nAdegan 5\\nDinten Senen Rais kesah teng sekolah kanggo ngedamel tugas\\nremedial biology teng Bu guru. Rencang-rencang sejene\\nnonggoni Rais teng griyae Enggar.\\nSesampune pragat ngedamel tugas biology, Rais kesah teng\\ngriyae Enggar\\nRais : Rencang-rencaang...... kula berhasil (sambil loncat-\\nloncat seneng)\\nErina : Alhamdulillah, syukurlah, Is!\\nSelma : Selamat ya, Is!\\nEnggar      : Leres boten, upami Rais saged usaha pasti\\nberhasil!\\nRais : Nggih.. kesuwun ya\\nEnggar      : Iya sami-sami, Is, kita milet seneng..\\nSelma : Eh, kepripun upami kita ngerayaaken mawon sareng\\nliburan teng dusun?\\nNurul : Yeeeeee.... pasti seru pisan ya upami kita saged liburan\\nteng dusun bebarengan..\\nRais : Nggih.. angsal kula setuju mawon\\nEnggar      : Keprupun upami kita kesah teng pegunungan\\nmawon?\\nUna : Ide bagus, nggar!\\nNurul : Emangnya kita bade punapa teng pegunungan? Mending\\nkita teng Waterboom..\\nIntan : Nggih.. kula lebih seneng dolanan banyu teng niku\\nIfa : Teng waterboom krihin teran mengkin kita jalan-jalan\\n\\x0cteng dusun\\nSelma : Leres, Fa. Asiiik... dados kita saged teng dusun kih?\\nNurul : Ya nggih lah... emangnya sampeyan bade teng pundi,\\nSel? (nanya menyelidik)\\nSelma : Ya nggih, kula milet kalian liburan, emang bade teng\\npundi? (njawab polos)\\nNurul : Nggih..nggih...nggih... (sambil gemuyu)\\nErina : Sampun, dados kapan kesahe?\\nIfa : Mengkin enjing, kepripun rencang-rencang?\\nIntan : Waah.. lamun mengkin enjing, kula mboten saged, Fa.\\nKula bade kesah sareng mimi. Dinten Rabo mawon kepripun?\\nSelma & Nurul      : Saged!\\nErina : Dinten Rabo? Saged mawon..\\nRais : Kula mawon saged dinten Rabo, kepripun Fa? Saged\\nmboten?\\nIfa : Nggih, saged.. kapanpun kula mah saged. Sampeyan-\\nsampeyan saged kan? (nanya teng Enggar sareng Una)\\nEnggar     : Saged, saged..\\nUna : Nggih saged.\\nRais : Nggih sampun, kula bade wangsul seniki\\nNurul : Oke upami mekoten, kita wangsul krihin ya nggar\\nIfa : Iya nggar, sampun sonten\\nEnggar     : Hati-hati teng jalane, aja klalen dugi malih\\nmengkin dinten Rebo ya..\\nSelma : Nggih, kesuwun ya, sampe ketemu malih\\nEnggar     : Nggih sami-sami (sambil senyum teng rencang-\\nrencange)\\nAdegan 6\\n\\x0cDinten Rabo, sesampune nyiapnang rasukan lan sejene, mereka\\nsiap-siap kanggo kesah teng Waterboom.\\nNurul : Ayoo kita rencang-rencang mlebet..\\nUna : Mengkin tunggu krihin, Rul. Sing sejene masih ning buri.\\nRais : Hey.... tonggoaken kula! Barange abot kih..\\nIfa : Aduuh, Rais.. Sampeyan mbakta napa mawon? (sembari\\ngemuyu)\\nNurul : Iya, Is! Tase ageng pisan (melu gemuyu)\\nSelma : Mbakta rasukan selemar, Is? (melu gemuyu)\\nEnggar      : Duuh, sampun.. sampun.. ayo mlebet!\\nMereka mlebet teng waterboom teras kiyambek ganti rasukan.\\nErina : Duh rame pisan ya..\\nIntan : Leres.. mboten kalah sareng ning kota\\nIfa : Padahal tempat niki masih anyar, dereng tersohor\\nRais : Heh.. kalian siweg punama malih? Ayo dolanaan..\\nUna : Ya sampun, yoo kita dolanan\\nSelami teng niku mereka gemuyu lan sumringah. Mboten klelen\\nkiyambeke foto-foto. Kiyambeke sampun lungse teras\\nninggalaken dolanane. Teras mampir teng warung dahar,\\nsampun rampung mereka nerasaken malih pelampahane. Kangge\\nnikmati pemandangan teng dusun.\\nIfa : Duh pegel ya, sedintenan dolanan mawon teng toya lan\\nsekiyen ngirup udara seger sareng nikmati pemandangan kaya\\nniki. (sembari menghela napas)\\nNurul : Leres, Fa. Kita foto-foto yu?\\nSelma : Sae..sae.. kita musti abadikan kanggo kenang-kenangan\\nErina : Nah, leres tuh..\\nEnggar      : Ya sok, kalian teng riku kula saged motoaken\\nkalian\\n\\x0cRais : Mileeet.. (Gabung teng sampinge Nurul)\\nEnggar       : Na, Ntan, milet mboten?\\nSelma : Mriki, ntan.. aja malu-malu\\nNurul : Duh si Una, ayo biasanya ge sampeyan narsis upami\\ndifoto\\nUna : (dengan terpaksa gabung teng golongan Nurul) Kula\\npegel, Rul..\\nErina : (narik tangan Intan) Ayolah ntaan..\\nEnggar       : 1, 2, 3 (moto rencang-rencange)\\nNurul : Sampun? Deleeeng.. (Ngrebut kamera sing tangan\\nEnggar)\\nEh bagus pisaan, malih yu foto-foto teng rika.. (sembari\\nmlampah)\\nIfa, Selma, Erina, Intan milet teng burie Nurul. Rais, Enggar\\nsareng Una duduk teng bawah pohon sembari mandengi\\nrencang-rencang perempuannya.\\nEnggar       : Kula mboten klalen dinten niki\\nRais : Sami, nggar. Kita beruntung gadah sahabat kaya\\nmereka\\nUna : Mereka sae-sae sareng kita\\nRais : Leres, Na. Kita mboten salah dadosaken mereka\\nsederek kita, mereka emang yang paling sae\\nNurul, Ifa,, Selma, Erina sareng Intang gabung teng Rais, Una\\nsareng Enggar.\\nIfa : Hayoo ngomongin punapa mau?\\nRais : Mboten ngobrol punapa-punapa, nggih kan nggar?\\n(sembari senyum teng Enggar)\\nEnggar       : Nggih mboten napa-napa\\nNurul : Pasti ngomongin kita-kita ya? (sembari gemuyu)\\n\\x0cSelma : hayoo...\\nUna : Sampeyan sampeyan niku ‘geer’ pisan ya\\nErina : ‘geer’ niku lebih sae dibanding minder, Na..\\nIntan : Sampun.. sampun.. mending saniki kita wangsul mawon,\\nmengkin kedaluan\\nUna : Leres, Ntan.. yu wangsul..\\nMereka wangsul mawon teng griyae masing-masing.', 'PEMERINTAH KABUPATEN CIREBON\\n                                         DINAS PENDIDIKAN\\n                      SMP SATU ATAP NEGERI LOSARI\\n                               NSS. 201021703005                                  NPSN: 20258349\\n                       Jl. Pulosari Desa Tawangsari Kecamatan Losari Kabupaten Cirebon 45192 Tlp. (0321) 3626559\\n\\n\\n                                  NASKAH SOAL UJIAN SEKOLAH\\n                                       TAHUN PELAJARAN 2013/2014                                                   B\\n                               Mata Pelajaran            :   Bahasa Cirebon                                   199\\n                               Hari, Tanggal             :   Sabtu, 29 Maret 2014\\n                               Kelas                     :   IX (Sembilan)\\n                               Waktu                     :   07.30 – 09.00\\n                                                                                                              1\\n       PETUNJUK UMUM\\n1. Perhatikan petunjuk pengisian pada lembar jawaban yang disediakan\\n2. Periksa dan bacalah soal dengan teliti sebelum menjawab\\n3. Jawaban dikerjakan di LJK\\n4. Gunakan Pencil 2B\\n5. Laporkan pada pengawas jika terdapat tulisan pada soal yang kurang jelas/rusak\\n6. LJK jangan sampai sobek, terlipat, kotor, atau basah\\n7. Jawab soal-soal yang paling mudah dulu\\n8. Periksa kembali kembali hasil pekerjaanmu sebelum diserahkan kepada pengawas\\nI. Pilihlah salah satu jawaban yang paling benar\\n   (A, B, C, atau D)\\n1.    Apa nami alat tradisional kangge damel                           a.   Whu melinjo\\n      gethek niku ....                                                 b.   Akeh wit tangkil\\n      a. Cantang                                                       c.   Nggon nabatake jaran\\n      b. Canting                                                       d.   Gedokan jaran\\n      c. Anjun\\n                                                                  7.   Aksara jawa kang urutan wolu yaiku ....\\n      d. Ani-ani\\n                                                                       a.\\n2.    Kalong cilik saba gedhang, sumedhot rasa\\n      ning ati.                                                        b.\\n      Wangsalan iku artie ...\\n      a. Curut                                                         c.\\n      b. Codot\\n      c. Agel                                                          d.\\n      d. Bedul\\n                                                                  8.   Kang Warsida bade teng sabin, tembung\\n3.    Lagu cerbon bisa anggo nada apa ....\\n                                                                       kang artie ....\\n      a. Pentatonis\\n                                                                       a. Perkara\\n      b. Mesosopran\\n                                                                       b. Lan\\n      c. Pelok bae\\n                                                                       c. Kakang\\n      d. Salendro bae\\n                                                                       d. Sing\\n4.    Wite kacang arane ....\\n                                                                  9.   Lare-lare sekolah (barengan) olahraga.\\n      a. Tebon\\n                                                                       (barengan) tembung dwi purwane ....\\n      b. Pulang\\n                                                                       a. Babarengan\\n      c. Ruyang\\n                                                                       b. Bebarengan\\n      d. Rendeng\\n                                                                       c. Bebareng\\n5.    Taman jendral sinten kota cerbon diarani                         d. Bareng-bareng\\n      grade ...\\n                                                                  10. Teng surat pribadi pundi kang disebut\\n      a. Vander Pellas\\n                                                                      titimangsa, yaiku ....\\n      b. Jons Peter Sunkun\\n                                                                      a. Hormat kami\\n      c. Raples\\n                                                                      b. Asalamualaikum wr.wb\\n      d. Danddeles\\n                                                                      c. Katur\\n6.    Wonten desa tangkil sebelah kerisidenan                             Kangge kang Warsida\\n      cerbon napa artine tangkil ....                                     Teng surabaya\\n                                                                      d. Cirebon, 20 oktober 2012\\n\\n\\n                                                              1\\n     Bahasa Cirebon P-B 199                                                                UJIAN SEKOLAH 2014\\n\\x0c11. Hormat wong tua, guru lan ratu, kalimat                a.   Perintah\\n    niki pesan saking ....                                 b.   Pekarepan\\n                                                           c.   Perintah langsung\\n    a.   Sunan Gunung Jati                                 d.   Perintah bli langsung\\n    b.   Sunan Kalijaga\\n                                                       21. Anake sapi arane ....\\n    c.   Sunan Ampel\\n                                                           a. Pedet\\n    d.   Sunan Muria\\n                                                           b. Bledug\\n12. Kesenian punapa kang diengge ngangge                   c. Gudhel\\n    ngormati tamu-tamu teng kraton ....                    d. suwiyah\\n    a. Sampyong\\n                                                       22. Labuh molah sawah disebut ....\\n    b. Berokan\\n                                                           a. Mluko\\n    c. Tayuban\\n                                                           b. Nyebar\\n    d. Wayang kulit\\n                                                           c. Pacekruk\\n13. Pas jaman kemerdhekaan tetabuhan kang                  d. Macul\\n    masih jejek ana ning endi ....\\n                                                       23. Semelung-melunge gong, masih melung\\n    a. Sanggar\\n                                                           omongane wong artie ....\\n    b. Keraton\\n                                                           a. Omongane aja diladeni\\n    c. Padepokan\\n                                                           b. Omongane wong kudu dipercaya\\n    d. Pesantren\\n                                                           c. Omongane wong teka maning\\n14. Ana macem-maceme tetabuhan             terus           d. Omongane wong gelis nyiar ning endi-\\n    direka-reka dadi kesenian apa ...                         endi\\n    a. Riyam\\n                                                       24. Nggeraaken sesuatu       ngangge   mbuang\\n    b. Ruyam\\n                                                           kotoran niku nopo ....\\n    c. Campuran\\n                                                           a. Tawurji\\n    d. Caruban\\n                                                           b. Ngirab\\n15. Sauwise aran melodi kota udang sekiyen                 c. Ngapem\\n    dadi apa ....                                          d. Muludan\\n    a. Musik pesisiran\\n                                                       25. Arti pribasa “ Gedhe prahu, gedhe\\n    b. Musik ayu\\n                                                           ombake” yaiku ....\\n    c. Musik gambang kromo\\n                                                           a. Gedhe pengeluaran, gedhe penghasilan\\n    d. Musik tarling\\n                                                           b. Gedhe penghasilan, gedhe pengeluaran\\n16. Wangsalane niku kedah wonten ....                      c. Serakah\\n    a. Guritan                                             d. Wong gedhe musuhe wong cilik\\n    b. Wangsulan\\n                                                       26. Arti sewalike “ duwur kencur” yaiku ....\\n    c. Terusane\\n                                                           a. Pendhek pisan\\n    d. Sampirane\\n                                                           b. Duwur pisan\\n17. Kang kesebut pokok surat yaiku ....                    c. Binyek letrek\\n    a. Isi                                                 d. Akeh sing ngenteni\\n    b. Penutup\\n                                                       27. Datasawala, aksara jawane ....\\n    c. Bebuka\\n                                                           a.\\n    d. Rangka surat\\n18. Punapa arane pelabuhan cerbon niku ....                b.\\n    a. Tanjung perak\\n    b. Tanjung mas                                         c.\\n    c. Muara jati\\n    d. Tanjung priuk                                       d.\\n19. Prabotan utawa kang digawe sing lemah\\n                                                       28. Dolanan bocah kang            nganggo      alat\\n    lempung dibakar niku pengertian saking ....\\n                                                           carang/kayu cilik yaiku ...\\n    a. Gethek\\n                                                           a. Slodor\\n    b. Gethuk\\n                                                           b. Sluku-sluku batok\\n    c. Gethok\\n                                                           c. Glatikan\\n    d. Gethak\\n                                                           d. Ceciwi\\n20. Hazna mboten purun dahar.\\n                                                       29. Aksara jawa carakan jumlahe wonten ....\\n    Saking contoh kalimat         teng    inggil\\n    ngrupiaken kalimat.\\n\\n                                                   2\\n   Bahasa Cirebon P-B 199                                                     UJIAN SEKOLAH 2014\\n\\x0c    a.   18 aksara                                 38. Tiyang cerbon gadah tradisi tiap sasi safar\\n    b.   28 aksara                                     damel ....\\n    c.   38 aksara                                     a. Apem\\n    d.   20 aksara                                     b. Serabi\\n                                                       c. Sega kuning\\n30. Teng surat pribadi pundi kang disebut\\n                                                       d. Bubur abang putih\\n    kepala surat ....\\n    a. Hormat kami                                 39. Teng andhap niki pundi kang termasuk\\n    b. Asalamualaikum wr.wb                            contoh saking tembung dwi lingga.\\n    c. Cirebon, 20 oktober 2013                        a. Lare-lare\\n    d. Katur                                           b. Bapak-ibu\\n       Kangge kang Kamid                               c. Tetabuhan\\n       Teng                                            d. Adem ayem\\n       Jakarta\\n                                                   40. Apa kang dimaksud ukara majemuk ....\\n31. Saling tukar pikiran kangge medalaken              a. Perintah\\n    masalah yaiku ....                                 b. Ukara loro dadi siji\\n    a. Tukaran                                         c. Permintaan\\n    b. Gunemar                                         d. Tanya\\n    c. Diskusi\\n                                                   41. Pepindhan asale saking tembung ....\\n    d. Cerita\\n                                                       a. Pepin\\n32. Lagu warung pojok. Karyane ....                    b. Indhan\\n    a. Suratno Marta Atmaja                            c. Pindhe\\n    b. H. Abdul Ajid                                   d. pindhahan\\n    c. Kuntring\\n                                                   42. Maknae manungsa kedah ngertos marahe\\n    d. Hj. Dariyah\\n                                                       watake kiyambek yaiku ...\\n33. Gula abang                                         a. Kodhok ngemuli lenga\\n    Gula pasir                                         b. Luruwa galihe kangkung\\n    Klambi abang,                                      c. Tapak kuntul\\n    Akeh sing naksir                                   d. Botok bolu ayame kate\\n    Kalimat teng inggil kelebet.\\n                                                   43. Tiyang kang tugase mandu acara kesebut\\n    a. Parikan\\n                                                       ....\\n    b. Wangsalan\\n                                                       a. Pewawancara\\n    c. Pupuh\\n                                                       b. Wartawan\\n    d. Jawokan\\n                                                       c. Decorator\\n34. Lukisan kaca kang dikenal wonten 2 jenis           d. Pembawa acara\\n    nggih niku ....\\n                                                   44. Mimi siweg damel batik tulis teng griya,\\n    a. Lukisan batik lan kaligrafi\\n                                                       griya padane ....\\n    b. Kaligrafi lan gambar wayang\\n                                                       a. Umah\\n    c. Gambar wayang lan kendaraan\\n                                                       b. Gubug\\n    d. Gambar alam lan bintang\\n                                                       c. Arepan\\n35. Kalimat sedina-dina diengge ....                   d. Toko\\n    a. Guru ngobrol sareng wong tua\\n                                                   45. Tuku batik ning trusmi akeh maceme,\\n    b. Murid ngobrol sareng wong tua\\n                                                       bebasane ....\\n    c. Guru ngobrol sareng murid\\n                                                       a. Tuku ning trusmi batik katah macame\\n    d. Pembantu ngobrol sareng majikan\\n                                                       b. Tuku batik ning trusmi katah maceme\\n36. Karya seni/produk seni batik mega                  c. Tumbas batik teng trusmi akeh macame\\n    mendung, satunggal khas ciri batik ...             d. Tumbas batik teng trusmi katah\\n    a. Pekalongan                                         maceme\\n    b. Solo\\n    c. Cirebon\\n    d. Surakarta\\n37. Nembang doa tawurji artie ...\\n    a. Adus ning sumur\\n    b. Selamet panjang umur\\n    c. Sodakoe bari manjur\\n    d. Penyakite padha kawur\\n\\n\\n                                               3\\n   Bahasa Cirebon P-B 199                                                UJIAN SEKOLAH 2014\\n\\x0cII. Essai\\n\\n\\n46. Jalan sing endi sampai maring endi kang\\n    dibangun oleh Danddeles !\\n47. Cobi sebutken langkah-langkah saking\\n    layang pribados yaiku!\\n48. Salin ning tulisan biasa\\n\\n\\n\\n\\n49. Damel      silsilah     keturunan   didamel\\n    organigram sing awit anak sampe grephak\\n    senther!\\n50. Lanang, wadon, gedhe, cilik, tuwa, enom\\n    pada nonton wayang. Bebasane ....\\n\\n\\n\\n\\n                                                  4\\n   Bahasa Cirebon P-B 199                             UJIAN SEKOLAH 2014'], 'title': ['tugas-Bahasa-Cirebon-1-docx', 'US-Bahasa-Cirebon-2014']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2121/2121 [00:00<00:00, 137602.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "dataset = gather_corpus()\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Total valid documents: {len(dataset)}\")\n",
    "print(\"\\nFirst few examples:\")\n",
    "print(dataset[:2])\n",
    "\n",
    "# Save the dataset (optional)\n",
    "dataset.save_to_disk(\"cirebonese_corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "\n",
    "# load the model\n",
    "model = fasttext.load_model(\"./models/glotlid/model.bin\")\n",
    "\n",
    "class CustomLID:\n",
    "    def __init__(self, model_path, languages = -1, mode='before'):\n",
    "        self.model = fasttext.load_model(model_path)\n",
    "        self.output_matrix = self.model.get_output_matrix()\n",
    "        self.labels = self.model.get_labels()\n",
    "        \n",
    "        # compute language_indices\n",
    "        if languages !=-1 and isinstance(languages, list):\n",
    "            self.language_indices = [self.labels.index(l) for l in list(set(languages)) if l in self.labels]\n",
    "\n",
    "        else:\n",
    "            self.language_indices = list(range(len(self.labels)))\n",
    "\n",
    "        # limit labels to language_indices\n",
    "        self.labels = list(np.array(self.labels)[self.language_indices])\n",
    "        \n",
    "        # predict\n",
    "        self.predict = self.predict_limit_after_softmax if mode=='after' else self.predict_limit_before_softmax\n",
    "\n",
    "    \n",
    "    def predict_limit_before_softmax(self, text, k=1):\n",
    "        \n",
    "        # sentence vector\n",
    "        sentence_vector = self.model.get_sentence_vector(text)\n",
    "        \n",
    "        # dot\n",
    "        result_vector = np.dot(self.output_matrix[self.language_indices, :], sentence_vector)\n",
    "\n",
    "        # softmax\n",
    "        softmax_result = np.exp(result_vector - np.max(result_vector)) / np.sum(np.exp(result_vector - np.max(result_vector)))\n",
    "\n",
    "        # top k predictions\n",
    "        top_k_indices = np.argsort(softmax_result)[-k:][::-1]\n",
    "        top_k_labels = [self.labels[i] for i in top_k_indices]\n",
    "        top_k_probs = softmax_result[top_k_indices]\n",
    "\n",
    "        return tuple(top_k_labels), top_k_probs\n",
    "\n",
    "\n",
    "    def predict_limit_after_softmax(self, text, k=1):\n",
    "        \n",
    "        # sentence vector\n",
    "        sentence_vector = self.model.get_sentence_vector(text)\n",
    "        \n",
    "        # dot\n",
    "        result_vector = np.dot(self.output_matrix, sentence_vector)\n",
    "\n",
    "        # softmax\n",
    "        softmax_result = np.exp(result_vector - np.max(result_vector)) / np.sum(np.exp(result_vector - np.max(result_vector)))\n",
    "\n",
    "        # limit softmax to language_indices\n",
    "        softmax_result = softmax_result[self.language_indices]\n",
    "\n",
    "        \n",
    "        # top k predictions\n",
    "        top_k_indices = np.argsort(softmax_result)[-k:][::-1]\n",
    "        top_k_labels = [self.labels[i] for i in top_k_indices]\n",
    "        top_k_probs = softmax_result[top_k_indices]\n",
    "\n",
    "        return tuple(top_k_labels), top_k_probs\n",
    "\n",
    "# to make sure these languages are available in GlotLID check the list of supported labels in model.labels\n",
    "limited_languages = ['__label__ind_Latn', '__label__sun_Latn', '__label__jav_Latn', '__label__ban_Latn']\n",
    "\n",
    "model = CustomLID(\"./models/glotlid/model.bin\", languages = limited_languages , mode='before')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_non_id_text(data):\n",
    "    sentences = data[\"indonesian\"].split(\"\\n\")\n",
    "    first_line = sentences[0]\n",
    "    first_pred = model.predict(first_line)[0][0]\n",
    "    if len(sentences) == 1:\n",
    "      if first_pred == \"__label__ind_Latn\":\n",
    "          return None\n",
    "      else:\n",
    "          return data\n",
    "    else:\n",
    "      second_line = sentences[1]\n",
    "      second_pred = model.predict(second_line)[0][0]\n",
    "      if first_pred == \"__label__ind_Latn\" and second_pred == \"__label__ind_Latn\":\n",
    "          return None\n",
    "      else:\n",
    "          return data\n",
    "\n",
    "def filter_id_text(data):\n",
    "    sentences = data[\"indonesian\"].split(\"\\n\")\n",
    "    first_line = sentences[0]\n",
    "    first_pred = model.predict(first_line)[0][0]\n",
    "    if len(sentences) == 1:\n",
    "      if first_pred == \"__label__ind_Latn\":\n",
    "          return data\n",
    "      else:\n",
    "          return None\n",
    "    else:\n",
    "      second_line = sentences[1]\n",
    "      second_pred = model.predict(second_line)[0][0]\n",
    "      if first_pred == \"__label__ind_Latn\" and second_pred == \"__label__ind_Latn\":\n",
    "          return data\n",
    "      else:\n",
    "          return None\n",
    "\n",
    "def filter_id_text_2(data):\n",
    "    sentences = data[\"text\"].split(\"\\n\")\n",
    "    first_line = sentences[0]\n",
    "    first_pred = model.predict(first_line)[0][0]\n",
    "    if len(sentences) == 1:\n",
    "      if first_pred == \"__label__ind_Latn\":\n",
    "          return data\n",
    "      else:\n",
    "          return None\n",
    "    else:\n",
    "      second_line = sentences[1]\n",
    "      second_pred = model.predict(second_line)[0][0]\n",
    "      if first_pred == \"__label__ind_Latn\" and second_pred == \"__label__ind_Latn\":\n",
    "          return data\n",
    "      else:\n",
    "          return None\n",
    "\n",
    "def filter_all_text(data):\n",
    "    cbn_sentences = data[\"cirebonese\"].split(\"\\n\")\n",
    "    ban_sentences = data[\"balinese\"].split(\"\\n\")\n",
    "    \n",
    "    cbn_first_line = cbn_sentences[0]\n",
    "    cbn_first_pred = model.predict(cbn_first_line)[0][0]\n",
    "    cbn_verdict = False\n",
    "    if len(cbn_sentences) == 1:\n",
    "      if cbn_first_pred == \"__label__jav_Latn\":\n",
    "          cbn_verdict = True\n",
    "    else:\n",
    "      cbn_second_line = cbn_sentences[1]\n",
    "      cbn_second_pred = model.predict(cbn_second_line)[0][0]\n",
    "      if cbn_first_pred == \"__label__jav_Latn\" or cbn_second_pred == \"__label__jav_Latn\":\n",
    "          cbn_verdict = True\n",
    "\n",
    "    ban_first_line = ban_sentences[0]\n",
    "    ban_first_pred = model.predict(ban_first_line)[0][0]\n",
    "    ban_verdict = False\n",
    "    if len(ban_sentences) == 1:\n",
    "      if ban_first_pred == \"__label__ban\":\n",
    "          ban_verdict = True\n",
    "    else:\n",
    "      ban_second_line = ban_sentences[1]\n",
    "      ban_second_pred = model.predict(ban_second_line)[0][0]\n",
    "      if ban_first_pred == \"__label__ban\" or ban_second_pred == \"__label__ban\":\n",
    "          ban_verdict = True\n",
    "\n",
    "    if cbn_verdict and ban_verdict:\n",
    "        return data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def filter_bali_text(data):\n",
    "    ban_sentences = data[\"text\"].split(\"\\n\")\n",
    "    \n",
    "    ban_first_line = ban_sentences[0]\n",
    "    ban_first_pred = model.predict(ban_first_line)[0][0]\n",
    "    ban_verdict = False\n",
    "    if len(ban_sentences) == 1:\n",
    "      if ban_first_pred == \"__label__ban\":\n",
    "          ban_verdict = True\n",
    "    else:\n",
    "      ban_second_line = ban_sentences[1]\n",
    "      ban_second_pred = model.predict(ban_second_line)[0][0]\n",
    "      if ban_first_pred == \"__label__ban\" or ban_second_pred == \"__label__ban\":\n",
    "          ban_verdict = True\n",
    "\n",
    "    if ban_verdict:\n",
    "        return data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def filter_cbn_text(data):\n",
    "    cbn_sentences = data[\"text\"].split(\"\\n\")\n",
    "    \n",
    "    cbn_first_line = cbn_sentences[0]\n",
    "    cbn_first_pred = model.predict(cbn_first_line)[0][0]\n",
    "    cbn_verdict = False\n",
    "    if len(cbn_sentences) == 1:\n",
    "      if cbn_first_pred == \"__label__jav_Latn\" or cbn_first_pred == \"__label__ind_Latn\":\n",
    "          cbn_verdict = True\n",
    "    else:\n",
    "      cbn_second_line = cbn_sentences[1]\n",
    "      cbn_second_pred = model.predict(cbn_second_line)[0][0]\n",
    "      if cbn_first_pred == \"__label__jav_Latn\" or cbn_first_pred == \"__label__ind_Latn\" or cbn_second_pred == \"__label__jav_Latn\" or cbn_second_pred == \"__label__ind_Latn\":\n",
    "          cbn_verdict = True\n",
    "\n",
    "    if cbn_verdict:\n",
    "        return data\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=8): 100%|██████████| 2117/2117 [00:00<00:00, 5793.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "all_cbn_hq_dedup = load_from_disk(\"dataset/cbn_hq/all_cbn_hq_dedup\")\n",
    "all_cbn_hq_clean = all_cbn_hq_dedup.filter(filter_cbn_text, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1246039"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens_in_dataset(all_cbn_hq_clean, \"text\", num_tokens_from_string, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2105/2105 [00:00<00:00, 61783.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "all_cbn_hq_clean.save_to_disk(\"dataset/cbn_hq/all_cbn_hq_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\"Maapkan aku guru, kau pilih kasih kenapa ajian tapak cecak itu kau wariskan pada musuh, bukan pada ku.\"\\n\\n\"Ah, masalah itu rupanya, Wuluh..., itu sudah suratan takdir sang hyang widhi.\"\\n\\n\"Aku tidak peduli, yang jelas kami datang menginginkan jiwamu.\"\\n\\n\"Kau benar-benar murid murtad, Niluh Seroja tak pantas jadi anakmu.\"\\n\\n\"Jangan bawa-bawa anak durhaka itu, karena dialah pemuda murid Sunan Jati Purba itu mewarisi ajian tapak cecak.\"\\n\\n\"Murid murtad nan licik aku menyesal jadi gurumu.\"\\n\\n\"Tua bangka terima ajalmu...!\"\\n\\nKembali pemuda tegap berikat kepala merah lentingkan badannya ke udara dan dari sepuluh jari nya membersit racun mematikan warangan temiang geni, Resi Maruta mandra kebut lengan jubahnya, serangkum angin membuyarka serpihan debu mematikan tersebut.\\n\\n\"Siapa kau Kisanak, apa hubunganmu dengan Resi sesat Mahendra Thabita?\"\\n\\nPemuda tegap yang dipanggil Jungjungan oleh Wuluh Balang cuma menyeringai.\\n\\n\"Dengar orang tua, agar kematianmu tak penasaran aku adalah jungjungan para Warok hutan Sinang perbukitan Loyang, sudah puas dengan keteranganku.\"\\n\\n\"Ah ternyata kau orangnya, yang menggegerkan Kedaton Pakungwati.\"',\n",
       " 'title': 'kesatria-kesultanan-cirebon_page-0256'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(all_cbn_hq_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2105"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_cbn_hq_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2105/2105 [00:00<00:00, 16499.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "all_cbn_hq_clean.save_to_disk(\"dataset/cpt/cbn_hq_2k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Tokenizer Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haznitrama/scale-resources/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# 1. Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"./models/Bali/instruct/BaliQwen-3B-base_HQ-instruct_en\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 2. Load dataset\n",
    "dataset = load_from_disk(\"./dataset/bali_ift/bali_muri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'dataset_name', 'subdataset_name', 'language', 'split', 'language_name'],\n",
       "    num_rows: 6026\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 6026/6026 [00:00<00:00, 24405.81 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of formatted data:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Sesuratan puniki nyihnayang samian rumah sakit ring Provinsi Sulawesi Utara, Indonesia.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Puniki suratan indik lis rumah sakit ring propinsi Sulawesi Utara, Indonésia sané kawagi manut wewidangan kabupatén miwah kota.\n",
      "\n",
      "Pustaka\n",
      "\n",
      "Pranala jaba \n",
      "  Sistem Informasi Rumah Sakit (SIRS) Kementerian Kesehatan RI \n",
      "  Perhimpunan Rumah Sakit Seluruh Indonesia (PERSI) \n",
      "\n",
      "Rumah sakit ring Sulawesi Utara\n",
      "Sulawesi Utara\n",
      "Sulawesi Utara<|im_end|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def format_chat(example, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example[\"input\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"output\"]}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Apply formatting to dataset\n",
    "formatted_dataset = dataset.map(\n",
    "    lambda x: format_chat(x, tokenizer),\n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=8\n",
    ")\n",
    "\n",
    "# Show example\n",
    "print(\"Example of formatted data:\")\n",
    "print(formatted_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nSesuratan puniki nyihnayang samian rumah sakit ring Provinsi Sulawesi Utara, Indonesia.<|im_end|>\\n<|im_start|>assistant\\nPuniki suratan indik lis rumah sakit ring propinsi Sulawesi Utara, Indonésia sané kawagi manut wewidangan kabupatén miwah kota.\\n\\nPustaka\\n\\nPranala jaba \\n  Sistem Informasi Rumah Sakit (SIRS) Kementerian Kesehatan RI \\n  Perhimpunan Rumah Sakit Seluruh Indonesia (PERSI) \\n\\nRumah sakit ring Sulawesi Utara\\nSulawesi Utara\\nSulawesi Utara<|im_end|>\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "--------------------------------------------------\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Sesuratan puniki nyihnayang samian rumah sakit ring Provinsi Sulawesi Utara, Indonesia.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Puniki suratan indik lis rumah sakit ring propinsi Sulawesi Utara, Indonésia sané kawagi manut wewidangan kabupatén miwah kota.\n",
      "\n",
      "Pustaka\n",
      "\n",
      "Pranala jaba \n",
      "  Sistem Informasi Rumah Sakit (SIRS) Kementerian Kesehatan RI \n",
      "  Perhimpunan Rumah Sakit Seluruh Indonesia (PERSI) \n",
      "\n",
      "Rumah sakit ring Sulawesi Utara\n",
      "Sulawesi Utara\n",
      "Sulawesi Utara<|im_end|>\n",
      "\n",
      "\n",
      "Decoded text:\n",
      "--------------------------------------------------\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Sesuratan puniki nyihnayang samian rumah sakit ring Provinsi Sulawesi Utara, Indonesia.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Puniki suratan indik lis rumah sakit ring propinsi Sulawesi Utara, Indonésia sané kawagi manut wewidangan kabupatén miwah kota.\n",
      "\n",
      "Pustaka\n",
      "\n",
      "Pranala jaba \n",
      "  Sistem Informasi Rumah Sakit (SIRS) Kementerian Kesehatan RI \n",
      "  Perhimpunan Rumah Sakit Seluruh Indonesia (PERSI) \n",
      "\n",
      "Rumah sakit ring Sulawesi Utara\n",
      "Sulawesi Utara\n",
      "Sulawesi Utara<|im_end|>\n",
      "\n",
      "\n",
      "Length of tokens: 181\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check tokenizer encode-decode on formatted dataset\n",
    "def check_tokenization(example, max_length=8192):\n",
    "    # Encode the text\n",
    "    encoded = tokenizer.encode(\n",
    "        example[\"text\"], \n",
    "        max_length=max_length, \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Decode back to text\n",
    "    decoded = tokenizer.decode(encoded, skip_special_tokens=False)\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"Original text:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(example[\"text\"])\n",
    "    print(\"\\nDecoded text:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(decoded)\n",
    "    print(\"\\nLength of tokens:\", len(encoded))\n",
    "    print(\"-\" * 50)\n",
    "    return len(encoded)\n",
    "\n",
    "# Check first example\n",
    "token_length = check_tokenization(formatted_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 6026/6026 [00:00<00:00, 204066.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "formatted_dataset.save_to_disk(\"./dataset/ift/bali_ift_6k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Data Based on Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset = load_from_disk(\"dataset/paralel_3_lang/combined_paralel_dataset_705k_dedup_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'cirebonese', 'indonesian', 'balinese'],\n",
       "    num_rows: 491113\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_id_text(data):\n",
    "    sentences = [s for s in data[\"indonesian\"].split(\"\\n\") if s.strip()]\n",
    "    if not sentences:\n",
    "        return None\n",
    "\n",
    "    # Predict top label for each sentence\n",
    "    preds = [model.predict(s)[0][0] for s in sentences]\n",
    "    ind_count = sum(1 for p in preds if p == \"__label__ind_Latn\")\n",
    "\n",
    "    if len(sentences) == 1:\n",
    "        return data if ind_count == 1 else None\n",
    "    else:\n",
    "        # Require strict majority of sentences to be Indonesian\n",
    "        return data if ind_count > (len(sentences) / 2) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=8): 100%|██████████| 491113/491113 [01:34<00:00, 5190.18 examples/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_id_1 = all_dataset.filter(filter_id_text, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'cirebonese', 'indonesian', 'balinese'],\n",
       "    num_rows: 490291\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_id_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (6/6 shards): 100%|██████████| 490291/490291 [00:05<00:00, 81859.61 examples/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_id_1.save_to_disk(\"dataset/paralel_3_lang/combined_paralel_dataset_705k_dedup_clean_filtered-id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cirebonese_only columns: ['id', 'cirebonese', 'indonesian']\n",
      "balinese_only columns: ['id', 'indonesian', 'balinese']\n"
     ]
    }
   ],
   "source": [
    "# Split filtered_id_1 into two datasets\n",
    "# - cirebonese: drop the 'balinese' column\n",
    "# - balinese: drop the 'cirebonese' column\n",
    "\n",
    "cirebonese_only = filtered_id_1.remove_columns([c for c in [\"balinese\"] if c in filtered_id_1.column_names])\n",
    "balinese_only = filtered_id_1.remove_columns([c for c in [\"cirebonese\"] if c in filtered_id_1.column_names])\n",
    "\n",
    "print(\"cirebonese_only columns:\", cirebonese_only.column_names)\n",
    "print(\"balinese_only columns:\", balinese_only.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balinese valid percentage (mean on first 5000): 48.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 490291/490291 [00:40<00:00, 12195.79 examples/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balinese valid percentage (mean, full): 48.26%\n"
     ]
    }
   ],
   "source": [
    "# Compute Balinese valid percentage metric using dictionary-based matching\n",
    "# Re-uses the approach from annotated_data_analysis: token match against Balinese dictionary\n",
    "\n",
    "import json, re\n",
    "import numpy as np\n",
    "\n",
    "# Load Balinese word set from dictionary keys\n",
    "# The JSON is expected to be a mapping where keys are Balinese tokens\n",
    "with open(\"dict/bali_idn.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    _bali_dict_data = json.load(f)\n",
    "BALINESE_WORD_SET = set(k.lower() for k in _bali_dict_data.keys())\n",
    "\n",
    "# Optional: load KBBI wordlist if you want to treat non-KBBI tokens as valid too\n",
    "# from annotated_data_analysis, we can skip for now to keep it fast\n",
    "KBBI_WORDLIST = None  # or set to a Python set of Indonesian tokens to use the alternate rule\n",
    "\n",
    "_word_regex = re.compile(r\"\\b\\w+\\b\", flags=re.UNICODE)\n",
    "\n",
    "def calculate_valid_percentage(text: str, dictionary_words: set, kbbi_wordlist: set | None = None) -> float:\n",
    "    if text is None:\n",
    "        return 0.0\n",
    "    text = str(text)\n",
    "    if not text.strip():\n",
    "        return 0.0\n",
    "    words = _word_regex.findall(text.lower())\n",
    "    total_words = len(words)\n",
    "    if total_words == 0:\n",
    "        return 0.0\n",
    "\n",
    "    valid_words = 0\n",
    "    for w in words:\n",
    "        if w in dictionary_words:\n",
    "            valid_words += 1\n",
    "        elif kbbi_wordlist is not None:\n",
    "            # Treat as valid if not in KBBI and not in dictionary\n",
    "            if w not in kbbi_wordlist:\n",
    "                valid_words += 1\n",
    "\n",
    "    return (valid_words / total_words) * 100.0\n",
    "\n",
    "# Batched helper (picklable and efficient per batch)\n",
    "\n",
    "def add_valid_percentage_batch(batch):\n",
    "    texts = batch.get(\"balinese\", [])\n",
    "    return {\n",
    "        \"valid_percentage\": [\n",
    "            calculate_valid_percentage(t, BALINESE_WORD_SET, KBBI_WORDLIST) for t in texts\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Quick sample-based mean (fast smoke check)\n",
    "sample_size = min(5000, len(balinese_only))\n",
    "sample = balinese_only.select(range(sample_size))\n",
    "sample_metrics = sample.map(add_valid_percentage_batch, batched=True)\n",
    "mean_valid_pct_sample = float(np.mean(sample_metrics[\"valid_percentage\"]))\n",
    "print(f\"Balinese valid percentage (mean on first {sample_size}): {mean_valid_pct_sample:.2f}%\")\n",
    "\n",
    "# To process the full dataset and attach the column, uncomment below (may take a while):\n",
    "balinese_with_metrics = balinese_only.map(add_valid_percentage_batch, batched=True)\n",
    "mean_valid_pct = float(np.mean(balinese_with_metrics[\"valid_percentage\"]))\n",
    "print(f\"Balinese valid percentage (mean, full): {mean_valid_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 490291/490291 [00:51<00:00, 9542.99 examples/s] \n",
      "Filter: 100%|██████████| 490291/490291 [00:51<00:00, 9542.99 examples/s] \n",
      "Filter: 100%|██████████| 490291/490291 [00:55<00:00, 8912.75 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts:\n",
      "  good (>=70): 226950\n",
      "  bad  (<70): 263341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Split Balinese data by valid_percentage threshold\n",
    "# good: valid_percentage >= 70\n",
    "# bad:  valid_percentage < 70\n",
    "\n",
    "def is_good(example):\n",
    "    try:\n",
    "        return float(example.get(\"valid_percentage\", 0.0)) >= 50.0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def is_bad(example):\n",
    "    try:\n",
    "        return float(example.get(\"valid_percentage\", 0.0)) < 50.0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "balinese_good = balinese_with_metrics.filter(is_good)\n",
    "balinese_bad = balinese_with_metrics.filter(is_bad)\n",
    "\n",
    "print(\"Counts:\")\n",
    "print(\"  good (>=70):\", len(balinese_good))\n",
    "print(\"  bad  (<70):\", len(balinese_bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72045815"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens_in_dataset(balinese_good, \"balinese\", num_tokens_from_string, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239628839"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens_in_dataset(balinese_with_metrics, \"balinese\", num_tokens_from_string, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (2/2 shards): 100%|██████████| 226950/226950 [00:02<00:00, 82845.94 examples/s] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "balinese_good.save_to_disk(\"dataset/paralel_3_lang/balinese_annotation-filter_valid-pct_50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balinese BT valid match percentage (mean): 83.94%\n",
      "Balinese with BT good (>= 85.0): 241,408\n",
      "Balinese with BT bad  (< 85.0): 248,883\n"
     ]
    }
   ],
   "source": [
    "# Add BT valid match percentage for Balinese and split by threshold (mirrors Cirebonese in cell 147)\n",
    "import numpy as np\n",
    "\n",
    "# Use existing Balinese->Indonesian dictionary map prepared earlier\n",
    "# BAL_TO_ID_MAP should already be available from prior cells\n",
    "assert 'BAL_TO_ID_MAP' in globals(), \"BAL_TO_ID_MAP not found. Run the dictionary load cell first.\"\n",
    "assert 'balinese_only' in globals(), \"balinese_only dataset not found. Run the dataset split cell first.\"\n",
    "\n",
    "# Helper to add bt_valid_match_percentage to a batch\n",
    "# Relies on bt_valid_match_pct(balinese_text, indonesian_text, dict_map) defined earlier\n",
    "\n",
    "def add_bt_metric_batch_bal(batch):\n",
    "    bal_texts = batch.get('balinese', [])\n",
    "    id_texts = batch.get('indonesian', [])\n",
    "    scores = []\n",
    "    for bal, idt in zip(bal_texts, id_texts):\n",
    "        scores.append(bt_valid_match_pct(bal, idt, BAL_TO_ID_MAP))\n",
    "    return {\"bt_valid_match_percentage\": scores}\n",
    "\n",
    "# Compute metric across the full Balinese dataset (single-process for stability)\n",
    "base_ds = balinese_only\n",
    "balinese_with_bt = base_ds.map(\n",
    "    add_bt_metric_batch_bal,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=None,\n",
    "    desc=\"Computing BT valid match % for Balinese\"\n",
    ")\n",
    "\n",
    "# Quick summary\n",
    "mean_bt_bal = float(np.mean(balinese_with_bt[\"bt_valid_match_percentage\"])) if len(balinese_with_bt) > 0 else 0.0\n",
    "print(f\"Balinese BT valid match percentage (mean): {mean_bt_bal:.2f}%\")\n",
    "\n",
    "# Split into good/bad using same threshold style as Cirebonese\n",
    "THRESH_BT_BAL = 85.0\n",
    "\n",
    "def bal_is_good_bt(example):\n",
    "    return float(example.get(\"bt_valid_match_percentage\", 0.0)) >= THRESH_BT_BAL\n",
    "\n",
    "def bal_is_bad_bt(example):\n",
    "    return float(example.get(\"bt_valid_match_percentage\", 0.0)) < THRESH_BT_BAL\n",
    "\n",
    "bal_good_bt = balinese_with_bt.filter(bal_is_good_bt, num_proc=1)\n",
    "bal_bad_bt = balinese_with_bt.filter(bal_is_bad_bt, num_proc=1)\n",
    "\n",
    "print(f\"Balinese with BT good (>= {THRESH_BT_BAL}): {len(bal_good_bt):,}\")\n",
    "print(f\"Balinese with BT bad  (< {THRESH_BT_BAL}): {len(bal_bad_bt):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92759958"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens_in_dataset(bal_good_bt, \"balinese\", num_tokens_from_string, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (2/2 shards): 100%|██████████| 241408/241408 [00:04<00:00, 48328.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "bal_good_bt.save_to_disk(\"dataset/paralel_3_lang/balinese_annotation-filter_bt-valid-pct_85\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cirebonese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:01<00:00, 4852.71 examples/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BT valid match percentage (mean on first 5000): 81.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 490291/490291 [01:38<00:00, 4966.77 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BT valid match percentage (mean, full): 81.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Backtranslation valid match metric for Balinese\n",
    "# Reuses logic from annotated_data_analysis: for each Balinese token, check if any Indonesian translation\n",
    "# (from dictionary) appears in the reference Indonesian tokens.\n",
    "\n",
    "import json, re\n",
    "import numpy as np\n",
    "\n",
    "# Load Balinese->Indonesian dictionary map (lowercased keys and values)\n",
    "def load_dict_map(dictionary_path: str):\n",
    "    try:\n",
    "        with open(dictionary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_map = json.load(f)\n",
    "        dict_map = {}\n",
    "        for k, v in raw_map.items():\n",
    "            key = str(k).lower()\n",
    "            if isinstance(v, str):\n",
    "                dict_map[key] = [v.lower()]\n",
    "            elif isinstance(v, list):\n",
    "                dict_map[key] = [str(item).lower() for item in v if isinstance(item, str)]\n",
    "            else:\n",
    "                dict_map[key] = []\n",
    "        return dict_map\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dictionary: {e}\")\n",
    "        return {}\n",
    "\n",
    "CBN_TO_ID_MAP = load_dict_map(\"dict/cbn_idn.json\")\n",
    "_tokenize = re.compile(r\"\\b\\w+\\b\", flags=re.UNICODE).findall\n",
    "\n",
    "# Compute bt_valid_match_percentage for one pair of texts\n",
    "def bt_valid_match_pct(cirebonese_text: str, indonesian_text: str, dict_map: dict) -> float:\n",
    "    if cirebonese_text is None or indonesian_text is None:\n",
    "        return 0.0\n",
    "    cirebonese_text = str(cirebonese_text)\n",
    "    indonesian_text = str(indonesian_text)\n",
    "    if not cirebonese_text.strip() or not indonesian_text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    cirebonese_tokens = _tokenize(cirebonese_text.lower())\n",
    "    if not cirebonese_tokens:\n",
    "        return 0.0\n",
    "    id_tokens_set = set(_tokenize(indonesian_text.lower()))\n",
    "\n",
    "    valid = 0\n",
    "    for cirebonese_w in cirebonese_tokens:\n",
    "        translations = dict_map.get(cirebonese_w, [])\n",
    "        # if no dictionary translations, allow direct match fallback\n",
    "        if not translations:\n",
    "            translations = [cirebonese_w]\n",
    "        if any(t in id_tokens_set for t in translations):\n",
    "            valid += 1\n",
    "\n",
    "    return (valid / len(cirebonese_tokens)) * 100.0\n",
    "\n",
    "# Batched mapper to add metric\n",
    "\n",
    "def add_bt_metric_batch(batch):\n",
    "    cirebonese_texts = batch.get(\"cirebonese\", [])\n",
    "    id_texts = batch.get(\"indonesian\", [])\n",
    "    return {\n",
    "        \"bt_valid_match_percentage\": [\n",
    "            bt_valid_match_pct(b, i, CBN_TO_ID_MAP) for b, i in zip(cirebonese_texts, id_texts)\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Ensure we have a base dataset to attach to; default to cirebonese_only if cirebonese_with_metrics is missing\n",
    "try:\n",
    "    base_ds = cirebonese_with_metrics\n",
    "except NameError:\n",
    "    base_ds = cirebonese_only\n",
    "\n",
    "# Quick sample mean to verify\n",
    "sample_n = min(5000, len(base_ds))\n",
    "sample_ds = base_ds.select(range(sample_n))\n",
    "sample_bt = sample_ds.map(add_bt_metric_batch, batched=True)\n",
    "mean_bt_sample = float(np.mean(sample_bt[\"bt_valid_match_percentage\"]))\n",
    "print(f\"BT valid match percentage (mean on first {sample_n}): {mean_bt_sample:.2f}%\")\n",
    "\n",
    "# To compute on full dataset and attach the column, uncomment below (can take a while):\n",
    "base_ds = base_ds.map(add_bt_metric_batch, batched=True)\n",
    "print(f\"BT valid match percentage (mean, full): {float(np.mean(base_ds['bt_valid_match_percentage'])):.2f}%\")\n",
    "\n",
    "# Expose dataset with metric\n",
    "cirebonese_with_bt = base_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cirebonese BT split counts:\n",
      "  good (>=80): 299071\n",
      "  bad  (<80): 191220\n"
     ]
    }
   ],
   "source": [
    "# Split Cirebonese data by backtranslation valid match percentage threshold\n",
    "# good: bt_valid_match_percentage >= 80\n",
    "# bad:  bt_valid_match_percentage < 80\n",
    "\n",
    "THRESH_BT_CBN = 80\n",
    "\n",
    "\n",
    "def cbn_is_good_bt(example):\n",
    "    try:\n",
    "        return float(example.get(\"bt_valid_match_percentage\", 0.0)) >= THRESH_BT_CBN\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def cbn_is_bad_bt(example):\n",
    "    try:\n",
    "        return float(example.get(\"bt_valid_match_percentage\", 0.0)) < THRESH_BT_CBN\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Expect cirebonese_with_bt to already include 'bt_valid_match_percentage'\n",
    "cbn_good_bt = cirebonese_with_bt.filter(cbn_is_good_bt)\n",
    "cbn_bad_bt = cirebonese_with_bt.filter(cbn_is_bad_bt)\n",
    "\n",
    "print(\"Cirebonese BT split counts:\")\n",
    "print(\"  good (>=80):\", len(cbn_good_bt))\n",
    "print(\"  bad  (<80):\", len(cbn_bad_bt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77022579"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens_in_dataset(cbn_good_bt, \"cirebonese\", num_tokens_from_string, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (3/3 shards): 100%|██████████| 299071/299071 [00:05<00:00, 59766.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "cbn_good_bt.save_to_disk(\"dataset/paralel_3_lang/cirebonese_annotation-filter_bt-valid-pct_80\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIREBONESE VALID%] Computing valid percentage and filtering (threshold=50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing valid% for Cirebonese: 100%|██████████| 490291/490291 [00:47<00:00, 10284.39 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cirebonese valid percentage (mean): 46.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 490291/490291 [00:46<00:00, 10645.12 examples/s]\n",
      "Filter: 100%|██████████| 490291/490291 [00:46<00:00, 10645.12 examples/s]\n",
      "Filter: 100%|██████████| 490291/490291 [00:58<00:00, 8340.29 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cirebonese valid% good (>= 50.0): 187,433\n",
      "Cirebonese valid% bad  (< 50.0): 302,858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[CIREBONESE VALID%] Computing valid percentage and filtering (threshold=50)\")\n",
    "# Add valid percentage metric to Cirebonese and filter with threshold 50 (like Balinese in cell 140)\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Ensure required inputs are available\n",
    "assert 'cirebonese_only' in globals(), \"cirebonese_only dataset not found. Run the earlier split cell first.\"\n",
    "assert 'CBN_TO_ID_MAP' in globals(), \"CBN_TO_ID_MAP not found. Load the Cirebonese dictionary map first.\"\n",
    "\n",
    "# Build Cirebonese word set from dictionary keys (analogous to Balinese)\n",
    "CBN_WORD_SET = set(CBN_TO_ID_MAP.keys())\n",
    "\n",
    "# Use existing calculate_valid_percentage if present; otherwise define it\n",
    "def _calc_valid_pct_impl(text, dictionary_words, kbbi_wordlist=None):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", str(text).lower())\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    valid = 0\n",
    "    for tok in tokens:\n",
    "        if tok in dictionary_words:\n",
    "            if kbbi_wordlist is not None and tok in kbbi_wordlist:\n",
    "                # Optionally exclude Indonesian dictionary words\n",
    "                continue\n",
    "            valid += 1\n",
    "    return (valid / len(tokens)) * 100.0\n",
    "\n",
    "# Wrapper that prefers existing function if defined\n",
    "try:\n",
    "    calculate_valid_percentage  # type: ignore\n",
    "    _calc = calculate_valid_percentage  # reuse from Balinese metric\n",
    "except NameError:\n",
    "    _calc = _calc_valid_pct_impl\n",
    "\n",
    "# Batch mapper for Cirebonese\n",
    "\n",
    "def add_valid_percentage_batch_cbn(batch):\n",
    "    cbn_texts = batch.get('cirebonese', [])\n",
    "    scores = [_calc(txt, CBN_WORD_SET, KBBI_WORDLIST) for txt in cbn_texts]\n",
    "    return {\"valid_percentage\": scores}\n",
    "\n",
    "# Compute across the full dataset (single-process for notebook stability)\n",
    "base_ds = cirebonese_only\n",
    "cirebonese_with_metrics = base_ds.map(\n",
    "    add_valid_percentage_batch_cbn,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=None,\n",
    "    desc=\"Computing valid% for Cirebonese\"\n",
    ")\n",
    "\n",
    "# Summary\n",
    "mean_valid_pct_cbn = float(np.mean(cirebonese_with_metrics[\"valid_percentage\"])) if len(cirebonese_with_metrics) > 0 else 0.0\n",
    "print(f\"Cirebonese valid percentage (mean): {mean_valid_pct_cbn:.2f}%\")\n",
    "\n",
    "# Filter by threshold 50\n",
    "THRESH_VALID_CBN = 50.0\n",
    "\n",
    "def cbn_is_good_valid(example):\n",
    "    return float(example.get(\"valid_percentage\", 0.0)) >= THRESH_VALID_CBN\n",
    "\n",
    "def cbn_is_bad_valid(example):\n",
    "    return float(example.get(\"valid_percentage\", 0.0)) < THRESH_VALID_CBN\n",
    "\n",
    "cbn_good_valid = cirebonese_with_metrics.filter(cbn_is_good_valid, num_proc=1)\n",
    "cbn_bad_valid = cirebonese_with_metrics.filter(cbn_is_bad_valid, num_proc=1)\n",
    "\n",
    "print(f\"Cirebonese valid% good (>= {THRESH_VALID_CBN}): {len(cbn_good_valid):,}\")\n",
    "print(f\"Cirebonese valid% bad  (< {THRESH_VALID_CBN}): {len(cbn_bad_valid):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (2/2 shards): 100%|██████████| 187433/187433 [00:03<00:00, 52308.59 examples/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cbn_good_valid.save_to_disk(\"dataset/paralel_3_lang/cirebonese_annotation-filter_valid-pct_50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE DATASETS] Pruning columns to 'text' and saving under dataset/cpt ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 187433/187433 [00:01<00:00, 109327.29 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cbn_good_valid: 187,433 rows -> dataset/cpt/cbn_good_valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (2/2 shards): 100%|██████████| 299071/299071 [00:04<00:00, 69436.39 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cbn_good_bt: 299,071 rows -> dataset/cpt/cbn_good_bt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 226950/226950 [00:03<00:00, 62165.33 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved balinese_good: 226,950 rows -> dataset/cpt/balinese_good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 241408/241408 [00:03<00:00, 76124.04 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bal_good_bt: 241,408 rows -> dataset/cpt/bal_good_bt\n"
     ]
    }
   ],
   "source": [
    "print(\"[SAVE DATASETS] Pruning columns to 'text' and saving under dataset/cpt ...\")\n",
    "import os\n",
    "\n",
    "# Ensure required datasets exist\n",
    "required_vars = [\n",
    "    ('cbn_good_valid', 'cirebonese', 'cbn_good_valid'),\n",
    "    ('cbn_good_bt', 'cirebonese', 'cbn_good_bt'),\n",
    "    ('balinese_good', 'balinese', 'balinese_good'),\n",
    "    ('bal_good_bt', 'balinese', 'bal_good_bt'),\n",
    "]\n",
    "\n",
    "save_root = 'dataset/cpt'\n",
    "os.makedirs(save_root, exist_ok=True)\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "def process_and_save(ds, lang_col, out_name):\n",
    "    assert lang_col in ds.column_names, f\"Column '{lang_col}' not found in dataset '{out_name}'. Available: {ds.column_names}\"\n",
    "    keep_cols = [lang_col]\n",
    "    remove_cols = [c for c in ds.column_names if c not in keep_cols]\n",
    "    ds_pruned = ds.remove_columns(remove_cols) if remove_cols else ds\n",
    "    ds_text = ds_pruned.rename_column(lang_col, 'text') if lang_col != 'text' else ds_pruned\n",
    "    out_dir = os.path.join(save_root, out_name)\n",
    "    ds_text.save_to_disk(out_dir)\n",
    "    print(f\"Saved {out_name}: {len(ds_text):,} rows -> {out_dir}\")\n",
    "    return ds_text\n",
    "\n",
    "# Process all requested datasets\n",
    "_name_to_obj = globals()\n",
    "for var_name, lang_col, out_name in required_vars:\n",
    "    assert var_name in _name_to_obj, f\"Dataset variable '{var_name}' is not defined.\"\n",
    "    ds_obj = _name_to_obj[var_name]\n",
    "    process_and_save(ds_obj, lang_col, out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALIDATION SPLIT] Creating validation sets from HQ datasets (Balinese, Cirebonese)\n",
      "Balinese HQ size: 201,404 -> val: 5,000\n",
      "Cirebonese HQ size: 2,105 -> val: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 5000/5000 [00:00<00:00, 86217.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 500/500 [00:00<00:00, 47302.40 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Balinese validation -> dataset/cpt/bali_valid_hq_5000\n",
      "Saved Cirebonese validation -> dataset/cpt/cbn_valid_hq_500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[VALIDATION SPLIT] Creating validation sets from HQ datasets (Balinese, Cirebonese)\")\n",
    "import os\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Config: choose minimum validation sizes suitable for small LLM (causal LM) from scratch\n",
    "# Rationale: 1k-5k examples often suffice for stable validation; here we use 5k for large Bali HQ, 500 for smaller CBN HQ\n",
    "VAL_SIZE_BALI = 5_000\n",
    "VAL_SIZE_CBN = 500\n",
    "SAVE_DIR = \"dataset/cpt\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Helper to ensure a single 'text' column exists\n",
    "\n",
    "def normalize_to_text_column(ds):\n",
    "    cols = ds.column_names\n",
    "    if \"text\" in cols:\n",
    "        # If other columns exist, keep only 'text' to avoid leaking labels/extra fields\n",
    "        remove_cols = [c for c in cols if c != \"text\"]\n",
    "        return ds.remove_columns(remove_cols) if remove_cols else ds\n",
    "    # Try common language columns\n",
    "    preferred = [\"balinese\", \"cirebonese\", \"indonesian\"]\n",
    "    for c in preferred:\n",
    "        if c in cols:\n",
    "            keep = ds.remove_columns([x for x in cols if x != c]) if len(cols) > 1 else ds\n",
    "            return keep.rename_column(c, \"text\") if c != \"text\" else keep\n",
    "    # Fallback: if exactly one column, rename it to 'text'\n",
    "    if len(cols) == 1:\n",
    "        only = cols[0]\n",
    "        return ds.rename_column(only, \"text\") if only != \"text\" else ds\n",
    "    raise ValueError(f\"Cannot determine text column from columns: {cols}\")\n",
    "\n",
    "# Load HQ datasets\n",
    "bali_hq_path = os.path.join(SAVE_DIR, \"bali_hq_200k\")\n",
    "cbn_hq_path = os.path.join(SAVE_DIR, \"cbn_hq_2k\")\n",
    "\n",
    "bali_hq = load_from_disk(bali_hq_path)\n",
    "cbn_hq = load_from_disk(cbn_hq_path)\n",
    "\n",
    "# Normalize to 'text' and prune other columns\n",
    "bali_hq_text = normalize_to_text_column(bali_hq)\n",
    "cbn_hq_text = normalize_to_text_column(cbn_hq)\n",
    "\n",
    "# Decide actual validation sizes based on availability\n",
    "n_bali_val = min(VAL_SIZE_BALI, len(bali_hq_text))\n",
    "n_cbn_val = min(VAL_SIZE_CBN, len(cbn_hq_text))\n",
    "print(f\"Balinese HQ size: {len(bali_hq_text):,} -> val: {n_bali_val:,}\")\n",
    "print(f\"Cirebonese HQ size: {len(cbn_hq_text):,} -> val: {n_cbn_val:,}\")\n",
    "\n",
    "# Shuffle and take head for validation\n",
    "bali_val = bali_hq_text.shuffle(seed=42).select(range(n_bali_val))\n",
    "cbn_val = cbn_hq_text.shuffle(seed=42).select(range(n_cbn_val))\n",
    "\n",
    "# Save validation sets\n",
    "bali_val_dir = os.path.join(SAVE_DIR, f\"bali_valid_hq_{n_bali_val}\")\n",
    "cbn_val_dir = os.path.join(SAVE_DIR, f\"cbn_valid_hq_{n_cbn_val}\")\n",
    "\n",
    "# Overwrite behavior: allow saving anew by removing existing dirs if needed\n",
    "for p in [bali_val_dir, cbn_val_dir]:\n",
    "    if os.path.isdir(p):\n",
    "        # Clean existing to avoid Arrow dataset save errors\n",
    "        import shutil\n",
    "        shutil.rmtree(p)\n",
    "\n",
    "bali_val.save_to_disk(bali_val_dir)\n",
    "cbn_val.save_to_disk(cbn_val_dir)\n",
    "\n",
    "print(f\"Saved Balinese validation -> {bali_val_dir}\")\n",
    "print(f\"Saved Cirebonese validation -> {cbn_val_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268637"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens_in_dataset(bali_val, \"text\", num_tokens_from_string, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361418"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens_in_dataset(cbn_val, \"text\", num_tokens_from_string, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 201404/201404 [00:31<00:00, 6420.90 examples/s]\n",
      "Filter: 100%|██████████| 2105/2105 [00:00<00:00, 7036.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 196404/196404 [00:00<00:00, 198753.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1605/1605 [00:00<00:00, 37809.23 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bali_hq_total': 201404, 'bali_val_total': 5000, 'bali_hq_no_val': 196404, 'bali_saved_to': 'dataset/cpt/bali_hq_no_val'}\n",
      "{'cbn_hq_total': 2105, 'cbn_val_total': 500, 'cbn_hq_no_val': 1605, 'cbn_saved_to': 'dataset/cpt/cbn_hq_no_val'}\n"
     ]
    }
   ],
   "source": [
    "# Remove validation examples from HQ datasets and save cleaned HQ splits\n",
    "from datasets import Dataset\n",
    "\n",
    "# Helper to normalize to a single 'text' column\n",
    "def _to_text_only(ds: Dataset) -> Dataset:\n",
    "    cols = ds.column_names\n",
    "    if 'text' in cols:\n",
    "        return ds.remove_columns([c for c in cols if c != 'text']) if len(cols) > 1 else ds\n",
    "    for c in ('balinese', 'cirebonese', 'indonesian'):\n",
    "        if c in cols:\n",
    "            tmp = ds.remove_columns([x for x in cols if x != c]) if len(cols) > 1 else ds\n",
    "            return tmp.rename_column(c, 'text') if c != 'text' else tmp\n",
    "    # Fallback: single column dataset\n",
    "    if len(cols) == 1 and cols[0] != 'text':\n",
    "        return ds.rename_column(cols[0], 'text')\n",
    "    return ds\n",
    "\n",
    "# Ensure text columns\n",
    "_bali_hq_txt = _to_text_only(bali_hq)\n",
    "_bali_val_txt = _to_text_only(bali_val)\n",
    "_cbn_hq_txt = _to_text_only(cbn_hq)\n",
    "_cbn_val_txt = _to_text_only(cbn_val)\n",
    "\n",
    "# Build fast lookup sets of validation texts (stripped)\n",
    "_bali_val_set = set(s.strip() for s in _bali_val_txt['text'] if s is not None)\n",
    "_cbn_val_set = set(s.strip() for s in _cbn_val_txt['text'] if s is not None)\n",
    "\n",
    "# Filter HQ to exclude any row that appears in validation by exact text match\n",
    "bali_hq_no_val = _bali_hq_txt.filter(lambda x: x['text'] is not None and x['text'].strip() not in _bali_val_set)\n",
    "cbn_hq_no_val = _cbn_hq_txt.filter(lambda x: x['text'] is not None and x['text'].strip() not in _cbn_val_set)\n",
    "\n",
    "# Save cleaned HQ datasets\n",
    "bali_hq_no_val_dir = 'dataset/cpt/bali_hq_no_val'\n",
    "cbn_hq_no_val_dir = 'dataset/cpt/cbn_hq_no_val'\n",
    "\n",
    "bali_hq_no_val.save_to_disk(bali_hq_no_val_dir)\n",
    "cbn_hq_no_val.save_to_disk(cbn_hq_no_val_dir)\n",
    "\n",
    "print({\n",
    "    'bali_hq_total': len(_bali_hq_txt),\n",
    "    'bali_val_total': len(_bali_val_txt),\n",
    "    'bali_hq_no_val': len(bali_hq_no_val),\n",
    "    'bali_saved_to': bali_hq_no_val_dir,\n",
    "})\n",
    "print({\n",
    "    'cbn_hq_total': len(_cbn_hq_txt),\n",
    "    'cbn_val_total': len(_cbn_val_txt),\n",
    "    'cbn_hq_no_val': len(cbn_hq_no_val),\n",
    "    'cbn_saved_to': cbn_hq_no_val_dir,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
