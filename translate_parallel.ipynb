{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def llm_call(model, messages, **call_args):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        **call_args\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def create_batch_req_object(req_id, model, messages, response_format, temperature=0.0):\n",
    "    return {\n",
    "       \"custom_id\": req_id,\n",
    "       \"method\": \"POST\",\n",
    "       \"url\": \"/v1/chat/completions\",\n",
    "       \"body\": {\n",
    "          \"model\": model, \n",
    "          \"messages\": messages,\n",
    "          \"temperature\": temperature,\n",
    "          \"response_format\": response_format,\n",
    "        }\n",
    "    }\n",
    "\n",
    "def llm_batch_api(batch_filepath, purpose=\"\", desc=\"\", completion_window=\"24h\"):\n",
    "    batch_input_file = client.files.create(\n",
    "      file=open(batch_filepath, \"rb\"),\n",
    "      purpose=purpose\n",
    "    )\n",
    "\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "\n",
    "    batch_info = client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=completion_window,\n",
    "        metadata={\n",
    "          \"description\": desc\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return batch_info\n",
    "\n",
    "def llm_batch_check_retrieve(batch_info):\n",
    "    updated_batch = client.batches.retrieve(batch_info.id)\n",
    "    print(f\"Status of batch {updated_batch.id} is {updated_batch.status}\")\n",
    "    if updated_batch.status == \"completed\":\n",
    "      output_file = client.files.content(updated_batch.output_file_id)\n",
    "      return updated_batch, output_file\n",
    "    else:\n",
    "      return updated_batch, None\n",
    "\n",
    "def llm_batch_check_retrieve_dict(batch_info):\n",
    "    updated_batch = client.batches.retrieve(batch_info[\"id\"])\n",
    "    print(f\"Status of batch {updated_batch.id} is {updated_batch.status}\")\n",
    "    if updated_batch.status == \"completed\":\n",
    "      output_file = client.files.content(updated_batch.output_file_id)\n",
    "      return updated_batch, output_file\n",
    "    else:\n",
    "      return updated_batch, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "def num_tokens_from_string(string: str, encoder) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    num_tokens = len(encoder.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def count_tokens_in_dataset(dataset, num_tokens_from_string, encoder):\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for item in dataset:\n",
    "        text = item['text']\n",
    "        tokens = num_tokens_from_string(text, encoder)\n",
    "        total_tokens += tokens\n",
    "    \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(data_string, output_file):\n",
    "    \"\"\"\n",
    "    Writes JSONL string to a file.\n",
    "    \n",
    "    Args:\n",
    "        data_string (str): String containing JSONL data\n",
    "        output_file (str): Path to output file\n",
    "    \"\"\"\n",
    "    # Split the string into lines and filter out empty lines\n",
    "    json_lines = [line.strip() for line in data_string.split('\\n') if line.strip()]\n",
    "    \n",
    "    # Write each line to the file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for line in json_lines:\n",
    "            json_obj = json.loads(line)  # Parse the JSON string\n",
    "            f.write(json.dumps(json_obj) + '\\n')  # Write formatted JSON\n",
    "\n",
    "def read_jsonl(input_file):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and returns a list of JSON objects.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to input JSONL file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of parsed JSON objects\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Function to save the batch object using pickle\n",
    "def save_batch_to_pickle(batch_obj, output_file=\"batch_data.pkl\"):\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(batch_obj, f)\n",
    "\n",
    "# Function to load the batch object from a pickle file\n",
    "def load_batch_from_pickle(input_file=\"batch_data.pkl\"):\n",
    "    with open(input_file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def create_batches(dataset, batch_size=1000):\n",
    "    return [dataset.select(range(i, min(i + batch_size, len(dataset)))) for i in range(0, len(dataset), batch_size)]\n",
    "\n",
    "def load_dictionary(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate Parallel (CBN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngrams.append(tuple(words[i:i + n]))\n",
    "    return ngrams\n",
    "\n",
    "def get_dict_translation(text, dictionary):\n",
    "    unigrams = split_text_into_ngrams(text, 1)\n",
    "    bigrams = split_text_into_ngrams(text, 2)\n",
    "    trigrams = split_text_into_ngrams(text, 3)\n",
    "\n",
    "    word_translation = \"\"\n",
    "\n",
    "    for ngram in (trigrams, bigrams, unigrams):\n",
    "        for ngram_tuple in ngram:\n",
    "            ngram_str = ' '.join(ngram_tuple)\n",
    "            if ngram_str in dictionary:\n",
    "                word_translation += f\"- {ngram_str}: {', '.join(dictionary[ngram_str])}\\n\"\n",
    "\n",
    "    return word_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import datasets\n",
    "\n",
    "pt_data = datasets.load_from_disk(\"dataset/id_hq_data_dedup\")\n",
    "\n",
    "# Load the dictionary\n",
    "dictionary = load_dictionary(\"dict/idn_cbn.json\")\n",
    "\n",
    "def get_prompt_text(data):\n",
    "    prompt_text = f\"\"\"Translate the given Indonesian text in the <id_text> tag below into Cirebonese with the help of some word-to-word translation provided below. For one word, there can be multiple translations, and you need to choose the right one based on the context. The translations are as follows:\n",
    "{get_cbn_dict_translation(data[\"text\"].lower(), dictionary)}\n",
    "<id_text>\n",
    "{data[\"text\"]}\n",
    "</id_text>\n",
    "\n",
    "Return only the translated text in JSON format with key \"translated_text\".\"\"\"\n",
    "    data[\"text\"] = prompt_text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data = pt_data.map(get_prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (9/9 shards): 100%|██████████| 651856/651856 [00:25<00:00, 26003.35 examples/s] \n"
     ]
    }
   ],
   "source": [
    "pt_data.save_to_disk(\"id_hq_data_prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_tokens = count_tokens_in_dataset(pt_data, num_tokens_from_string, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1418349015"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data = pt_data.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data_60k = pt_data.select(range(60000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_tokens_60k = count_tokens_in_dataset(pt_data_60k, num_tokens_from_string, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133108166"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_tokens_60k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.98311245"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "133108166 / 4 / 1000000 * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 60000/60000 [00:00<00:00, 137178.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "pt_data_60k.save_to_disk(\"dataset/id_hq_data_prompt_60k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen_translate_w_dict(prompt):\n",
    "    sys_prompt = \"Always answer in a valid JSON format and provide only the JSON answer without anything else.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    model = \"gpt-4o-mini-2024-07-18\"\n",
    "    temperature = 1\n",
    "    max_tokens = 4096\n",
    "    response_format = {\"type\": \"json_object\"}\n",
    "    \n",
    "    return messages, model, temperature, max_tokens, response_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "batch_req_objects = []\n",
    "def create_batch_and_update_data(data):\n",
    "  random_uuid = uuid.uuid4()\n",
    "  uuid_str = str(random_uuid)\n",
    "  data[\"req_id\"] = f\"req-{uuid_str}\"\n",
    "  \n",
    "  messages, model, temperature, max_tokens, response_format = batch_gen_translate_w_dict(data[\"text\"])\n",
    "  batch_req_object = create_batch_req_object(req_id=data[\"req_id\"], model=model, messages=messages, response_format=response_format, temperature=temperature)\n",
    "  \n",
    "  batch_req_objects.append(batch_req_object)\n",
    "  \n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_batches_to_jsonl(batch_req_objects, batch_size, base_filename):\n",
    "    # Divide the batch_req_objects into smaller lists with the specified batch size\n",
    "    for i in range(0, len(batch_req_objects), batch_size):\n",
    "        batch = batch_req_objects[i:i + batch_size]\n",
    "        batch_filename = f\"{base_filename}_batch_{i // batch_size + 1}.jsonl\"\n",
    "        \n",
    "        # Convert the batch to JSONL format\n",
    "        batch_jsonl = \"\\n\".join([json.dumps(obj) for obj in batch])\n",
    "        \n",
    "        # Write the batch to a .jsonl file\n",
    "        write_jsonl(batch_jsonl, batch_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60000/60000 [00:03<00:00, 19475.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Write all the batch req objects into a jsonl file using write_jsonl\n",
    "pt_data_60k = pt_data_60k.map(create_batch_and_update_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_batches_to_jsonl(batch_req_objects, 20000, \"dataset/translate_batch/cbn_translate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the llm_batch_api function with all .jsonl files and gather batch_info\n",
    "batch_files = [f for f in os.listdir(\"dataset/translate_batch/\") if f.startswith(\"cbn_translate_batch_\") and f.endswith(\".jsonl\")]\n",
    "all_batch_info = []\n",
    "for batch_file in batch_files:\n",
    "    batch_number = batch_file.split('_')[-1].split('.')[0]\n",
    "    desc = f\"Batch {batch_number} of requests to generate translation from indonesian to Cirebonese with help of cirebonese word-by-word translation\"\n",
    "    batch_info = llm_batch_api(f\"dataset/translate_batch/{batch_file}\", purpose=\"batch\", desc=desc, completion_window=\"24h\")\n",
    "    all_batch_info.append(batch_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batch_info_dict = [b.to_dict() for b in all_batch_info]\n",
    "# Save all batch_info to a file\n",
    "with open(\"dataset/translate_batch/cbn_translate_batch_info.json\", \"w\") as f:\n",
    "    json.dump(all_batch_info_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status of batch batch_67b3ee9ae58881909226449f6d3f8202 is completed\n",
      "Batch(id='batch_67b3ee9ae58881909226449f6d3f8202', completion_window='24h', created_at=1739845274, endpoint='/v1/chat/completions', input_file_id='file-Jo9oi5WKePKiKp3WrsLboS', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1739857274, error_file_id='file-XFbnGk3njZvCqoWqesPjCB', errors=None, expired_at=None, expires_at=1739931674, failed_at=None, finalizing_at=1739852857, in_progress_at=1739845280, metadata={'description': 'Batch 1 of requests to generate translation from indonesian to Cirebonese with help of cirebonese word-by-word translation'}, output_file_id='file-PxGzyXCYYvpu8Bn9dgsiW4', request_counts=BatchRequestCounts(completed=19991, failed=9, total=20000))\n",
      "Status of batch batch_67b3eeac452c8190b9440e54c51c2ce4 is completed\n",
      "Batch(id='batch_67b3eeac452c8190b9440e54c51c2ce4', completion_window='24h', created_at=1739845292, endpoint='/v1/chat/completions', input_file_id='file-MF9Mt7PHxr3K4cVkKyRj1g', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1739854073, error_file_id='file-SdgcQDMpJSwX3EgwVCzJCT', errors=None, expired_at=None, expires_at=1739931692, failed_at=None, finalizing_at=1739851682, in_progress_at=1739845301, metadata={'description': 'Batch 3 of requests to generate translation from indonesian to Cirebonese with help of cirebonese word-by-word translation'}, output_file_id='file-14vCL7n7Q4HZrdtdpDzcwj', request_counts=BatchRequestCounts(completed=19991, failed=9, total=20000))\n",
      "Status of batch batch_67b3eebce45881908d7e2bf61d0720ad is completed\n",
      "Batch(id='batch_67b3eebce45881908d7e2bf61d0720ad', completion_window='24h', created_at=1739845309, endpoint='/v1/chat/completions', input_file_id='file-Rqju5ZV5aMnmiUAiahoT2s', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1739858288, error_file_id='file-Knai85BVLbDsGaiT73n1pF', errors=None, expired_at=None, expires_at=1739931709, failed_at=None, finalizing_at=1739854305, in_progress_at=1739845314, metadata={'description': 'Batch 2 of requests to generate translation from indonesian to Cirebonese with help of cirebonese word-by-word translation'}, output_file_id='file-Ba5L7igFuH8cJQTxwp5k9e', request_counts=BatchRequestCounts(completed=19992, failed=8, total=20000))\n"
     ]
    }
   ],
   "source": [
    "for batch_info_dict in all_batch_info_dict:\n",
    "    updated_batch, output_file = llm_batch_check_retrieve_dict(batch_info_dict)\n",
    "    print(updated_batch)\n",
    "    if output_file:\n",
    "        write_jsonl(output_file.text, f\"translate_batch_output_{batch_info_dict['id']}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Empty ID Translation for Bali Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "bali_dict = load_dictionary(\"dict/transformed_bali_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "def create_translation_dataset(input_dict):\n",
    "    data = {\n",
    "        'id': [],\n",
    "        'bali_word': [],\n",
    "        'translation_english': [],\n",
    "        'translation_indonesian': []\n",
    "    }\n",
    "    \n",
    "    for bali_word, values in input_dict.items():\n",
    "        # Skip if both translations are empty or non-existent\n",
    "        if len(values[\"translation_english\"]) == 0 and len(values[\"translation_indonesian\"]) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Only process if at least translation_english exists and is not empty\n",
    "        if  len(values['translation_english']) > 0 and len(values['translation_indonesian']) == 0:\n",
    "            data['id'].append(str(uuid.uuid4()))\n",
    "            data['bali_word'].append(bali_word)\n",
    "            data['translation_english'].append(values['translation_english'])\n",
    "            data['translation_indonesian'].append([])\n",
    "    \n",
    "    # Create HF Dataset\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "translation_dataset = create_translation_dataset(bali_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'bali_word', 'translation_english', 'translation_indonesian'],\n",
       "    num_rows: 4545\n",
       "})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_translation_prompt_text(data):\n",
    "    prompt_text = f\"\"\"Translate the given Balinese word into Indonesian, with the help of its English translation:\n",
    "- Balinese word: {data[\"bali_word\"]}\n",
    "- English translation: {data[\"translation_english\"]}\n",
    "\n",
    "The result must be a list of string, which is the Indonesian translation from the Balinese word, and since it is a list it can be more than one translation. Return only the Indonesian translation in JSON format with key \"translation_indonesian\".\"\"\"\n",
    "    data[\"prompt\"] = prompt_text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4545/4545 [00:00<00:00, 7166.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "translation_dataset = translation_dataset.map(get_id_translation_prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5338f62e-0155-4b45-9928-c1b496dbf153',\n",
       " 'bali_word': 'abah-abah',\n",
       " 'translation_english': ['tabiat', 'bakat'],\n",
       " 'translation_indonesian': [],\n",
       " 'prompt': 'Translate the given Balinese word into Indonesian, with the help of its English translation:\\n- Balinese word: abah-abah\\n- English translation: [\\'tabiat\\', \\'bakat\\']\\n\\nThe result must be a list of string, which is the Indonesian translation from the Balinese word, and since it is a list it can be more than one translation. Return only the Indonesian translation in JSON format with key \"translation_indonesian\".'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o-2024-08-06\"\n",
    "temperature = 0\n",
    "max_tokens = 512\n",
    "response_format = {\"type\": \"json_object\"}\n",
    "\n",
    "batch_req_objects = []\n",
    "for data in translation_dataset:\n",
    "  messages = [\n",
    "      {\"role\": \"user\", \"content\": data[\"prompt\"]}\n",
    "  ]\n",
    "  batch_req_object = create_batch_req_object(req_id=data[\"id\"], model=model, messages=messages, response_format=response_format, temperature=temperature)\n",
    "  \n",
    "  batch_req_objects.append(batch_req_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_batches_to_jsonl(batch_req_objects, 20000, \"dataset/translate_batch/bali_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the llm_batch_api function with all .jsonl files and gather batch_info\n",
    "batch_files = [f for f in os.listdir(\"dataset/translate_batch/\") if f.startswith(\"bali_dict_batch\") and f.endswith(\".jsonl\")]\n",
    "all_batch_info = []\n",
    "for batch_file in batch_files:\n",
    "    batch_number = batch_file.split('_')[-1].split('.')[0]\n",
    "    desc = f\"Batch {batch_number} of requests to generate Indonesian translation from Balinese word with help of English translation\"\n",
    "    batch_info = llm_batch_api(f\"dataset/translate_batch/{batch_file}\", purpose=\"batch\", desc=desc, completion_window=\"24h\")\n",
    "    all_batch_info.append(batch_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batch_info_dict = [b.to_dict() for b in all_batch_info]\n",
    "# Save all batch_info to a file\n",
    "with open(\"dataset/translate_batch/bali_dict_batch_info.json\", \"w\") as f:\n",
    "    json.dump(all_batch_info_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status of batch batch_67b57facc8248190897fdd3631a9cdee is completed\n",
      "Batch(id='batch_67b57facc8248190897fdd3631a9cdee', completion_window='24h', created_at=1739947948, endpoint='/v1/chat/completions', input_file_id='file-QWHGpJgFVv8Dob8eMy6EN3', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1739950437, error_file_id=None, errors=None, expired_at=None, expires_at=1740034348, failed_at=None, finalizing_at=1739949787, in_progress_at=1739947951, metadata={'description': 'Batch 1 of requests to generate Indonesian translation from Balinese word with help of English translation'}, output_file_id='file-4yRQdw1BtjLQMqf15nqL2t', request_counts=BatchRequestCounts(completed=4545, failed=0, total=4545))\n"
     ]
    }
   ],
   "source": [
    "for batch_info_dict in all_batch_info_dict:\n",
    "    updated_batch, output_file = llm_batch_check_retrieve_dict(batch_info_dict)\n",
    "    print(updated_batch)\n",
    "    if output_file:\n",
    "        write_jsonl(output_file.text, f\"bali_dict_batch_output.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20082"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bali_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5338f62e-0155-4b45-9928-c1b496dbf153',\n",
       " 'bali_word': 'abah-abah',\n",
       " 'translation_english': ['tabiat', 'bakat'],\n",
       " 'translation_indonesian': [],\n",
       " 'prompt': 'Translate the given Balinese word into Indonesian, with the help of its English translation:\\n- Balinese word: abah-abah\\n- English translation: [\\'tabiat\\', \\'bakat\\']\\n\\nThe result must be a list of string, which is the Indonesian translation from the Balinese word, and since it is a list it can be more than one translation. Return only the Indonesian translation in JSON format with key \"translation_indonesian\".'}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "def update_translations_from_jsonl(dataset, jsonl_path):\n",
    "    # Create a mapping of custom_id to translations from JSONL\n",
    "    translations = {}\n",
    "    with jsonlines.open(jsonl_path) as reader:\n",
    "        for obj in reader:\n",
    "            custom_id = obj['custom_id']\n",
    "            # Extract translation_indonesian from the response\n",
    "            try:\n",
    "                translation = json.loads(obj['response']['body']['choices'][0]['message']['content'])\n",
    "                translations[custom_id] = translation['translation_indonesian']\n",
    "            except (KeyError, json.JSONDecodeError):\n",
    "                continue\n",
    "\n",
    "    # Update dataset with translations\n",
    "    def update_translation(example):\n",
    "        if example['id'] in translations:\n",
    "            example['translation_indonesian'] = translations[example['id']]\n",
    "        return example\n",
    "\n",
    "    # Apply updates to dataset\n",
    "    updated_dataset = dataset.map(update_translation)\n",
    "    \n",
    "    return updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4545/4545 [00:00<00:00, 21266.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "translation_dataset = update_translations_from_jsonl(translation_dataset, \"dataset/translate_batch/bali_dict/bali_dict_batch_output.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict_translations(bali_dict, dataset):\n",
    "    # Convert dataset to dictionary for easier lookup\n",
    "    updated_entries = 0\n",
    "    \n",
    "    for item in dataset:\n",
    "        bali_word = item['bali_word']\n",
    "        \n",
    "        # Check if word exists in dictionary\n",
    "        if bali_word in bali_dict:\n",
    "            # Update translation_indonesian\n",
    "            bali_dict[bali_word]['translation_indonesian'] = item['translation_indonesian']\n",
    "            updated_entries += 1\n",
    "    \n",
    "    return bali_dict, updated_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the dictionary with new translations\n",
    "updated_dict, num_updated = update_dict_translations(bali_dict, translation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict/transformed_bali_dict_.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(updated_dict, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_translation_dicts(bali_dict):\n",
    "    # Initialize dictionaries\n",
    "    bali_indo = {}\n",
    "    indo_bali = defaultdict(list)\n",
    "    \n",
    "    # Process each entry\n",
    "    for bali_word, values in bali_dict.items():\n",
    "        # Skip entries without translations\n",
    "        if not isinstance(values, dict):\n",
    "            continue\n",
    "        if 'translation_indonesian' not in values or not values['translation_indonesian']:\n",
    "            continue\n",
    "            \n",
    "        # Add to Balinese-Indonesian dictionary\n",
    "        bali_indo[bali_word] = values['translation_indonesian']\n",
    "        \n",
    "        # Add to Indonesian-Balinese dictionary\n",
    "        for indo_word in values['translation_indonesian']:\n",
    "            indo_bali[indo_word].append(bali_word)\n",
    "    \n",
    "    return bali_indo, dict(indo_bali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create both dictionaries\n",
    "bali_indo_dict, indo_bali_dict = create_translation_dicts(bali_dict)\n",
    "\n",
    "# Save Balinese-Indonesian dictionary\n",
    "with open('dict/bali_idn.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(bali_indo_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save Indonesian-Balinese dictionary\n",
    "with open('dict/idn_bali.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(indo_bali_dict, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate Sentence Example for Bali Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "def create_sent_translation_dataset(input_dict):\n",
    "    data = {\n",
    "        'id': [],\n",
    "        'bali_word': [],\n",
    "        'example_idx': [],\n",
    "        'balinese_text': [],\n",
    "        'indonesian_text': []\n",
    "    }\n",
    "    \n",
    "    for bali_word, values in input_dict.items():\n",
    "        # Skip if no sentence examples\n",
    "        if not isinstance(values, dict) or 'sentence_examples' not in values:\n",
    "            continue\n",
    "            \n",
    "        for example_idx, example in enumerate(values['sentence_examples']):\n",
    "            # Skip if both are empty or both exist\n",
    "            has_bali = example.get('Balinese', '-') != '-'\n",
    "            has_indo = example.get('Indonesian', '-') != '-'\n",
    "            \n",
    "            if has_bali == has_indo:  # both True or both False\n",
    "                continue\n",
    "                \n",
    "            data['id'].append(str(uuid.uuid4()))\n",
    "            data['bali_word'].append(bali_word)\n",
    "            data['example_idx'].append(example_idx)\n",
    "            data['balinese_text'].append(example.get('Balinese', '-'))\n",
    "            data['indonesian_text'].append(example.get('Indonesian', '-'))\n",
    "    \n",
    "    # Create HF Dataset\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_translation_dataset = create_sent_translation_dataset(bali_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'd7b8db96-e304-47a2-8d7f-d326b85873ca',\n",
       " 'bali_word': 'anteg',\n",
       " 'example_idx': 0,\n",
       " 'balinese_text': 'Anteg jani Yan Galung tusing ada teka',\n",
       " 'indonesian_text': '-'}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_translation_dataset[86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "bali_indo_dict = load_dictionary(\"dict/bali_idn.json\")\n",
    "indo_bali_dict = load_dictionary(\"dict/idn_bali.json\")\n",
    "\n",
    "def get_translate_sentence_prompt_text(data):\n",
    "    if data[\"balinese_text\"] == \"-\" and data[\"indonesian_text\"] != \"-\":\n",
    "      prompt_text = f\"\"\"Translate the given Indonesian text in the <id_text> tag below into Balinese with the help of some word-to-word translation provided below. For one word, there can be multiple translations, and you need to choose the right one based on the context. The translations are as follows:\n",
    "{get_dict_translation(data[\"indonesian_text\"].lower(), indo_bali_dict)}\n",
    "<id_text>\n",
    "{data[\"indonesian_text\"]}\n",
    "</id_text>\n",
    "\n",
    "Return only the translated text in JSON format with key \"translated_text\".\"\"\"\n",
    "    elif data[\"balinese_text\"] != \"-\" and data[\"indonesian_text\"] == \"-\":\n",
    "      prompt_text = f\"\"\"Translate the given Balinese text in the <bali_text> tag below into Indonesian with the help of some word-to-word translation provided below. For one word, there can be multiple translations, and you need to choose the right one based on the context. The translations are as follows:\n",
    "{get_dict_translation(data[\"balinese_text\"].lower(), bali_indo_dict)}\n",
    "<bali_text>\n",
    "{data[\"balinese_text\"]}\n",
    "</bali_text>\n",
    "\n",
    "Return only the translated text in JSON format with key \"translated_text\".\"\"\"\n",
    "    \n",
    "    data[\"prompt_text\"] = prompt_text\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6421/6421 [00:00<00:00, 19170.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "sent_translation_dataset = sent_translation_dataset.map(get_translate_sentence_prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o-2024-08-06\"\n",
    "temperature = 0\n",
    "max_tokens = 512\n",
    "response_format = {\"type\": \"json_object\"}\n",
    "\n",
    "batch_req_objects = []\n",
    "for data in sent_translation_dataset:\n",
    "  messages = [\n",
    "      {\"role\": \"user\", \"content\": data[\"prompt_text\"]}\n",
    "  ]\n",
    "  batch_req_object = create_batch_req_object(req_id=data[\"id\"], model=model, messages=messages, response_format=response_format, temperature=temperature)\n",
    "  \n",
    "  batch_req_objects.append(batch_req_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_batches_to_jsonl(batch_req_objects, 20000, \"dataset/translate_batch/bali_sent/bali_sent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the llm_batch_api function with all .jsonl files and gather batch_info\n",
    "batch_files = [f for f in os.listdir(\"dataset/translate_batch/bali_sent\") if f.startswith(\"bali_sent_batch\") and f.endswith(\".jsonl\")]\n",
    "all_batch_info = []\n",
    "for batch_file in batch_files:\n",
    "    batch_number = batch_file.split('_')[-1].split('.')[0]\n",
    "    desc = f\"Batch {batch_number} of requests to generate Indonesian/Balinese translation from Indonesian/Balinese text with help of dictionary\"\n",
    "    batch_info = llm_batch_api(f\"dataset/translate_batch/bali_sent/{batch_file}\", purpose=\"batch\", desc=desc, completion_window=\"24h\")\n",
    "    all_batch_info.append(batch_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batch_info_dict = [b.to_dict() for b in all_batch_info]\n",
    "# Save all batch_info to a file\n",
    "with open(\"dataset/translate_batch/bali_sent/bali_sent_batch_info.json\", \"w\") as f:\n",
    "    json.dump(all_batch_info_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status of batch batch_67b5cfdf27d48190aea00909f1e03bbf is in_progress\n",
      "Batch(id='batch_67b5cfdf27d48190aea00909f1e03bbf', completion_window='24h', created_at=1739968479, endpoint='/v1/chat/completions', input_file_id='file-JeYcdKJB7Duhy3QyGUuh1r', object='batch', status='in_progress', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1740054879, failed_at=None, finalizing_at=None, in_progress_at=1739968480, metadata={'description': 'Batch 1 of requests to generate Indonesian/Balinese translation from Indonesian/Balinese text with help of dictionary'}, output_file_id=None, request_counts=BatchRequestCounts(completed=1510, failed=0, total=6421))\n"
     ]
    }
   ],
   "source": [
    "for batch_info_dict in all_batch_info_dict:\n",
    "    updated_batch, output_file = llm_batch_check_retrieve_dict(batch_info_dict)\n",
    "    print(updated_batch)\n",
    "    if output_file:\n",
    "        write_jsonl(output_file.text, f\"dataset/translate_batch/bali_sent/bali_sent_batch_output.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
