{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def llm_call(model, messages, **call_args):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        **call_args\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def create_batch_req_object(req_id, model, messages, response_format, temperature=0.0):\n",
    "    return {\n",
    "       \"custom_id\": req_id,\n",
    "       \"method\": \"POST\",\n",
    "       \"url\": \"/v1/chat/completions\",\n",
    "       \"body\": {\n",
    "          \"model\": model, \n",
    "          \"messages\": messages,\n",
    "          \"temperature\": temperature,\n",
    "          \"response_format\": response_format,\n",
    "        }\n",
    "    }\n",
    "\n",
    "def llm_batch_api(batch_filepath, purpose=\"\", desc=\"\", completion_window=\"24h\"):\n",
    "    batch_input_file = client.files.create(\n",
    "      file=open(batch_filepath, \"rb\"),\n",
    "      purpose=purpose\n",
    "    )\n",
    "\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "\n",
    "    batch_info = client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=completion_window,\n",
    "        metadata={\n",
    "          \"description\": desc\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return batch_info\n",
    "\n",
    "def llm_batch_check_retrieve(batch_info):\n",
    "    updated_batch = client.batches.retrieve(batch_info.id)\n",
    "    print(f\"Status of batch {updated_batch.id} is {updated_batch.status}\")\n",
    "    if updated_batch.status == \"completed\":\n",
    "      output_file = client.files.content(updated_batch.output_file_id)\n",
    "      return updated_batch, output_file\n",
    "    else:\n",
    "      return updated_batch, None\n",
    "\n",
    "def llm_batch_check_retrieve_dict(batch_info):\n",
    "    updated_batch = client.batches.retrieve(batch_info[\"id\"])\n",
    "    print(f\"Status of batch {updated_batch.id} is {updated_batch.status}\")\n",
    "    if updated_batch.status == \"completed\":\n",
    "      output_file = client.files.content(updated_batch.output_file_id)\n",
    "      return updated_batch, output_file\n",
    "    else:\n",
    "      return updated_batch, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "def num_tokens_from_string(string: str, encoder) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    num_tokens = len(encoder.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def count_tokens_in_dataset(dataset, num_tokens_from_string, encoder):\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for item in dataset:\n",
    "        text = item['text']\n",
    "        tokens = num_tokens_from_string(text, encoder)\n",
    "        total_tokens += tokens\n",
    "    \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(data_string, output_file):\n",
    "    \"\"\"\n",
    "    Writes JSONL string to a file.\n",
    "    \n",
    "    Args:\n",
    "        data_string (str): String containing JSONL data\n",
    "        output_file (str): Path to output file\n",
    "    \"\"\"\n",
    "    # Split the string into lines and filter out empty lines\n",
    "    json_lines = [line.strip() for line in data_string.split('\\n') if line.strip()]\n",
    "    \n",
    "    # Write each line to the file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for line in json_lines:\n",
    "            json_obj = json.loads(line)  # Parse the JSON string\n",
    "            f.write(json.dumps(json_obj) + '\\n')  # Write formatted JSON\n",
    "\n",
    "def read_jsonl(input_file):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and returns a list of JSON objects.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to input JSONL file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of parsed JSON objects\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Function to save the batch object using pickle\n",
    "def save_batch_to_pickle(batch_obj, output_file=\"batch_data.pkl\"):\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(batch_obj, f)\n",
    "\n",
    "# Function to load the batch object from a pickle file\n",
    "def load_batch_from_pickle(input_file=\"batch_data.pkl\"):\n",
    "    with open(input_file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def create_batches(dataset, batch_size=1000):\n",
    "    return [dataset.select(range(i, min(i + batch_size, len(dataset)))) for i in range(0, len(dataset), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dictionary(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def split_text_into_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngrams.append(tuple(words[i:i + n]))\n",
    "    return ngrams\n",
    "\n",
    "def get_dict_translation(text, dictionary):\n",
    "    unigrams = split_text_into_ngrams(text, 1)\n",
    "    bigrams = split_text_into_ngrams(text, 2)\n",
    "    trigrams = split_text_into_ngrams(text, 3)\n",
    "\n",
    "    word_translation = \"\"\n",
    "\n",
    "    for ngram in (trigrams, bigrams, unigrams):\n",
    "        for ngram_tuple in ngram:\n",
    "            ngram_str = ' '.join(ngram_tuple)\n",
    "            if ngram_str in dictionary:\n",
    "                word_translation += f\"- {ngram_str}: {', '.join(dictionary[ngram_str])}\\n\"\n",
    "\n",
    "    return word_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haznitrama/scale-resources/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import datasets\n",
    "\n",
    "pt_data = datasets.load_from_disk(\"id_hq_data_dedup\")\n",
    "\n",
    "# Load the dictionary\n",
    "dictionary = load_dictionary(\"dict/idn_cbn.json\")\n",
    "\n",
    "def get_prompt_text(data):\n",
    "    text = data[\"text\"].lower()\n",
    "    prompt_text = f\"\"\"Translate the given Indonesian text in the <id_text> tag below into Cirebonese with the help of some word-to-word translation provided below. For one word, there can be multiple translations, and you need to choose the right one based on the context. The translations are as follows:\n",
    "{get_dict_translation(text, dictionary)}\n",
    "<id_text>\n",
    "{text}\n",
    "</id_text>\n",
    "\n",
    "Return only the translated text in JSON format with key \"translated_text\".\"\"\"\n",
    "    data[\"text\"] = prompt_text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 651856/651856 [02:26<00:00, 4456.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "pt_data = pt_data.map(get_prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (9/9 shards): 100%|██████████| 651856/651856 [00:07<00:00, 92440.44 examples/s] \n"
     ]
    }
   ],
   "source": [
    "pt_data.save_to_disk(\"id_hq_data_prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_tokens = count_tokens_in_dataset(pt_data, num_tokens_from_string, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1418349015"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106.376176125"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1418349015 / 1000000 * 0.075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data = pt_data.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data_20k = pt_data.select(range(20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_tokens_20k = count_tokens_in_dataset(pt_data_20k, num_tokens_from_string, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43674330"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_tokens_20k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.102298999999999"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "43674330 / 1000000 * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 20000/20000 [00:00<00:00, 141942.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "pt_data_20k.save_to_disk(\"id_hq_data_prompt_20k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen_translate_w_dict(prompt):\n",
    "    sys_prompt = \"Always answer in a valid JSON format and provide only the JSON answer without anything else.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    model = \"gpt-4o-mini-2024-07-18\"\n",
    "    temperature = 1\n",
    "    max_tokens = 4096\n",
    "    response_format = {\"type\": \"json_object\"}\n",
    "    \n",
    "    return messages, model, temperature, max_tokens, response_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random UUID: 9027a724-6001-420b-8770-2f3b310b7557\n",
      "UUID as string: 9027a724-6001-420b-8770-2f3b310b7557\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# Generate a random UUID\n",
    "random_uuid = uuid.uuid4()\n",
    "print(f\"Random UUID: {random_uuid}\")\n",
    "\n",
    "# Convert to string\n",
    "uuid_str = str(random_uuid)\n",
    "print(f\"UUID as string: {uuid_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "batch_req_objects = []\n",
    "def create_batch_and_update_data(data):\n",
    "  random_uuid = uuid.uuid4()\n",
    "  uuid_str = str(random_uuid)\n",
    "  data[\"req_id\"] = f\"req-{uuid_str}\"\n",
    "  \n",
    "  messages, model, temperature, max_tokens, response_format = batch_gen_translate_w_dict(data[\"text\"])\n",
    "  batch_req_object = create_batch_req_object(req_id=data[\"req_id\"], model=model, messages=messages, response_format=response_format, temperature=temperature)\n",
    "  \n",
    "  batch_req_objects.append(batch_req_object)\n",
    "  \n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20000/20000 [00:01<00:00, 12636.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Write all the batch req objects into a jsonl file using write_jsonl\n",
    "pt_data_20k = pt_data_20k.map(create_batch_and_update_data)\n",
    "batch_req_objects_jsonl = \"\\n\".join([json.dumps(obj) for obj in batch_req_objects])\n",
    "write_jsonl(batch_req_objects_jsonl, \"gen_translate_batch.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_info = llm_batch_api(\"gen_translate_batch.jsonl\", purpose=\"batch\", desc=\"Batch of requests to generate translation from indonesian to Cirebonese with help of cirebonese word-by-word translation\", completion_window=\"24h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_info_dict = batch_info.to_dict()\n",
    "# Save all batch_info to a file\n",
    "with open(\"translate_batch_info.json\", \"w\") as f:\n",
    "    json.dump(batch_info_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status of batch batch_67b3308d97208190ab2f142a7aa99026 is completed\n",
      "Batch(id='batch_67b3308d97208190ab2f142a7aa99026', completion_window='24h', created_at=1739796621, endpoint='/v1/chat/completions', input_file_id='file-PK5wBtqUNSAua1fgxJguUx', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1739806397, error_file_id='file-KU92rTPq7AxWXvNEn7rjRe', errors=None, expired_at=None, expires_at=1739883021, failed_at=None, finalizing_at=1739803900, in_progress_at=1739796631, metadata={'description': 'Batch of requests to generate translation from indonesian to Cirebonese with help of cirebonese word-by-word translation'}, output_file_id='file-MUucbzkLMDmE4o8hbjs3Uw', request_counts=BatchRequestCounts(completed=19997, failed=3, total=20000))\n"
     ]
    }
   ],
   "source": [
    "updated_batch, output_file = llm_batch_check_retrieve_dict(batch_info_dict)\n",
    "print(updated_batch)\n",
    "if output_file:\n",
    "    write_jsonl(output_file.text, f\"translate_batch_output.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
